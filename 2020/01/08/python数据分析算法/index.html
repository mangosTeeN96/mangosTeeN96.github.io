<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">




  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2">



















<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.2',
    sidebar: {"position":"left","width":280,"display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="python数据分析算法，决策树、分类、聚类、关联规则挖掘、集成等">
<meta name="keywords" content="数据分析">
<meta property="og:type" content="article">
<meta property="og:title" content="python数据分析算法">
<meta property="og:url" content="http://mangosTeeN96.github.io/2020/01/08/python数据分析算法/index.html">
<meta property="og:site_name" content="MangosTeen">
<meta property="og:description" content="python数据分析算法，决策树、分类、聚类、关联规则挖掘、集成等">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/52.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/53.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/54.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/55.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/56.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/57.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/58.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/59.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/60.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/61.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/62.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/64.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/65.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/63.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/66.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/67.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/68.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/70.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/71.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/72.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/77.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/73.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/74.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/78.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/75.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/76.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/79.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/80.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/81.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/82.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/83.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/84.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/85.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/87.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/88.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/89.jpg">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/90.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/91.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/92.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/94.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/93.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/97.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/98.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/99.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/100.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/101.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/102.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/103.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/104.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/95.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/96.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/105.jpeg">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/106.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/107.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/108.jpeg">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/109.jpeg">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/110.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/111.jpeg">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/112.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/113.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/115.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/114.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/116.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/117.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/118.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/119.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/120.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/121.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/124.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/123.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/122.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/125.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/126.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/127.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/128.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/129.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/130.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/weixin.jpg">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/131.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/weixin_mark.jpg">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/weixin_mark_color.jpg">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/weixin_new.jpg">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/132.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/133.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/134.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/135.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/136.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/137.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/138.jpg">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/139.jpg">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/143.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/140.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/141.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/142.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/144.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/145.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/146.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/147.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/148.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/149.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/150.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/151.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/152.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/153.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/154.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/155.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/156.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/160.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/157.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/158.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/159.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/161.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/162.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/163.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/164.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/165.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/166.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/167.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/168.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/169.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/170.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/162.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/172.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/173.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/174.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/171.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/175.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/176.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/177.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/178.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/179.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/180.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/181.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/182.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/183.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/184.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/185.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/186.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/187.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/188.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/191.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/189.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/190.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/197.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/198.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/199.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/200.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/201.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/202.png">
<meta property="og:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/203.png">
<meta property="og:updated_time" content="2020-05-28T06:03:50.932Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="python数据分析算法">
<meta name="twitter:description" content="python数据分析算法，决策树、分类、聚类、关联规则挖掘、集成等">
<meta name="twitter:image" content="http://mangosteen96.github.io/2020/01/08/python数据分析算法/52.png">





  
  
  <link rel="canonical" href="http://mangosTeeN96.github.io/2020/01/08/python数据分析算法/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>python数据分析算法 | MangosTeen</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MangosTeen</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mangosTeeN96.github.io/2020/01/08/python数据分析算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MangosTeen">
      <meta itemprop="description" content="想养一只猫">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MangosTeen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">python数据分析算法

              
            
          </h1>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-01-08 10:38:45" itemprop="dateCreated datePublished" datetime="2020-01-08T10:38:45+08:00">2020-01-08</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-05-28 14:03:50" itemprop="dateModified" datetime="2020-05-28T14:03:50+08:00">2020-05-28</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          <br>
          

          

          
            <div class="post-description">python数据分析算法，决策树、分类、聚类、关联规则挖掘、集成等</div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="16-决策树"><a href="#16-决策树" class="headerlink" title="16.决策树"></a>16.决策树</h2><p>在现实生活中，我们会遇到各种选择，不论是选择男女朋友，还是挑选水果，都是基于以往的经验来做判断。如果把判断背后的逻辑整理成一个结构图，会发现它实际上是一个树状图，这就是决策树。</p>
<h3 id="决策树的工作原理"><a href="#决策树的工作原理" class="headerlink" title="决策树的工作原理"></a>决策树的工作原理</h3><p>决策树基本上就是把我们以前的经验总结出来。如果我们要出门打篮球，一般会根据“天气”、“温度”、“湿度”、“刮风”这几个条件来判断，最后得到结果：去打篮球？还是不去？</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/52.png" alt="52"></p>
<p>上面这个图就是一棵典型的决策树。我们在做决策树的时候，会经历两个阶段：<strong>构造</strong>和<strong>剪枝</strong>。</p>
<h4 id="构造"><a href="#构造" class="headerlink" title="构造"></a>构造</h4><p><strong>构造就是生成一棵完整的决策树</strong>。简单来说，<strong>构造的过程就是选择什么属性作为节点的过程</strong>，那么在构造过程中，会存在三种节点：</p>
<ol>
<li>根节点：就是树的最顶端，最开始的那个节点。在上图中，“天气”就是一个根节点；</li>
<li>内部节点：就是树中间的那些节点，比如说“温度”、“湿度”、“刮风”；</li>
<li>叶节点：就是树最底部的节点，也就是决策结果。</li>
</ol>
<p>节点之间存在父子关系。比如根节点会有子节点，子节点会有子子节点，但是到了叶节点就停止了，叶节点不存在子节点。那么在构造过程中，你要解决三个重要的问题：</p>
<ol>
<li>选择哪个属性作为根节点；</li>
<li>选择哪些属性作为子节点；</li>
<li>什么时候停止并得到目标状态，即叶节点。</li>
</ol>
<h4 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h4><p>剪枝就是给决策树瘦身，这一步想实现的目标就是，不需要太多的判断，同样可以得到不错的结果。之所以这么做，是为了防止“过拟合”（Overfitting）现象的发生。</p>
<p><strong>过拟合</strong>：指的是模型的训练结果“太好了”，以至于在实际应用的过程中，会存在“死板”的情况，导致分类错误。</p>
<p><strong>欠拟合</strong>：指的是模型的训练结果不理想。</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/53.png" alt="53"></p>
<p><strong>造成过拟合的原因</strong>：</p>
<p>一是因为训练集中样本量较小。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。</p>
<p><strong>泛化能力</strong>：指的分类器是通过训练集抽象出来的分类能力，你也可以理解是举一反三的能力。如果我们太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。</p>
<p><strong>剪枝的方法</strong>：</p>
<ul>
<li><strong>预剪枝</strong>：在决策树构造时就进行剪枝。方法是，在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。</li>
<li><strong>后剪枝</strong>：在生成决策树之后再进行剪枝。通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。</li>
</ul>
<p>实际上决策树分类器，以及决策树回归器（对应DecisionTreeRegressor类）都没有集成剪枝步骤。一般对决策树进行缩减，常用的方法是在构造DecisionTreeClassifier类时，对参数进行设置，比如maxdepth表示树的最大深度，max_leaf_nodes表示最大的叶子节点数。通过调整这两个参数，就能对决策树进行剪枝。当然也可以自己编写剪枝程序完成剪枝。</p>
<h3 id="如何构造决策树"><a href="#如何构造决策树" class="headerlink" title="如何构造决策树"></a>如何构造决策树</h3><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/54.png" alt="54" style="zoom: 50%;">

<p>我们该如何构造一个判断是否去打篮球的决策树呢？再回顾一下决策树的构造原理，在决策过程中有三个重要的问题：将哪个属性作为根节点？选择哪些属性作为后继节点？什么时候停止并得到目标值？</p>
<p>显然将哪个属性（天气、温度、湿度、刮风）作为根节点是个关键问题，在这里我们先介绍两个指标<strong>：纯度</strong>和<strong>信息熵</strong>。</p>
<h4 id="纯度："><a href="#纯度：" class="headerlink" title="纯度："></a>纯度：</h4><p><strong>你可以把决策树的构造过程理解成为寻找纯净划分的过程</strong>。数学上，我们可以用纯度来表示，纯度换一种方式来解释就是<strong>让目标变量的分歧最小</strong>。</p>
<p>举个例子，假设有 3 个集合：</p>
<ul>
<li>集合 1：6 次都去打篮球；</li>
<li>集合 2：4 次去打篮球，2 次不去打篮球；</li>
<li>集合 3：3 次去打篮球，3 次不去打篮球。</li>
</ul>
<p>按照纯度指标来说，集合 1&gt; 集合 2&gt; 集合 3。因为集合1 的分歧最小，集合 3 的分歧最大。</p>
<h4 id="信息熵：表示信息的不确定度"><a href="#信息熵：表示信息的不确定度" class="headerlink" title="信息熵：表示信息的不确定度"></a>信息熵：表示信息的不确定度</h4><p>在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/55.png" alt="55" style="zoom: 33%;">

<p>p(i|t) 代表了节点 t 为分类 i 的概率，其中 log2 为取以 2 为底的对数。这里我们不是来介绍公式的，而是说存在一种度量，它能帮我们反映出来这个信息的不确定度。<strong>当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。</strong></p>
<p>举个例子，假设有 2 个集合：</p>
<ul>
<li>集合 1：5 次去打篮球，1 次不去打篮球；</li>
<li>集合 2：3 次去打篮球，3 次不去打篮球。</li>
</ul>
<p>在集合 1 中，有 6 次决策，其中打篮球是 5 次，不打篮球是 1 次。那么假设：类别 1 为“打篮球”，即次数为 5；类别 2 为“不打篮球”，即次数为 1。那么节点划分为类别1的概率是 5/6，为类别2的概率是1/6，带入上述信息熵公式可以计算得出：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/56.png" alt="56" style="zoom:33%;">

<p>同样，集合 2 中，也是一共 6 次决策，其中类别 1 中“打篮球”的次数是 3，类别 2“不打篮球”的次数也是 3，那么信息熵为多少呢？我们可以计算得出：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/57.png" alt="57" style="zoom:33%;">

<p>从上面的计算结果中可以看出，<strong>信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。</strong></p>
<p>我们在构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是<strong>信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）</strong>。</p>
<h3 id="信息增益（ID3算法）："><a href="#信息增益（ID3算法）：" class="headerlink" title="信息增益（ID3算法）："></a>信息增益（ID3算法）：</h3><p>信息增益指的就是<strong>划分可以带来纯度的提高，信息熵的下降</strong>。它的计算公式，是<strong>父亲节点的信息熵减去所有子节点的信息熵</strong>。在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/58.png" alt="58" style="zoom:33%;">

<p>公式中 D 是父亲节点，Di 是子节点，Gain(D,a)中的 a 作为 D 节点的属性选择。</p>
<p><strong>举例</strong>：</p>
<p>我们基于 ID3 的算法规则，完整地计算下我们的训练集，训练集中一共有 7 条数据，3 个打篮球，4 个不打篮球，所以根节点的信息熵是：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/59.png" alt="59" style="zoom:33%;">

<p>如果你将天气作为属性的划分，会有三个叶子节点 D1、D2 和D3，分别对应的是晴天、阴天和小雨。我们用 + 代表去打篮球，- 代表不去打篮球。那么第一条记录，晴天不去打篮球，可以记为 1-，于是我们可以用下面的方式来记录 D1，D2，D3：</p>
<p>D1(天气 = 晴天)={1-,2-,6+}</p>
<p>D2(天气 = 阴天)={3+,7-}</p>
<p>D3(天气 = 小雨)={4+,5-}</p>
<p>我们先分别计算三个叶子节点的信息熵：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/60.png" alt="60" style="zoom: 50%;">

<p>D1 在 D（父节点）中的概率是 3/7，D2在父节点的概率是 2/7，D3 在父节点的概率是 2/7。那么作为子节点的归一化信息熵 = 3/7<em>0.918+2/7</em>1.0=0.965。</p>
<p>因为我们用 ID3 中的信息增益来构造决策树，所以要计算每个节点的信息增益。</p>
<p>天气作为属性节点的信息增益为，Gain(D , 天气)=0.985-0.965=0.020。</p>
<p>同理我们可以计算出其他属性作为根节点的信息增益，它们分别为：</p>
<p>Gain(D , 温度)=0.128</p>
<p>Gain(D , 湿度)=0.020</p>
<p>Gain(D , 刮风)=0.020</p>
<p>我们能看出来温度作为属性的信息增益最大。因为 ID3 就是要将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树，所以我们将温度作为根节点。其决策树状图分裂为下图所示：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/61.png" alt="61" style="zoom:50%;">

<p>然后我们要将上图中第一个叶节点，也就是 D1={1-,2-,3+,4+}进一步进行分裂，往下划分，计算其不同属性（天气、湿度、刮风）作为节点的信息增益，可以得到：</p>
<p>Gain(D , 天气)=0</p>
<p>Gain(D , 湿度)=0</p>
<p>Gain(D , 刮风)=0.0615</p>
<p>我们能看到刮风为 D1 的节点都可以得到最大的信息增益，这里我们选取刮风作为节点。同理，我们可以按照上面的计算步骤得到完整的决策树，结果如下：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/62.png" alt="62" style="zoom:50%;">

<p>于是我们通过 ID3 算法得到了一棵决策树。ID3 的算法规则相对简单，可解释性强。同样也存在缺陷，比如我们会发现 <strong>ID3 算法倾向于选择取值比较多的属性</strong>。这样，如果我们把“编号”作为一个属性（一般情况下不会这么做，这里只是举个例子），那么“编号”将会被选为最优属性 。但实际上“编号”是无关属性的，它对“打篮球”的分类并没有太大作用。</p>
<p>所以 ID3 有一个<strong>缺陷</strong>就是，有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。这种缺陷不是每次都会发生，只是存在一定的概率。在大部分情况下，ID3 都能生成不错的决策树分类。针对可能发生的缺陷，后人提出了新的算法进行改进。</p>
<h3 id="在-ID3-算法上进行改进的-C4-5-算法"><a href="#在-ID3-算法上进行改进的-C4-5-算法" class="headerlink" title="在 ID3 算法上进行改进的 C4.5 算法"></a>在 ID3 算法上进行改进的 C4.5 算法</h3><ol>
<li>采用信息增益率</li>
</ol>
<p>因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5 采用信息增益率的方式来选择属性。<strong>信息增益率 = 信息增益 / 属性熵</strong></p>
<p>当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。</p>
<ol start="2">
<li>采用悲观剪枝</li>
</ol>
<p>ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。</p>
<p>悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。</p>
<ol start="3">
<li>离散化处理连续属性</li>
</ol>
<p>C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，<strong>C4.5 选择具有最高信息增益的划分所对应的阈值</strong>。</p>
<ol start="4">
<li>处理缺失值</li>
</ol>
<p>针对数据集不完整的情况，C4.5 也可以进行处理。</p>
<p>假如我们得到的是如下的数据，你会发现这个数据中存在两点问题。第一个问题是，数据集中存在数值缺失的情况，如何进行属性选择？第二个问题是，假设已经做了属性划分，但是样本在这个属性上有缺失值，该如何对样本进行划分？</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/64.png" alt="64"></p>
<p>我们不考虑缺失的数值，可以得到温度 D={2-,3+,4+,5-,6+,7-}。温度 = 高：D1={2-,3+,4+}；温度 = 中：D2={6+,7-}；温度 = 低：D3={5-} 。这里 + 号代表打篮球，- 号代表不打篮球。比如ID=2 时，决策是不打篮球，我们可以记录为 2-。</p>
<p>所以三个叶节点的信息熵可以结算为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/65.png" alt="65" style="zoom: 43%;">

<p>这三个节点的归一化信息熵为 3/6<em>0.918+2/6</em>1.0+1/6*0=0.792。</p>
<p>针对将属性选择为温度的信息增益率为：</p>
<p>Gain(D′, 温度)=Ent(D′)-0.792=1.0-0.792=0.208</p>
<p>D′的样本个数为 6，而 D 的样本个数为 7，所以所占权重比例为 6/7，所以 Gain(D′，温度) 所占权重比例为6/7，所以：</p>
<p>Gain(D, 温度)=6/7*0.208=0.178</p>
<p>这样即使在温度属性的数值有缺失的情况下，我们依然可以计算信息增益，并对属性进行选择。</p>
<p>小结：</p>
<p>首先 ID3 算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。C4.5 在 IID3 的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于 C4.5 需要对数据集进行多次扫描，算法效率相对较低。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/63.png" alt="63" style="zoom:50%;">

<h3 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h3><p>D3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作分类树，又可以作回归树。</p>
<p>那么你首先需要了解的是，什么是分类树，什么是回归树呢？</p>
<p>我用下面的训练数据举个例子，你能看到不同职业的人，他们的年龄不同，学习时间也不同。如果我构造了一棵决策树，想要基于数据判断这个人的职业身份，这个就属于<strong>分类树</strong>，因为是从几个分类中来做选择。如果是给定了数据，想要预测这个人的年龄，那就属于<strong>回归树</strong>。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/66.png" alt="64" style="zoom:50%;">

<p>分类树可以处理<strong>离散</strong>数据，也就是数据种类有限的数据，它输出的是样本的类别。</p>
<p>回归树可以对<strong>连续</strong>型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。</p>
<h4 id="CART-分类树的工作流程"><a href="#CART-分类树的工作流程" class="headerlink" title="CART 分类树的工作流程"></a>CART 分类树的工作流程</h4><p>通过上一讲，我们知道决策树的核心就是寻找纯净的划分，因此引入了纯度的概念。</p>
<p>在属性选择上，我们是通过统计“不纯度”来做判断的，ID3 是基于信息增益做判断，C4.5 在 ID3 的基础上做了改进，提出了信息增益率的概念。实际上 CART 分类树与 C4.5 算法类似，只是属性选择的指标采用的是基尼系数。</p>
<p>你可能在经济学中听过说基尼系数，它是用来衡量一个国家收入差距的常用指标。当基尼系数大于 0.4 的时候，说明财富差异悬殊。基尼系数在 0.2-0.4 之间说明分配合理，财富差距不大。</p>
<p>基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。</p>
<p>所以 CART 算法在构造分类树的时候，会选择<strong>基尼系数最小</strong>的属性作为属性的划分。</p>
<p>假设 t 为节点，那么该节点的 GINI 系数的计算公式为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/67.png" alt="64" style="zoom: 23%;">

<p>这里 p(Ck|t) 表示节点 t 属于类别 Ck 的概率，节点 t 的基尼系数为 1 减去各类别 Ck 概率平方和。</p>
<p>通过下面这个例子，我们计算一下两个集合的基尼系数分别为多少：</p>
<p>集合 1：6 个都去打篮球；</p>
<p>集合 2：3 个去打篮球，3 个不去打篮球。</p>
<p>针对集合 1，所有人都去打篮球，所以 p(Ck|t)=1，因此 GINI(t)=1-1=0。</p>
<p>针对集合 2，有一半人去打篮球，而另一半不去打篮球，所以，p(C1|t)=0.5，p(C2|t)=0.5，GINI(t)=1-（0.5<em>0.5+0.5</em>0.5）=0.5。</p>
<p>通过两个基尼系数你可以看出，集合 1 的基尼系数最小，也证明样本最稳定，而集合 2 的样本不稳定性更大。</p>
<p>在 CART 算法中，基于基尼系数对特征属性进行二元分裂，假设属性 A 将节点 D 划分成了 D1 和 D2，如下图所示：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/68.png" alt="64" style="zoom:50%;">

<p>节点 D 的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，用公式表示为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/70.png" alt="64" style="zoom: 33%;">

<p>归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点 D 中的比例。</p>
<p>上面我们已经计算了集合 D1 和集合 D2 的 GINI 系数，得到：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/71.png" alt="64" style="zoom: 15%;">

<p>所以节点 D 的基尼系数为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/72.png" alt="64" style="zoom: 23%;">

<p>节点 D 被属性 A 划分后的基尼系数越大，样本集合的不确定性越大，也就是不纯度越高。</p>
<h4 id="如何使用-CART-算法来创建分类树"><a href="#如何使用-CART-算法来创建分类树" class="headerlink" title="如何使用 CART 算法来创建分类树"></a>如何使用 CART 算法来创建分类树</h4><p>在 Python 的 sklearn 中，如果我们想要创建CART 分类树，可以直接使用 DecisionTreeClassifier 这个类。创建这个类的时候，默认情况下 criterion 这个参数等于 gini，也就是按照基尼系数来选择属性划分，即默认采用的是 CART 分类树。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取特征集和分类标识</span></span><br><span class="line">features = iris.data</span><br><span class="line">labels = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机抽取 33% 的数据作为测试集，其余为训练集</span></span><br><span class="line">train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = <span class="number">0.33</span>, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 CART 分类树</span></span><br><span class="line">clf = DecisionTreeClassifier(criterion = <span class="string">'gini'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合构造 CART 分类树</span></span><br><span class="line">clf = clf.fit(train_features, train_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用 CART 分类树做预测</span></span><br><span class="line">test_predict = clf.predict(test_features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测结果与测试集结果作比对</span></span><br><span class="line">score = accuracy_score(test_labels, test_predict)</span><br><span class="line">print(<span class="string">"CART 分类树准确率 %.4lf"</span> %score)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 运行结果：</span><br><span class="line">CART 分类树准确率 0.9600</span><br></pre></td></tr></table></figure>

<p>如果把决策树画出来得到结果：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/77.png" alt="77" style="zoom:50%;">

<p>首先train_test split可以帮助我们把数据集抽取一部分作为测试集，这样我们就可以得到训练集和测试集。</p>
<p>使用clf=DecisionTreeClassifier(criterion=‘gini’)初始化一棵CART分类树。这样你就可以对CART分类树进行训练。</p>
<p>使用clf.fit(train_features, train_labels)函数，将训练集的特征值和分类标识作为参数进行拟合 到CART分类树。</p>
<p>使用clf.predict(test_features)函数进行预测，传入测试集的特征值，可以得到测试结！ _predict,</p>
<p>最后使用accuracy_score(test labels，test_predict)函数，传入测试集的预 结果与实际的结果作为参数，得到准确率score.</p>
<p>我们能看到sklearn帮我们做了CART分类树的使用封装，使用起来还是很方便的。</p>
<h4 id="CART-回归树的工作流程"><a href="#CART-回归树的工作流程" class="headerlink" title="CART 回归树的工作流程"></a>CART 回归树的工作流程</h4><p>CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。</p>
<p>在 CART 分类树中采用的是基尼系数作为标准，在 CART 回归树中，要根据样本的混乱程度，也就是<strong>样本的离散程度</strong>来评价“不纯度”。</p>
<p>样本的离散程度具体的计算方式是，先计算所有样本的均值，然后计算每个样本值到均值的差值。我们假设 x 为样本的个体，均值为u。为了统计样本的离散程度，我们可以取差值的绝对值，或者方差。</p>
<p>其中差值的绝对值为样本值减去样本均值的绝对值：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/73.png" alt="64" style="zoom: 20%;">

<p>方差为每个样本值减去样本均值的平方和除以样本个数：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/74.png" alt="64" style="zoom: 33%;">

<p>所以这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。</p>
<h4 id="如何使用-CART-回归树做预测"><a href="#如何使用-CART-回归树做预测" class="headerlink" title="如何使用 CART 回归树做预测"></a>如何使用 CART 回归树做预测</h4><p>这里我们使用到 sklearn 自带的波士顿房价数据集，该数据集给出了影响房价的一些指标，比如犯罪率，房产税等，最后给出了房价。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error,mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">boston = load_boston()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 探索数据</span></span><br><span class="line">print(boston.feature_names)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取特征集和房价</span></span><br><span class="line">features = boston.data</span><br><span class="line">prices = boston.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机抽取 33% 的数据作为测试集，其余为训练集</span></span><br><span class="line">train_features, test_features, train_price, test_price = train_test_split(features, prices, test_size=<span class="number">0.33</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 CART 回归树</span></span><br><span class="line">dtr = DecisionTreeRegressor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合构造 CART 回归树</span></span><br><span class="line">dtr.fit(train_features, train_price)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测测试集中的房价</span></span><br><span class="line">predict_price = dtr.predict(test_features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集的结果评价</span></span><br><span class="line">print(<span class="string">'回归树二乘偏差均值:'</span>, mean_squared_error(test_price, predict_price))</span><br><span class="line">print(<span class="string">'回归树绝对值偏差均值:'</span>, mean_absolute_error(test_price, predict_price))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 运行结果（每次运行结果可能会有不同）：</span><br><span class="line">[&apos;CRIM&apos; &apos;ZN&apos; &apos;INDUS&apos; &apos;CHAS&apos; &apos;NOX&apos; &apos;RM&apos; &apos;AGE&apos; &apos;DIS&apos; &apos;RAD&apos; &apos;TAX&apos; &apos;PTRATIO&apos;</span><br><span class="line"> &apos;B&apos; &apos;LSTAT&apos;]</span><br><span class="line">回归树二乘偏差均值: 22.308083832335328</span><br><span class="line">回归树绝对值偏差均值: 3.3934131736526947</span><br></pre></td></tr></table></figure>

<p>回归树画出来：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/78.png" alt="77" style="zoom:50%;">

<p>首先加载了波士顿房价数据集，得到特征集和房价。然后通过train_test_split帮助我们把数据集抽取</p>
<p>部分作为测试集，其余作为训练集。</p>
<p>使用dtr=DecisionTreeRegressor()初始化一棵CART回归树。</p>
<p>使用dtr.fit(train_features,train_price)函数，将训练集的特征值和结果作为参数进行拟合，得到CART回归树。</p>
<p>使用dtr.predict(test_features)函数进行预测，传入测试集的特征值，可以得到预测结果predict_price。</p>
<p>最后我们可以求得这棵回归树的二乘偏差均值，以及绝对值偏差均值。</p>
<p>我们能看到CART回归树的使用和分类树类似，只是最后求得的预测值是个连续值。</p>
<h4 id="CART-决策树的剪枝"><a href="#CART-决策树的剪枝" class="headerlink" title="CART 决策树的剪枝"></a>CART 决策树的剪枝</h4><p>CART 决策树的剪枝主要采用的是 <strong>CCP</strong> 方法，它是一种后剪枝的方法，英文全称叫做 cost-complexity prune，中文叫做代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。用公式表示则是：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/75.png" alt="64" style="zoom: 15%;">

<p>其中 Tt 代表以 t 为根节点的子树，C(Tt) 表示节点t 的子树没被裁剪时子树 Tt 的误差，C(t) 表示节点t 的子树被剪枝后节点 t 的误差，|Tt|代子树 Tt的叶子数，剪枝后，T 的叶子数减少了|Tt|-1。</p>
<p>所以节点的表面误差率增益值等于节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量。</p>
<p>因为我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。</p>
<p>得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。</p>
<h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><p>CART 决策树，它是一棵决策二叉树，既可以做分类树，也可以做回归树。你需要记住的是，作为分类树，CART 采用基尼系数作为节点划分的依据，得到的是离散的结果，也就是分类结果；作为回归树，CART 可以采用最小绝对偏差（LAD），或者最小二乘偏差（LSD）作为节点划分的依据，得到的是连续值，即回归预测结果。</p>
<p>三种决策树之间在属性选择标准上的差异：</p>
<ul>
<li>ID3 算法，基于信息增益做判断；</li>
<li>C4.5 算法，基于信息增益率做判断；</li>
<li>CART 算法，分类树是基于基尼系数做判断。回归树是基于偏差做判断。</li>
</ul>
<p>在工具使用上，我们可以使用 sklearn 中的 DecisionTreeClassifier 创建 CART 分类树，通过 DecisionTreeRegressor 创建 CART 回归树。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/76.png" alt="64" style="zoom: 50%;">

<h3 id="※-决策树之泰坦尼克号生存预测"><a href="#※-决策树之泰坦尼克号生存预测" class="headerlink" title="※ 决策树之泰坦尼克号生存预测"></a>※ 决策树之泰坦尼克号生存预测</h3><p>决策树分类的应用场景非常广泛，在各行各业都有应用，比如在金融行业可以用决策树做贷款风险评估，医疗行业可以用决策树生成辅助诊断，电商行业可以用决策树对销售额进行预测等。</p>
<p>基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。</p>
<h4 id="sklearn-中的决策树模型"><a href="#sklearn-中的决策树模型" class="headerlink" title="sklearn 中的决策树模型"></a>sklearn 中的决策树模型</h4><p>到目前为止，sklearn 中只实现了 ID3 与 CART决策树，所以我们暂时只能使用这两种决策树，在构造 DecisionTreeClassifier 类时，其中有一个参数是criterion，意为标准。它决定了构造的分类树是采用 ID3 分类树，还是 CART 分类树，对应的取值分别是 entropy 或者 gini：</p>
<ul>
<li>entropy: 基于信息熵，也就是 ID3 算法，实际结果与 C4.5 相差不大；</li>
<li>gini：默认参数，基于基尼系数。CART 算法是基于基尼系数做属性划分的，所以 criterion=gini 时，实际上执行的是 CART 算法。</li>
</ul>
<p>我们通过设置 criterion=’entropy’可以创建一个 ID3 决策树分类器，然后打印下 clf，看下决策树在sklearn 中是个什么东西？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`DecisionTreeClassifier(class_weight``=``<span class="literal">None</span>``, criterion``=``<span class="string">'entropy'</span>``, max_depth``=``<span class="literal">None</span>``,``            ``max_features``=``<span class="literal">None</span>``, max_leaf_nodes``=``<span class="literal">None</span>``,``            ``min_impurity_decrease``=``<span class="number">0.0</span>``, min_impurity_split``=``<span class="literal">None</span>``,``            ``min_samples_leaf``=``<span class="number">1</span>``, min_samples_split``=``<span class="number">2</span>``,``            ``min_weight_fraction_leaf``=``<span class="number">0.0</span>``, presort``=``<span class="literal">False</span>``, random_state``=``<span class="literal">None</span>``,``            ``splitter``=``<span class="string">'best'</span>``)`</span><br></pre></td></tr></table></figure>

<p>这里我们看到了很多参数，除了设置 criterion 采用不同的决策树算法外，一般建议使用默认的参数，默认参数不会限制决策树的最大深度，不限制叶子节点数，认为所有分类的权重都相等等。当然你也可以调整这些参数，来创建不同的决策树模型。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/79.png" alt="64" style="zoom: 90%;">

<p>在构造决策树分类器后，我们可以使用 fit 方法让分类器进行拟合，使用 predict 方法对新数据进行预测，得到预测的分类结果，也可以使用 score 方法得到分类器的准确率。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/80.png">

<h4 id="Titanic-乘客生存预测"><a href="#Titanic-乘客生存预测" class="headerlink" title="Titanic 乘客生存预测"></a>Titanic 乘客生存预测</h4><p>数据集：<a href="https://github.com/cystanford/Titanic_Data" target="_blank" rel="noopener">https://github.com/cystanford/Titanic_Data</a></p>
<p>其中数据集格式为 csv，一共有两个文件：</p>
<ul>
<li>train.csv 是训练数据集，包含特征信息和存活与否的标签；</li>
<li>test.csv: 测试数据集，只包含特征信息。</li>
</ul>
<p>现在我们需要用决策树分类对训练集进行训练，针对测试集中的乘客进行生存预测，并告知分类器的准确率。</p>
<p>在训练集中，包括了以下字段，它们具体为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/81.png">

<h4 id="生存预测的关键流程"><a href="#生存预测的关键流程" class="headerlink" title="生存预测的关键流程"></a>生存预测的关键流程</h4><p>我们要对训练集中乘客的生存进行预测，这个过程可以划分为两个重要的阶段：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/82.png">

<ol>
<li>准备阶段：我们首先需要对训练集、测试集的数据进行探索，分析数据质量，并对数据进行清洗，然后通过特征选择对数据进行降维，方便后续分类运算；</li>
<li>分类阶段：首先通过训练集的特征矩阵、分类结果得到决策树分类器，然后将分类器应用于测试集。然后我们对决策树分类器的准确性进行分析，并对决策树模型进行可视化。</li>
</ol>
<h5 id="模块-1：数据探索"><a href="#模块-1：数据探索" class="headerlink" title="模块 1：数据探索"></a>模块 1：数据探索</h5><ul>
<li>使用 info() 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度；</li>
<li>使用 describe() 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等；</li>
<li>使用 describe(include=[‘O’]) 查看字符串类型（非数字）的整体情况；</li>
<li>使用 head 查看前几行数据（默认是前 5 行）；</li>
<li>使用 tail 查看后几行数据（默认是最后 5 行）。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="keyword">import</span>` `pandas <span class="keyword">as</span> pd``<span class="comment"># 数据加载``train_data ``=` `pd.read_csv(``'./Titanic_Data/train.csv'``)``test_data ``=` `pd.read_csv(``'./Titanic_Data/test.csv'``)``# 数据探索``print``(train_data.info())``print``(``'-'``*``30``)``print``(train_data.describe())``print``(``'-'``*``30``)``print``(train_data.describe(include``=``[``'O'``]))``print``(``'-'``*``30``)``print``(train_data.head())``print``(``'-'``*``30``)``print``(train_data.tail())`</span></span><br></pre></td></tr></table></figure>

<h5 id="模块-2：数据清洗"><a href="#模块-2：数据清洗" class="headerlink" title="模块 2：数据清洗"></a>模块 2：数据清洗</h5><p>通过数据探索，我们发现 Age、Fare、Cabin、Embarked 这几个字段的数据有所缺失。其中 Age 为年龄字段，是数值型，我们可以通过平均值进行补齐。Fare票价一样（注意，不仅要看训练数据，也要看测试数据缺失）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="comment"># 使用平均年龄来填充年龄中的 nan 值``train_data[``'Age'``].fillna(train_data[``'Age'``].mean(), inplace``=``True``)``test_data[``'Age'``].fillna(test_data[``'Age'``].mean(),inplace``=``True``)``# 使用票价的均值填充票价中的 nan 值``train_data[``'Fare'``].fillna(train_data[``'Fare'``].mean(), inplace``=``True``)``test_data[``'Fare'``].fillna(test_data[``'Fare'``].mean(),inplace``=``True``)`</span></span><br></pre></td></tr></table></figure>

<p>Cabin 为船舱，有大量的缺失值。在训练集和测试集中的缺失率分别为 77% 和 78%，无法补齐；Embarked 为登陆港口，有少量的缺失值，我们可以把缺失值补齐。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="keyword">print</span>``(train_data[``<span class="string">'Embarked'</span>``].value_counts())`  `<span class="comment"># 运行结果：``S    ``644``C    ``168``Q     ``77`</span></span><br></pre></td></tr></table></figure>

<p>我们发现一共就 3 个登陆港口，其中 S 港口人数最多，占到了 72%，因此我们将其余缺失的 Embarked 数值均设置为 S：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="comment"># 使用登录最多的港口来填充登录港口的 nan 值``train_data[``'Embarked'``].fillna(``'S'``, inplace``=``True``)``test_data[``'Embarked'``].fillna(``'S'``,inplace``=``True``)`</span></span><br></pre></td></tr></table></figure>

<h5 id="模块-3：特征选择"><a href="#模块-3：特征选择" class="headerlink" title="模块 3：特征选择"></a>模块 3：特征选择</h5><p>特征选择是分类器的关键。特征选择不同，得到的分类器也不同。通过数据探索我们发现：</p>
<p>PassengerId 为乘客编号，对分类没有作用，可以放弃；Name 为乘客姓名，对分类没有作用，可以放弃；Cabin 字段缺失值太多，可以放弃；Ticke字段为船票号码，杂乱无章且无规律，可以放弃。</p>
<p>其余的字段包括：Pclass、Sex、Age、SibSp、Parch 和 Fare，这些属性分别表示了乘客的船票等级、性别、年龄、亲戚数量以及船票价格，可能会和乘客的生存预测分类有关系。具体是什什么关系，我们可以交给分类器来处理。</p>
<p>因此我们先将 Pclass、Sex、Age 等这些其余的字段作特征，放到特征向量 features 里。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="comment"># 特征选择``features ``=` `[``'Pclass'``, ``'Sex'``, ``'Age'``, ``'SibSp'``, ``'Parch'``, ``'Fare'``, ``'Embarked'``]``train_features ``=` `train_data[features]``train_labels ``=` `train_data[``'Survived'``]``test_features ``=` `test_data[features]`</span></span><br></pre></td></tr></table></figure>

<p>特征值里有一些是字符串，这样不方便后续的运算，需要转成数值类型，比如 Sex 字段，有 male 和 female 两种取值。我们可以把它变成 Sex=male 和 Sex=female 两个字段，数值用 0 或 1 来表示。</p>
<p>同理 Embarked 有 S、C、Q 三种可能，我们也可以改成 Embarked=S、Embarked=C 和 Embarked=Q 三个字段，数值用 0 或 1 来表示。</p>
<p>那该如何操作呢，我们可以使用 sklearn 特征选择中的 DictVectorizer 类，用它将可以处理符号化的对象，将符号转成数字 0/1 进行表示。具体方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="keyword">from</span>` `sklearn.feature_extraction ``<span class="keyword">import</span>` `DictVectorizer``dvec``=``DictVectorizer(sparse``=``<span class="literal">False</span>``)``train_features``=``dvec.fit_transform(train_features.to_dict(orient``=``<span class="string">'record'</span>``))`</span><br></pre></td></tr></table></figure>

<p>你会看到代码中使用了 fit_transform 这个函数，它可以将特征向量转化为特征值矩阵。然后我们看下 dvec 在转化后的特征属性是怎样的，即查看 dvec 的 feature_names_ 属性值，方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="keyword">print</span>``(dvec.feature_names_)`  `<span class="comment"># 运行结果``[``'Age'``, ``'Embarked=C'``, ``'Embarked=Q'``, ``'Embarked=S'``, ``'Fare'``, ``'Parch'``, ``'Pclass'``, ``'Sex=female'``, ``'Sex=male'``, ``'SibSp'``]`</span></span><br></pre></td></tr></table></figure>

<p>这样 train_features 特征矩阵就包括 10 个特征值（列），以及 891 个样本（行），即 891 行，10 列的特征矩阵。</p>
<h5 id="模块-4：决策树模型"><a href="#模块-4：决策树模型" class="headerlink" title="模块 4：决策树模型"></a>模块 4：决策树模型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="keyword">from</span>` `sklearn.tree ``<span class="keyword">import</span>` `DecisionTreeClassifier``<span class="comment"># 构造 ID3 决策树``clf ``=` `DecisionTreeClassifier(criterion``=``'entropy'``)``# 决策树训练``clf.fit(train_features, train_labels)`</span></span><br></pre></td></tr></table></figure>

<h5 id="模块-5：模型预测-amp-评估"><a href="#模块-5：模型预测-amp-评估" class="headerlink" title="模块 5：模型预测 &amp; 评估"></a>模块 5：模型预测 &amp; 评估</h5><p>在预测中，我们首先需要得到测试集的特征值矩阵，然后使用训练好的决策树 clf 进行预测，得到预测结果 pred_labes：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`test_features``=``dvec.transform(test_features.to_dict(orient``=``<span class="string">'record'</span>``))``<span class="comment"># 决策树预测``pred_labels ``=` `clf.predict(test_features)`</span></span><br></pre></td></tr></table></figure>

<p>在模型评估中，决策树提供了 score 函数可以直接得到准确率，但是我们并不知道真实的预测结果，所以无法用预测值和真实的预测结果做比较（test_labels, test_predict）。</p>
<p>我们只能使用训练集中的数据进行模型评估（不是比较了），可以使用决策树自带的 score 函数计算下得到的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="comment"># 得到决策树准确率``acc_decision_tree ``=` `round``(clf.score(train_features, train_labels), ``6``)``print``(u``'score 准确率为 %.4lf'` `%` `acc_decision_tree)`  `# 运行结果：``score 准确率为 ``0.9820`</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>注：我在构造特征向量时使用了 DictVectorizer 类，使用 fit_transform 函数将特征向量转化为特征值矩阵。DictVectorizer 类同时也提供 transform 函数，这两个函数区别：</p>
<p>fit:从一个训练集中学习模型参数，其中就包括了归一化时用到的均值，标准偏差等，可以理解为一个训练过程。<br>transform: 在fit的基础上，对数据进行标准化，降维，归一化等数据转换操作<br>fit_transform: 将模型训练和转化合并到一起，训练样本先做fit，得到mean，standard deviation，然后将这些参数用于transform（归一化训练数据），使得到的训练数据是归一化的，而测试数据只需要在原先fit得到的mean，std上来做归一化就行了，所以用transform就行了。　</p>
</blockquote>
<p>你会发现你刚用训练集做训练，再用训练集自身做准确率评估自然会很高。但这样得出的准确率并不能代表决策树分类器的准确率。</p>
<p>那么有什么办法，来统计决策树分类器的准确率呢？</p>
<p>这里可以使用 <strong>K 折交叉验证</strong>的方式，交叉验证是一种常用的验证分类准确率的方法，原理是拿出大部分样本进行训练，少量的用于分类器的验证。K 折交叉验证，就是做 K 次交叉验证，每次选取K 分之一的数据作为验证，其余作为训练。轮流 K 次，取平均值。</p>
<p>K 折交叉验证的原理是这样的：</p>
<ol>
<li>将数据集平均分割成 K 个等份；</li>
<li>使用 1 份数据作为测试数据，其余作为训练数据；</li>
<li>计算测试准确率；</li>
<li>使用不同的测试集，重复 2、3 步骤。</li>
</ol>
<p>在 sklearn 的 model_selection 模型选择中提供了 cross_val_score 函数。cross_val_score 函数中的参数 cv 代表对原始数据划分成多少份，也就是我们的 K 值，一般建议 K 值取 10，因此我们可以设置 CV=10，我们可以对比下 score和 cross_val_score 两种函数的正确率的评估结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="keyword">import</span>` `numpy <span class="keyword">as</span> np``<span class="keyword">from</span>` `sklearn.model_selection ``<span class="keyword">import</span>` `cross_val_score``<span class="comment"># 使用 K 折交叉验证 统计决策树准确率``print``(u``'cross_val_score 准确率为 %.4lf'` `%` `np.mean(cross_val_score(clf, train_features, train_labels, cv``=``10``)))`  `# 运行结果：``cross_val_score 准确率为 ``0.7835`</span></span><br></pre></td></tr></table></figure>

<h5 id="模块-6：决策树可视化"><a href="#模块-6：决策树可视化" class="headerlink" title="模块 6：决策树可视化"></a>模块 6：决策树可视化</h5><p>Graphviz 可视化工具</p>
<h5 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">'./Titanic_Data-master/train.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'./Titanic_Data-master/test.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据探索</span></span><br><span class="line">print(train_data.info())</span><br><span class="line">print(train_data.describe())</span><br><span class="line">print(train_data.describe(include=[<span class="string">'O'</span>]))</span><br><span class="line">print(train_data.head())</span><br><span class="line">print(train_data.tail())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据清洗</span></span><br><span class="line"><span class="comment"># 使用平均年龄来填充年龄中的Nan值</span></span><br><span class="line">train_data[<span class="string">'Age'</span>].fillna(train_data[<span class="string">'Age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">'Age'</span>].fillna(test_data[<span class="string">'Age'</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 使用票价的均值填充票价中的Nan值</span></span><br><span class="line">train_data[<span class="string">'Fare'</span>].fillna(train_data[<span class="string">'Fare'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">'Fare'</span>].fillna(test_data[<span class="string">'Fare'</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用登录最多的港口来填充登录港口的nan值</span></span><br><span class="line"><span class="comment"># print(train_data['Embarked'].value_counts())</span></span><br><span class="line">train_data[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">'Embarked'</span>].fillna(<span class="string">'S'</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征选择</span></span><br><span class="line">features = [<span class="string">'Pclass'</span>, <span class="string">'Sex'</span>, <span class="string">'Age'</span>, <span class="string">'SibSp'</span>, <span class="string">'Parch'</span>, <span class="string">'Fare'</span>, <span class="string">'Embarked'</span>]</span><br><span class="line">train_features = train_data[features]</span><br><span class="line">train_labels = train_data[<span class="string">'Survived'</span>]</span><br><span class="line">test_features = test_data[features]</span><br><span class="line"></span><br><span class="line">dvec = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">train_features = dvec.fit_transform(train_features.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造ID3决策树</span></span><br><span class="line">clf = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树训练</span></span><br><span class="line">clf.fit(train_features, train_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 模型预测 &amp; 评估</span></span><br><span class="line"><span class="comment"># test_features=dvec.transform(test_features.to_dict(orient='record'))</span></span><br><span class="line"><span class="comment"># # 决策树预测</span></span><br><span class="line"><span class="comment"># pred_labels = clf.predict(test_features)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到决策树准确率</span></span><br><span class="line">acc_decision_tree = round(clf.score(train_features, train_labels), <span class="number">6</span>)</span><br><span class="line">print(<span class="string">u'score准确率为 %.4lf'</span> % acc_decision_tree)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用K折交叉验证 统计决策树准确率</span></span><br><span class="line">print(<span class="string">u'cross_val_score准确率为 %.4lf'</span> % np.mean(cross_val_score(clf, train_features, train_labels, cv=<span class="number">10</span>)))</span><br></pre></td></tr></table></figure>

<h4 id="决策树模型使用技巧总结"><a href="#决策树模型使用技巧总结" class="headerlink" title="决策树模型使用技巧总结"></a>决策树模型使用技巧总结</h4><p>今天用泰坦尼克乘客生存预测案例把决策树模型的流程跑了一遍。在实战中，你需要注意一下几点：</p>
<ol>
<li>特征选择是分类模型好坏的关键。选择什么样的特征，以及对应的特征值矩阵，决定了分类模型的好坏。通常情况下，特征值不都是数值类型，可以使用 DictVectorizer 类进行转化；</li>
<li>模型准确率需要考虑是否有测试集的实际结果可以做对比，当测试集没有真实结果可以对比时，需要使用 K 折交叉验证 cross_val_score；</li>
<li>Graphviz 可视化工具可以很方便地将决策模型呈现出来，帮助你更好理解决策树的构建。</li>
</ol>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/83.png">

<h2 id="17-朴素贝叶斯分类"><a href="#17-朴素贝叶斯分类" class="headerlink" title="17.朴素贝叶斯分类"></a>17.朴素贝叶斯分类</h2><p>贝叶斯原理跟我们的生活联系非常紧密。举个例子，如果你看到一个人总是花钱，那么会推断这个人多半是个有钱人。当然这也不是绝对，也就是说，当你不能准确预知一个事物本质的时候，你可以依靠和事物本质相关的事件来进行判断，如果事情发生的频次多，则证明这个属性更有可能存在。 </p>
<h3 id="贝叶斯原理"><a href="#贝叶斯原理" class="headerlink" title="贝叶斯原理"></a>贝叶斯原理</h3><p>贝叶斯原理是怎么来的呢？贝叶斯为了解决一个叫“逆向概率”问题写了一篇文章，尝试解答在没有太多可靠证据的情况下，怎样做出更符合数学逻辑的推测。</p>
<p><strong>逆向概率</strong></p>
<p>所谓“逆向概率”是相对“正向概率”而言。正向概率的问题很容易理解，比如我们已经知道袋子里面有 N 个球，不是黑球就是白球，其中 M 个是黑球，那么把手伸进去摸一个球，就能知道摸出黑球的概率是多少。但这种情况往往是上帝视角，即了解了事情的全貌再做判断。</p>
<p>在现实生活中，我们很难知道事情的全貌。贝叶斯则从实际场景出发，提了一个问题：如果我们事先不知道袋子里面黑球和白球的比例，而是通过我们摸出来的球的颜色，能判断出袋子里面黑白球的比例么？</p>
<p>正是这样的一个问题，影响了接下来近 200 年的统计学理论。这是因为，贝叶斯原理与其他统计学推断方法截然不同，它是建立在主观判断的基础上：在我们不了解所有客观事实的情况下，同样可以先估计一个值，然后根据实际结果不断进行修正。</p>
<p>我们用一个题目来体会下：假设有一种病叫做“贝叶死”，它的发病率是万分之一，即 10000 人中会有 1 个人得病。现有一种测试可以检验一个人是否得病的准确率是 99.9%，它的误报率是 0.1%，那么现在的问题是，如果一个人被查出来患有“叶贝死”，实际上患有的可能性有多大？</p>
<p>你可能会想说，既然查出患有“贝叶死”的准确率是 99.9%，那是不是实际上患“贝叶死”的概率也是 99.9% 呢？实际上不是的。你自己想想，在 10000 个人中，还存在 0.1% 的误查的情况，也就是 10 个人没有患病但是被诊断成阳性。当然 10000 个人中，也确实存在一个患有贝叶死的人，他有 99.9% 的概率被检查出来。所以你可以粗算下，患病的这个人实际上是这 11 个人里面的一员，即实际患病比例是 1/11≈9%。</p>
<p>上面这个例子中，实际上涉及到了贝叶斯原理中的几个概念：</p>
<p><strong>先验概率</strong></p>
<p>通过经验来判断事情发生的概率，比如说“贝叶死”的发病率是万分之一，就是先验概率。再比如南方的梅雨季是 6-7 月，就是通过往年的气候总结出来的经验，这个时候下雨的概率就比其他时间高出很多。</p>
<p><strong>后验概率</strong></p>
<p>后验概率就是发生结果之后，推测原因的概率。比如说某人查出来了患有“贝叶死”，那么患病的原因可能是 A、B 或 C。患有“贝叶死”是因为原因 A 的概率就是后验概率。它是属于条件概率的一种。</p>
<p><strong>条件概率</strong></p>
<p>事件 A 在另外一个事件 B 已经发生条件下的发生概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”。比如原因 A 的条件下，患有“贝叶死”的概率，就是条件概率。</p>
<p><strong>似然函数（likelihood function）</strong></p>
<p>你可以把概率模型的训练过程理解为求参数估计的过程。举个例子，如果一个硬币在 10 次抛落中正面均朝上。那么你肯定在想，这个硬币是均匀的可能性是多少？这里硬币均匀就是个参数，似然函数就是用来衡量这个模型的参数。似然在这里就是可能性的意思，它是关于统计参数的函数。</p>
<br>

<p>介绍完贝叶斯原理中的这几个概念，我们再来看下贝叶斯原理，实际上贝叶斯原理就是求解后验概率，我们假设：A 表示事件 “测出为阳性”, 用 B1 表示“患有贝叶死”, B2 表示“没有患贝叶死”。根据上面那道题，我们可以得到下面的信息。</p>
<p>患有贝叶死的情况下，测出为阳性的概率为 P(A|B1)=99.9%，没有患贝叶死，但测出为阳性的概率为 P(A|B2)=0.1%。另外患有贝叶死的概率为 P(B1)=0.01%，没有患贝叶死的概率 P(B2)=99.99%。</p>
<p>那么我们检测出来为阳性，而且是贝叶死的概率 P(B1A）=P(B1)<em>P(A|B1)=0.01%</em>99.9%=0.00999%。</p>
<p>这里 P(B1A) 代表的是联合概率，同样我们可以求得 P(B2A)=P(B2)<em>P(A|B2)=99.99%</em>0.1%=0.09999%。</p>
<p>然后我们想求得是检查为阳性的情况下，患有贝叶死的概率，也即是 P(B1|A)。</p>
<p>所以检查出阳性，且患有贝叶死的概率为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/84.png" style="zoom:50%;">

<p>检查出是阳性，但没有患有贝叶死的概率为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/85.png" style="zoom:50%;">

<p>这里我们能看出来 0.01%+0.1% 均出现在了 P(B1|A) 和 P(B2|A) 的计算中作为分母。我们把它称之为论据因子，也相当于一个权值因子。</p>
<p>其中 P(B1）、P(B2) 就是先验概率，我们现在知道了观测值，就是被检测出来是阳性，来求患贝叶死的概率，也就是求后验概率。求后验概率就是贝叶斯原理要求的，基于刚才求得的 P(B1|A)，P(B2|A)，我们可以总结出贝叶斯公式为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/87.png" style="zoom:50%;">

<p>由此，我们可以得出通用的<strong>贝叶斯公式</strong>：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/88.png" style="zoom:50%;">

<h4 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a><strong>朴素贝叶斯</strong></h4><p>朴素贝叶斯，是一种简单但极为强大的预测建模算法 。之所以称为朴素贝叶斯，是因为它假设每个输入变量是独立的。这是一个强硬的假设，实际情况并不一定，但是这项技术对于绝大部分的复杂问题仍然非常有效。</p>
<p>朴素贝叶斯模型由两种类型的概率组成：</p>
<ul>
<li><p>每个类别的概率 <strong>P(Cj)</strong></p>
</li>
<li><p>每个属性的条件概率 <strong>P(Ai|Cj)</strong></p>
</li>
</ul>
<p>举个例子说明下什么是类别概率和条件概率。假设我有 7 个棋子，其中 3 个是白色的，4 个是黑色的。那么棋子是白色的概率就是 3/7，黑色的概率就是 4/7，这个就是类别概率。</p>
<p>假设我把这 7 个棋子放到了两个盒子里，其中盒子 A 里面有 2 个白棋，2 个黑棋；盒子 B 里面有 1 个白棋，2 个黑棋。那么在盒子 A 中抓到白棋的概率就是 1/2，抓到黑棋的概率也是 1/2，这个就是条件概率，也就是在某个条件（比如在盒子 A 中）下的概率。</p>
<p>在朴素贝叶斯中，我们要统计的是<strong>属性的条件概率</strong>，也就是假设取出来的是白色的棋子，那么它属于盒子 A 的概率是 2/3。</p>
<p>为了训练朴素贝叶斯模型，我们需要先给出训练数据，以及这些数据对应的分类。那么上面这两个概率，也就是类别概率和条件概率。他们都可以从给出的训练数据中计算出来。一旦计算出来，概率模型就可以使用贝叶斯原理对新数据进行预测。</p>
<p>另外，贝叶斯原理、贝叶斯分类和朴素贝叶斯这三者之间是有区别的。</p>
<p>贝叶斯原理是最大的概念，它解决了概率论中“逆向概率”的问题，在这个理论基础上，人们设计出了贝叶斯分类器，朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。朴素贝叶斯之所以朴素是因为它假设属性是相互独立的，因此对实际情况有所约束，如果属性之间存在关联，分类准确率会降低。不过好在对于大部分情况下，朴素贝叶斯的分类效果都不错。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/89.jpg" style="zoom: 25%;">

<h4 id="朴素贝叶斯分类工作原理"><a href="#朴素贝叶斯分类工作原理" class="headerlink" title="朴素贝叶斯分类工作原理"></a>朴素贝叶斯分类工作原理</h4><p>朴素贝叶斯分类是常用的贝叶斯分类方法。我们日常生活中看到一个陌生人，要做的第一件事情就是判断 TA 的性别，判断性别的过程就是一个分类的过程。根据以往的经验，我们通常会从身高、体重、鞋码、头发长短、服饰、声音等角度进行判断。这里的“经验”就是一个训练好的关于性别判断的模型，其训练数据是日常中遇到的各式各样的人，以及这些人实际的性别数据。</p>
<h5 id="离散数据案例"><a href="#离散数据案例" class="headerlink" title="离散数据案例"></a>离散数据案例</h5><p>我们遇到的数据可以分为两种，一种是离散数据，另一种是连续数据。</p>
<p>我以下面的数据为例，这些是根据你之前的经验所获得的数据。然后给你一个新的数据：身高“高”、体重“中”，鞋码“中”，请问这个人是男还是女？</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/90.png" style="zoom: 90%;">

<p>针对这个问题，我们先确定一共有 3 个属性，假设我们用 A 代表属性，用 A1, A2, A3 分别为身高 = 高、体重 = 中、鞋码 = 中。一共有两个类别，假设用 C 代表类别，那么 C1,C2 分别是：男、女，在未知的情况下我们用 Cj 表示。</p>
<p>那么我们想求在 A1、A2、A3 属性下，Cj 的概率，用条件概率表示就是 P(Cj|A1A2A3)。根据上面讲的贝叶斯的公式，我们可以得出：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/91.png" style="zoom:50%;">

<p>因为一共有 2 个类别，所以我们只需要求得 P(C1|A1A2A3) 和 P(C2|A1A2A3) 的概率即可，然后比较下哪个分类的可能性大，就是哪个分类结果。</p>
<p>在这个公式里，因为 P(A1A2A3) 都是固定的，我们想要寻找使得 P(Cj|A1A2A3) 的最大值，就等价于求 P(A1A2A3|Cj)P(Cj) 最大值。</p>
<p>我们假定 Ai 之间是相互独立的，那么： P(A1A2A3|Cj)=P(A1|Cj)P(A2|Cj)P(A3|Cj)</p>
<p>然后我们需要从 Ai 和 Cj 中计算出 P(Ai|Cj) 的概率，带入到上面的公式得出 P(A1A2A3|Cj)，最后找到使得 P(A1A2A3|Cj) 最大的类别 Cj。</p>
<p>我分别求下这些条件下的概率：</p>
<p>P(A1|C1)=1/2, P(A2|C1)=1/2, P(A3|C1)=1/4，P(A1|C2)=0, P(A2|C2)=1/2, P(A3|C2)=1/2，所以 P(A1A2A3|C1)=1/16, P(A1A2A3|C2)=0。</p>
<p>因为 P(A1A2A3|C1)P(C1)&gt;P(A1A2A3|C2)P(C2)，所以应该是 C1 类别，即男性。</p>
<h5 id="连续数据案例"><a href="#连续数据案例" class="headerlink" title="连续数据案例"></a>连续数据案例</h5><p>实际生活中我们得到的是连续的数值，比如下面这组数据：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/92.png" style="zoom:90%;">

<p>那么如果给你一个新的数据，身高 180、体重 120，鞋码 41，请问该人是男是女呢？</p>
<p>公式还是上面的公式，这里的困难在于，由于身高、体重、鞋码都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办呢？</p>
<p>这时，可以假设男性和女性的身高、体重、鞋码都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。比如，男性的身高是均值 179.5、标准差为 3.697 的正态分布。所以男性的身高为 180 的概率为 0.1069。怎么计算得出的呢? 你可以使用 EXCEL 的 NORMDIST(x,mean,standard_dev,cumulative) 函数，一共有 4 个参数：</p>
<p>x：正态分布中，需要计算的数值；</p>
<p>Mean：正态分布的平均值；</p>
<p>Standard_dev：正态分布的标准差；</p>
<p>Cumulative：取值为逻辑值，即 False 或 True。它决定了函数的形式。当为 TRUE 时，函数结果为累积分布；为 False 时，函数结果为概率密度。</p>
<p>这里我们使用的是 NORMDIST(180,179.5,3.697,0)=0.1069。</p>
<p>同理我们可以计算得出男性体重为 120 的概率为 0.000382324，男性鞋码为 41 号的概率为 0.120304111。</p>
<p>所以我们可以计算得出：</p>
<p>P(A1A2A3|C1)=P(A1|C1)P(A2|C1)P(A3|C1)=0.1069<em>0.000382324</em> 0.120304111=4.9169e-6</p>
<p>同理我们也可以计算出来该人为女的可能性：</p>
<p>P(A1A2A3|C2)=P(A1|C2)P(A2|C2)P(A3|C2)=0.00000147489* 0.015354144* 0.120306074=2.7244e-9</p>
<p>很明显这组数据分类为男的概率大于分类为女的概率。</p>
<p>当然在 Python 中，有第三方库可以直接帮我们进行上面的操作，这个我们会在下文中介绍。这里主要是给你讲解下具体的运算原理。</p>
<h3 id="朴素贝叶斯分类器工作流程"><a href="#朴素贝叶斯分类器工作流程" class="headerlink" title="朴素贝叶斯分类器工作流程"></a>朴素贝叶斯分类器工作流程</h3><p>朴素贝叶斯分类常用于<strong>文本</strong>分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。</p>
<p>朴素贝叶斯分类器需要三个流程。</p>
<p><strong>第一阶段：准备阶段</strong></p>
<p>在这个阶段我们需要<strong>确定特征属性</strong>，比如上面案例中的“身高”、“体重”、“鞋码”等，并对每个特征属性进行适当<strong>划分</strong>，然后由人工对一部分数据进行分类，形成训练样本。</p>
<p>这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。</p>
<p><strong>第二阶段：训练阶段</strong></p>
<p>这个阶段就是<strong>生成分类器</strong>，主要工作是<strong>计算</strong>每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率。</p>
<p>输入是特征属性和训练样本，输出是分类器。</p>
<p><strong>第三阶段：应用阶段</strong></p>
<p>这个阶段是使用分类器对新数据进行<strong>分类</strong>。</p>
<p>输入是分类器和新数据，输出是新数据的分类结果。 </p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/94.png" style="zoom:50%;">

<h3 id="sklearn-机器学习包"><a href="#sklearn-机器学习包" class="headerlink" title="sklearn 机器学习包"></a>sklearn 机器学习包</h3><p>接下来带你一起使用朴素贝叶斯做下文档分类的项目，最重要的工具就是 sklearn 这个机器学习神器。</p>
<p>sklearn 的全称叫 Scikit-learn，它给我们提供了 3 个朴素贝叶斯分类算法，分别是高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）。 </p>
<p>这三种算法适合应用在不同的场景下，我们应该根据特征变量的不同选择不同的算法：</p>
<p><strong>高斯朴素贝叶斯</strong> ：特征变量是<strong>连续</strong>变量，符合高斯分布，比如说人的身高，物体的长度。</p>
<p><strong>多项式朴素贝叶斯</strong> ：特征变量是<strong>离散</strong>变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。</p>
<p><strong>伯努利朴素贝叶斯</strong> ：特征变量是<strong>布尔</strong>变量，符合 0/1 分布，在文档分类中特征是单词是否出现。</p>
<p>伯努利朴素贝叶斯是以文件为粒度，如果该单词在某文件中出现了即为 1，否则为 0。而多项式朴素贝叶斯是以单词为粒度，会计算在某个文件中的具体次数。而高斯朴素贝叶斯适合处理特征变量是连续变量，且符合正态分布（高斯分布）的情况。比如身高、体重这种自然界的现象就比较适合用高斯朴素贝叶斯来处理。而文本分类是使用多项式朴素贝叶斯或者伯努利朴素贝叶斯。</p>
<p><strong>TF-IDF 值</strong>：</p>
<p>TF-IDF 是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。</p>
<p>TF-IDF 实际上是两个词组 Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF，分别代表了词频和逆向文档频率。</p>
<p>词频 TF 计算了一个单词<strong>在文档中出现的次数</strong>，它认为一个单词的重要性和它在文档中出现的次数呈正比。</p>
<p>逆向文档频率 IDF ，是指一个<strong>单词在文档中的区分度</strong>。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF 越大就代表该单词的区分度越大。</p>
<p>所以 TF-IDF 实际上是词频 TF 和逆向文档频率 IDF 的乘积 。这样我们倾向于找到 TF 和 IDF 取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。</p>
<p>TF-IDF 如何计算</p>
<p>首先我们看下词频 TF 和逆向文档概率 IDF 的公式。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/93.png" style="zoom:50%;">

<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/97.png" style="zoom:50%;">

<p>TF-IDF=TF*IDF</p>
<p>（ IDF 为了避免分母为 0，统一给单词出现的文档数都加 1）</p>
<p>举个例子。假设一个文件夹里一共有 10 篇文档，其中一篇文档有 1000 个单词，“this”这个单词出现 20 次，“bayes”出现了 5 次。“this”在所有文档中均出现过，而“bayes”只在 2 篇文档中出现过。我们来计算一下这两个词语的 TF-IDF 值。</p>
<p>针对“this”，计算 TF-IDF 值：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/98.png" style="zoom:50%;">

<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/99.png" style="zoom:50%;">

<p>所以 TF-IDF=0.02*(-0.0414)=-8.28e-4。</p>
<p>针对“bayes”，计算 TF-IDF 值：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/100.png" style="zoom:50%;">

<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/101.png" style="zoom:50%;">

<p>很明显“bayes”的 TF-IDF 值要大于“this”的 TF-IDF 值。这就说明用“bayes”这个单词做区分比单词“this”要好。</p>
<h4 id="如何求-TF-IDF"><a href="#如何求-TF-IDF" class="headerlink" title="如何求 TF-IDF"></a>如何求 TF-IDF</h4><p>在 sklearn 中我们直接使用 <strong>TfidfVectorizer</strong> 类，它可以帮我们计算单词 TF-IDF 向量的值。在这个类中，取 sklearn 计算的对数 log 时，底数是 e，不是 10。</p>
<p>下面我来讲下如何创建 TfidfVectorizer 类。</p>
<p>TfidfVectorizer 类的创建：</p>
<p>创建 TfidfVectorizer 的方法是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)</span><br></pre></td></tr></table></figure>

<p>我们在创建的时候，有两个构造参数，可以自定义<strong>停用词 stop_words</strong> 和<strong>规律规则 token_pattern</strong>。需要注意的是传递的数据结构，停用词 stop_words 是一个列表 List 类型，而过滤规则 token_pattern 是正则表达式。</p>
<p>什么是停用词？停用词就是在分类中没有用的词，这些词一般词频 TF 高，但是 IDF 很低，起不到分类的作用。为了节省空间和计算时间，我们把这些词作为停用词 stop words，告诉机器这些词不需要帮我计算。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/102.png">

<p>当我们创建好 TF-IDF 向量类型时，可以用 <strong>fit_transform</strong> 帮我们计算，返回给我们文本矩阵，该矩阵表示了每个单词在每个文档中的 TF-IDF 值。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/103.png">

<p>在我们进行 fit_transform 拟合模型后，我们可以得到更多的 TF-IDF 向量属性，比如，我们可以得到词汇的对应关系（字典类型）和向量的 IDF 值，当然也可以获取设置的停用词 stop_words。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/104.png">

<br>

<p>举个例子，假设我们有 4 个文档：</p>
<p>文档 1：this is the bayes document；</p>
<p>文档 2：this is the second second document；</p>
<p>文档 3：and the third one；</p>
<p>文档 4：is this the document。</p>
<p>现在想要计算文档里都有哪些单词，这些单词在不同文档中的 TF-IDF 值是多少呢？</p>
<p>首先我们创建 TfidfVectorizer 类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">tfidf_vec = TfidfVectorizer()</span><br></pre></td></tr></table></figure>

<p>然后我们创建 4 个文档的列表 documents，并让创建好的 tfidf_vec 对 documents 进行拟合，得到 TF-IDF 矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> documents = [</span><br><span class="line"> <span class="string">'this is the bayes document'</span>,</span><br><span class="line"> <span class="string">'this is the second second document'</span>,</span><br><span class="line"> <span class="string">'and the third one'</span>,</span><br><span class="line"> <span class="string">'is this the document'</span></span><br><span class="line">]</span><br><span class="line"> tfidf_matrix = tfidf_vec.fit_transform(documents)</span><br></pre></td></tr></table></figure>

<p>输出文档中所有不重复的词：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'不重复的词:'</span>, tfidf_vec.get_feature_names())</span><br></pre></td></tr></table></figure>

<p>运行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">不重复的词: [&apos;and&apos;, &apos;bayes&apos;, &apos;document&apos;, &apos;is&apos;, &apos;one&apos;, &apos;second&apos;, &apos;the&apos;, &apos;third&apos;, &apos;this&apos;]</span><br></pre></td></tr></table></figure>

<p>输出每个单词对应的 id 值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'每个单词的 ID:'</span>, tfidf_vec.vocabulary_)</span><br></pre></td></tr></table></figure>

<p>运行结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">每个单词的 ID: &#123;&apos;this&apos;: 8, &apos;is&apos;: 3, &apos;the&apos;: 6, &apos;bayes&apos;: 1, &apos;document&apos;: 2, &apos;second&apos;: 5, &apos;and&apos;: 0, &apos;third&apos;: 7, &apos;one&apos;: 4&#125;</span><br></pre></td></tr></table></figure>

<p>输出每个单词在每个文档中的 TF-IDF 值，向量里的顺序是按照词语的 id 顺序来的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'每个单词的 tfidf 值:'</span>, tfidf_matrix.toarray())</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> 每个单词的 tfidf 值: [[0. 0.63314609 0.40412895 0.40412895 0. 0.</span><br><span class="line"> 0.33040189 0. 0.40412895]</span><br><span class="line"> [0. 0. 0.27230147 0.27230147 0. 0.85322574</span><br><span class="line">0.22262429 0. 0.27230147]</span><br><span class="line"> [0.55280532 0. 0. 0. 0.55280532 0.</span><br><span class="line"> 0.28847675 0.55280532 0. ]</span><br><span class="line"> [0. 0. 0.52210862 0.52210862 0. 0.</span><br><span class="line"> 0.42685801 0. 0.52210862]]</span><br></pre></td></tr></table></figure>

<h3 id="如何对文档进行分类"><a href="#如何对文档进行分类" class="headerlink" title="如何对文档进行分类"></a>如何对文档进行分类</h3><p>如果我们要对文档进行分类，有两个重要的阶段：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/95.png">

<ol>
<li><p>基于分词的数据准备 ，包括分词、单词权重计算、去掉停用词；</p>
</li>
<li><p>应用朴素贝叶斯分类进行分类 ，首先通过训练集得到朴素贝叶斯分类器，然后将分类器应用于测试集，并与实际结果做对比，最终得到测试集的分类准确率。</p>
</li>
</ol>
<p>下面，我分别对这些模块进行介绍。</p>
<p><strong>模块 1：对文档进行分词</strong></p>
<p>在准备阶段里，最重要的就是分词。那么如果给文档进行分词呢？英文文档和中文文档所使用的分词工具不同。</p>
<p>在英文文档中，最常用的是 NTLK 包。NTLK 包中包含了英文的停用词 stop words、分词和标注方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">word_list = nltk.word_tokenize(text) <span class="comment"># 分词</span></span><br><span class="line">nltk.pos_tag(word_list) <span class="comment"># 标注单词的词性</span></span><br></pre></td></tr></table></figure>

<p>在中文文档中，最常用的是 jieba 包。jieba 包中包含了中文的停用词 stop words 和分词方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">word_list = jieba.cut (text) <span class="comment"># 中文分词</span></span><br></pre></td></tr></table></figure>

<p><strong>模块 2：加载停用词表</strong></p>
<p>我们需要自己读取停用词表文件，从网上可以找到中文常用的停用词保存在 stop_words.txt，然后利用 Python 的文件读取函数读取文件，保存在 stop_words 数组中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop_words = [line.strip().decode(<span class="string">'utf-8'</span>) <span class="keyword">for</span> line <span class="keyword">in</span> io.open(<span class="string">'stop_words.txt'</span>).readlines()]</span><br></pre></td></tr></table></figure>

<p><strong>模块 3：计算单词的权重</strong></p>
<p>这里我们用到 sklearn 里的 TfidfVectorizer 类，上面我们介绍过它使用的方法。</p>
<p>直接创建 TfidfVectorizer 类，然后使用 fit_transform 方法进行拟合，得到 TF-IDF 特征空间 features，你可以理解为选出来的分词就是特征。我们计算这些特征在文档上的特征向量，得到特征空间 features。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf = TfidfVectorizer(stop_words=stop_words, max_df=<span class="number">0.5</span>)</span><br><span class="line">features = tf.fit_transform(train_contents)</span><br></pre></td></tr></table></figure>

<p>这里 max_df 参数用来描述单词在文档中的最高出现率。假设 max_df=0.5，代表一个单词在 50% 的文档中都出现过了，那么它只携带了非常少的信息，因此就不作为分词统计。</p>
<p>一般很少设置 min_df，因为 min_df 通常都会很小。</p>
<p><strong>模块 4：生成朴素贝叶斯分类器</strong></p>
<p>我们将特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 传递给贝叶斯分类器 clf，它会自动生成一个符合特征空间和对应分类的分类器。</p>
<p>这里我们采用的是多项式贝叶斯分类器，其中 alpha 为平滑参数。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为 0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为 0。为了解决这个问题，我们需要做平滑处理。</p>
<p>当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。</p>
<p>当 0&lt;alpha&lt;1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。</p>
<p># 多项式贝叶斯分类器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">clf = MultinomialNB(alpha=<span class="number">0.001</span>).fit(train_features, train_labels)</span><br></pre></td></tr></table></figure>

<p><strong>模块 5：使用生成的分类器做预测</strong></p>
<p>首先我们需要得到测试集的特征矩阵。</p>
<p>方法是用训练集的分词创建一个 TfidfVectorizer 类，使用同样的 stop_words 和 max_df，然后用这个 TfidfVectorizer 类对测试集的内容进行 fit_transform 拟合，得到测试集的特征矩阵 test_features。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_tf = TfidfVectorizer(stop_words=stop_words, max_df=<span class="number">0.5</span>, vocabulary=train_vocabulary)</span><br><span class="line">test_features=test_tf.fit_transform(test_contents)</span><br></pre></td></tr></table></figure>

<p>然后我们用训练好的分类器对新数据做预测。</p>
<p>方法是使用 predict 函数，传入测试集的特征矩阵 test_features，得到分类结果 predicted_labels。predict 函数做的工作就是求解所有后验概率并找出最大的那个。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predicted_labels=clf.predict(test_features)</span><br></pre></td></tr></table></figure>

<p><strong>模块 6：计算准确率</strong></p>
<p>计算准确率实际上是对分类模型的评估。我们可以调用 sklearn 中的 metrics 包，在 metrics 中提供了 accuracy_score 函数，方便我们对实际结果和预测的结果做对比，给出模型的准确率。</p>
<p>使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">print</span> metrics.accuracy_score(test_labels, predicted_labels)</span><br></pre></td></tr></table></figure>

<br>

<p>数据挖掘神器sklearn</p>
<p>从数据挖掘的流程来看，一般包括了获取数据、数据清洗、模型训练、模型评估和模型部署这几个过程。</p>
<p>sklearn中包含了大量的数据挖掘算法，比如三种朴素贝叶斯算法，我们只需要了解不同算法的适用条件，以及创建时所需的参数，就可以用模型帮我们进行训练。在模型评估中 klearn提供了metrics包，帮我们对预测结果与实际结果进行评估。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/96.png">

<h2 id="18-SVM分类"><a href="#18-SVM分类" class="headerlink" title="18.SVM分类"></a>18.SVM分类</h2><p>SVM 的英文叫 Support Vector Machine，中文名为<strong>支持向量机</strong>。它是常见的一种分类方法，在机器学习中，SVM 是<strong>有监督</strong>的学习模型。</p>
<p>什么是有监督的学习模型呢？它指的是我们需要事先对数据打上分类标签，这样机器就知道这个数据属于哪个分类。同样无监督学习，就是数据没有被打上分类标签，这可能是因为我们不具备先验的知识，或者打标签的成本很高。所以我们需要机器代我们部分完成这个工作，比如将数据进行聚类，方便后续人工对每个类进行分析。SVM 作为有监督的学习模型，通常可以帮我们模式识别、分类以及回归分析。 </p>
<h3 id="一、SVM-的工作原理"><a href="#一、SVM-的工作原理" class="headerlink" title="一、SVM 的工作原理"></a>一、SVM 的工作原理</h3><p>用 SVM 计算的过程就是帮我们找到一个超平面，能够将样本区分的过程，这个超平面就是我们的 SVM 分类器。</p>
<p>比如下图所示的直线 A、直线 B 和直线 C，究竟哪种才是更好的划分呢？</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/105.jpeg" alt="105" style="zoom: 25%;">

<p>很明显图中的直线 B 更靠近蓝色球，但是在真实环境下，球再多一些的话，蓝色球可能就被划分到了直线 B 的右侧，被认为是红色球。同样直线 A 更靠近红色球，在真实环境下，如果红色球再多一些，也可能会被误认为是蓝色球。所以相比于直线 A 和直线 B，直线 C 的划分更优，因为它的鲁棒性更强。</p>
<p>那怎样才能寻找到直线 C 这个更优的答案呢？这里，我们引入一个 SVM 特有的概念： <strong>分类间隔</strong>。</p>
<p>实际上，我们的分类环境不是在二维平面中的，而是在多维空间中，这样直线 C 就变成了决策面 C。</p>
<p>在保证决策面不变，且分类不产生错误的情况下，我们可以移动决策面 C，直到产生两个极限的位置：如图中的决策面 A 和决策面 B。极限的位置是指，如果越过了这个位置，就会产生分类错误。这样的话，两个极限位置 A 和 B 之间的分界线 C 就是最优决策面。极限位置到最优决策面 C 之间的距离，就是“<strong>分类间隔</strong>”，英文叫做 margin。</p>
<br>

<p>如果我们转动这个最优决策面，你会发现可能存在多个最优决策面，它们都能把数据集正确分开，这些最优决策面的分类间隔可能是不同的，而那个拥有“<strong>最大间隔</strong>”（max margin）的决策面就是 SVM 要找的最优解。</p>
 <br>

<p><strong>点到超平面的距离公式</strong></p>
<p>在上面这个例子中，如果我们把红蓝两种颜色的球放到一个三维空间里，你发现决策面就变成了一个平面。这里我们可以用线性函数来表示，如果在一维空间里就表示一个点，在二维空间里表示一条直线，在三维空间中代表一个平面，当然空间维数还可以更多，这样我们给这个线性函数起个名称叫做“超平面”。超平面的数学表达可以写成：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/106.png" alt="106" style="zoom: 33%;">

<p>在这个公式里，w、x 是 n 维空间里的向量，其中 x 是函数变量；w 是法向量。法向量这里指的是垂直于平面的直线所表示的向量，它决定了超平面的方向。</p>
<p>SVM 就是帮我们找到一个超平面 ，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化。</p>
<p>在这个过程中， 支持向量 就是离 分类超平面 最近的样本点，实际上如果确定了支持向量也就确定了这个超平面。所以支持向量决定了分类间隔到底是多少，而在最大间隔以外的样本点，其实对分类都没有意义。</p>
<p>所以说， SVM 就是求解最大分类间隔的过程，我们还需要对分类间隔的大小进行定义。</p>
<p>首先，我们定义某类样本集到超平面的距离是这个样本集合内的样本到超平面的最短距离。我们用 di 代表点 xi 到超平面 wxi+b=0 的欧氏距离。因此我们要求 di 的最小值，用它来代表这个样本到超平面的最短距离。di 可以用公式计算得出：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/107.png" alt="106" style="zoom: 33%;">

<p>其中||w||为超平面的范数，di 的公式可以用解析几何知识进行推导，这里不做解释。</p>
<p><strong>最大间隔的优化模型</strong></p>
<p>我们的目标就是找出所有分类间隔中最大的那个值对应的超平面。在数学上，这是一个凸优化问题（凸优化就是关于求凸集中的凸函数最小化的问题，这里不具体展开）。通过凸优化问题，最后可以求出最优的 w 和 b，也就是我们想要找的最优超平面。中间求解的过程会用到拉格朗日乘子，和 KKT（Karush-Kuhn-Tucker）条件。数学公式比较多，这里不进行展开。 </p>
<p><strong>硬间隔、软间隔和非线性 SVM</strong></p>
<p>假如数据是完全的线性可分的，那么学习到的模型可以称为硬间隔支持向量机。 换个说法，硬间隔指的就是完全分类准确，不能存在分类错误的情况。软间隔，就是允许一定量的样本分类错误。</p>
<p>我们知道，实际工作中的数据没有那么“干净”，或多或少都会存在一些噪点。所以线性可分是个理想情况。这时，我们需要使用到软间隔 SVM（近似线性可分），比如下面这种情况：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/108.jpeg" alt="108" style="zoom:25%;">

<p>另外还存在一种情况，就是非线性支持向量机。</p>
<p>比如下面的样本集就是个非线性的数据。图中的两类数据，分别分布为两个圆圈的形状。那么这种情况下，不论是多高级的分类器，只要映射函数是线性的，就没法处理，SVM 也处理不了。这时，我们需要引入一个新的概念： 核函数。它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分 。这样我们就可以使用原来的推导来进行计算，只是所有的推导是在新的空间，而不是在原来的空间中进行。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/109.jpeg" alt="109" style="zoom:25%;">

<p>所以在非线性 SVM 中，核函数的选择就是影响 SVM 最大的变量。最常用的核函数有线性核、多项式核、高斯核、拉普拉斯核、sigmoid 核，或者是这些核函数的组合。这些函数的区别在于映射方式的不同。通过这些核函数，我们就可以把样本空间投射到新的高维空间中。</p>
<p>当然软间隔和核函数的提出，都是为了方便我们对上面超平面公式中的 w* 和 b* 进行求解，从而得到最大分类间隔的超平面。</p>
<h3 id="二、-用-SVM-如何解决多分类问题"><a href="#二、-用-SVM-如何解决多分类问题" class="headerlink" title="二、 用 SVM 如何解决多分类问题"></a>二、 用 SVM 如何解决多分类问题</h3><p>SVM 本身是一个二值分类器，最初是为二分类问题设计的，也就是回答 Yes 或者是 No。而实际上我们要解决的问题，可能是多分类的情况，比如对文本进行分类，或者对图像进行识别。</p>
<p>针对这种情况，我们可以将多个二分类器组合起来形成一个多分类器，常见的方法有“一对多法”和“一对一法”两种。</p>
<p><strong>1. 一对多法</strong></p>
<p>假设我们要把物体分成 A、B、C、D 四种分类，那么我们可以先把其中的一类作为分类 1，其他类统一归为分类 2。这样我们可以构造 4 种 SVM，分别为以下的情况：</p>
<p>（1）样本 A 作为正集，B，C，D 作为负集；</p>
<p>（2）样本 B 作为正集，A，C，D 作为负集；</p>
<p>（3）样本 C 作为正集，A，B，D 作为负集；</p>
<p>（4）样本 D 作为正集，A，B，C 作为负集。</p>
<p>这种方法，针对 K 个分类，需要训练 K 个分类器，分类速度较快，但训练速度较慢，因为每个分类器都需要对全部样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对称的情况，而且当增加新的分类，比如第 K+1 类时，需要重新对分类器进行构造。</p>
<p><strong>2. 一对一法</strong></p>
<p>一对一法的初衷是想在训练的时候更加灵活。我们可以在任意两类样本之间构造一个 SVM，这样针对 K 类的样本，就会有 C(k,2) 类分类器。</p>
<p>比如我们想要划分 A、B、C 三个类，可以构造 3 个分类器：</p>
<p>（1）分类器 1：A、B；</p>
<p>（2）分类器 2：A、C；</p>
<p>（3）分类器 3：B、C。    </p>
<p>当对一个未知样本进行分类时，每一个分类器都会有一个分类结果，即为 1 票，最终得票最多的类别就是整个未知样本的类别。</p>
<p>这样做的好处是，如果新增一类，不需要重新训练所有的 SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个 SVM 模型的时候，训练速度快。</p>
<p>但这种方法的不足在于，分类器的个数与 K 的平方成正比，所以当 K 较大时，训练和测试的时间会比较慢。 </p>
<h3 id="三、-如何在-sklearn-中使用-SVM"><a href="#三、-如何在-sklearn-中使用-SVM" class="headerlink" title="三、 如何在 sklearn 中使用 SVM"></a>三、 如何在 sklearn 中使用 SVM</h3><p>在 Python 的 sklearn 工具包中有 SVM 算法，首先需要引用工具包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br></pre></td></tr></table></figure>

<p>SVM 既可以做<strong>回归</strong>，也可以做<strong>分类</strong>器。</p>
<p>当用 SVM 做回归的时候，我们可以使用 SVR 或 LinearSVR。SVR 的英文是 Support Vector Regression。这篇文章只讲分类，这里只是简单地提一下。</p>
<p>当做分类器的时候，我们使用的是 SVC 或者 LinearSVC。SVC 的英文是 Support Vector Classification。</p>
<p>我简单说一下这两者之前的差别。</p>
<p>从名字上你能看出 LinearSVC 是个<strong>线性</strong>分类器，用于处理线性可分的数据，只能使用线性核函数。上一节，我讲到 SVM 是通过核函数将样本从原始空间映射到一个更高维的特质空间中，这样就使得样本在新的空间中线性可分。</p>
<p>如果是针对<strong>非线性</strong>的数据，需要用到 SVC。在 SVC 中，我们既可以使用到线性核函数（进行线性划分），也能使用高维的核函数（进行非线性划分）。</p>
<p><strong>创建一个 SVM 分类器</strong></p>
<p>我们首先使用 SVC 的构造函数：model = svm.SVC(kernel=‘rbf’, C=1.0, gamma=‘auto’)，这里有三个重要的参数 kernel、C 和 gamma。</p>
<ul>
<li><p>kernel 代表核函数的选择，它有四种选择，只不过默认是 rbf，即高斯核函数。</p>
<p>linear：线性核函数</p>
<p>poly：多项式核函数</p>
<p>rbf：高斯核函数（默认）</p>
<p>sigmoid：sigmoid 核函数</p>
</li>
</ul>
<p>这四种函数代表不同的映射方式，在实际工作中，如何选择这 4 种核函数：</p>
<p>线性核函数，是在数据线性可分的情况下使用的，运算速度快，效果好。不足在于它不能处理线性不可分的数据。</p>
<p>多项式核函数可以将数据从低维空间映射到高维空间，但参数比较多，计算量大。</p>
<p>高斯核函数同样可以将样本映射到高维空间，但相比于多项式核函数来说所需的参数比较少，通常性能不错，所以是默认使用的核函数。</p>
<p>了解深度学习的同学应该知道 sigmoid 经常用在神经网络的映射中。因此当选用 sigmoid 核函数时，SVM 实现的是多层神经网络。</p>
<br>

<p>上面介绍的 4 种核函数，除了第一种线性核函数外，其余 3 种都可以处理线性不可分的数据。</p>
<ul>
<li><p>参数 C 代表目标函数的惩罚系数</p>
<p>惩罚系数指的是分错样本时的惩罚程度，默认情况下为 1.0。当 C 越大的时候，分类器的准确性越高，但同样容错率会越低，泛化能力会变差。相反，C 越小，泛化能力越强，但是准确性会降低。</p>
</li>
<li><p>参数 gamma 代表核函数的系数</p>
<p>默认为样本特征数的倒数，即 gamma = 1 / n_features。</p>
</li>
</ul>
<p>在创建 SVM 分类器之后，就可以输入训练集对它进行训练。我们使用 model.fit(train_X,train_y)，传入训练集中的特征值矩阵 train_X 和分类标识 train_y。特征值矩阵就是我们在特征选择后抽取的特征值矩阵（当然你也可以用全部数据作为特征值矩阵）；分类标识就是人工事先针对每个样本标识的分类结果。这样模型会自动进行分类器的训练。我们可以使用 prediction=model.predict(test_X) 来对结果进行预测，传入测试集中的样本特征矩阵 test_X，可以得到测试集的预测分类结果 prediction。</p>
<p>同样我们也可以创建线性 SVM 分类器，使用 model=svm.LinearSVC()。在 LinearSVC 中没有 kernel 这个参数，限制我们只能使用线性核函数。由于 LinearSVC 对线性分类做了优化，对于数据量大的线性可分问题，使用 LinearSVC 的效率要高于 SVC。</p>
<p>如果你不知道数据集是否为线性，可以直接使用 SVC 类创建 SVM 分类器。</p>
<p>在训练和预测中，LinearSVC 和 SVC 一样，都是使用 model.fit(train_X,train_y) 和 model.predict(test_X)。</p>
<h3 id="四、-如何用-SVM-进行乳腺癌检测"><a href="#四、-如何用-SVM-进行乳腺癌检测" class="headerlink" title="四、 如何用 SVM 进行乳腺癌检测"></a>四、 如何用 SVM 进行乳腺癌检测</h3><p>在了解了如何创建和使用 SVM 分类器后，我们来看一个实际的项目，数据集来自美国威斯康星州的乳腺癌诊断数据集， 点击<a href="https://github.com/cystanford/breast_cancer_data/" target="_blank" rel="noopener">这里</a>进行下载。</p>
<p>医疗人员采集了患者乳腺肿块经过细针穿刺 (FNA) 后的数字化图像，并且对这些数字图像进行了特征提取，这些特征可以描述图像中的细胞核呈现。肿瘤可以分成良性和恶性。部分数据截屏如下所示：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/110.png" alt="106">

<p>数据表一共包括了 32 个字段，代表的含义如下：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/111.jpeg" alt="111" style="zoom:80%;">

<p>上面的表格中，mean 代表平均值，se 代表标准差，worst 代表最大值（3 个最大值的平均值）。每张图像都计算了相应的特征，得出了这 30 个特征值（不包括 ID 字段和分类标识结果字段 diagnosis），实际上是 10 个特征值（radius、texture、perimeter、area、smoothness、compactness、concavity、concave points、symmetry 和 fractal_dimension_mean）的 3 个维度，平均、标准差和最大值。这些特征值都保留了 4 位数字。字段中没有缺失的值。在 569 个患者中，一共有 357 个是良性，212 个是恶性。</p>
<p>好了，我们的目标是生成一个乳腺癌诊断的 SVM 分类器，并计算这个分类器的准确率。 </p>
<ol>
<li>首先加载数据并对数据做部分的探索：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集，你需要把数据放到目录中</span></span><br><span class="line">data = pd.read_csv(<span class="string">"./data.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据探索</span></span><br><span class="line">print(data.columns)</span><br><span class="line">print(data.info())</span><br><span class="line">print(data.head(<span class="number">5</span>))</span><br><span class="line">print(data.describe())</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>对数据进行清洗</li>
</ol>
<p>运行结果中，你能看到 32 个字段里，id 是没有实际含义的，可以去掉。diagnosis 字段的取值为 B 或者 M，我们可以用 0 和 1 来替代。另外其余的 30 个字段，其实可以分成三组字段，下划线后面的 mean、se 和 worst 代表了每组字段不同的度量方式，分别是平均值、标准差和最大值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将特征字段分成 3 组(获得三组columns list)</span></span><br><span class="line">features_mean= list(data.columns[<span class="number">2</span>:<span class="number">12</span>])</span><br><span class="line">features_se= list(data.columns[<span class="number">12</span>:<span class="number">22</span>])</span><br><span class="line">features_worst=list(data.columns[<span class="number">22</span>:<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据清洗</span></span><br><span class="line"><span class="comment"># ID 列没有用，删除该列</span></span><br><span class="line">data.drop(<span class="string">"id"</span>,axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 将 B 良性替换为 0，M 恶性替换为 1</span></span><br><span class="line">data[<span class="string">'diagnosis'</span>]=data[<span class="string">'diagnosis'</span>].map(&#123;<span class="string">'M'</span>:<span class="number">1</span>,<span class="string">'B'</span>:<span class="number">0</span>&#125;)</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>做特征字段的筛选</li>
</ol>
<p>首先需要观察下 features_mean 各变量之间的关系，这里我们可以用 DataFrame 的 corr() 函数，然后用热力图帮我们可视化呈现。同样，我们也会看整体良性、恶性肿瘤的诊断情况。</p>
<p>（特征字段：根据什么字段分类）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将肿瘤诊断结果可视化</span></span><br><span class="line">sns.countplot(data[<span class="string">'diagnosis'</span>],label=<span class="string">"Count"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用热力图呈现 features_mean 字段之间的相关性</span></span><br><span class="line">corr = data[features_mean].corr()</span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">14</span>))</span><br><span class="line"><span class="comment"># annot=True 显示每个方格的数据</span></span><br><span class="line">sns.heatmap(corr, annot=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>这是运行的结果：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/112.png" alt="112" style="zoom: 40%;">

<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/113.png" alt="113" style="zoom:50%;">

<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/115.png" alt="113" style="zoom:50%;">

<p>热力图中对角线上的为单变量自身的相关系数是 1。颜色越浅代表相关性越大。所以你能看出来 radius_mean、perimeter_mean 和 area_mean 相关性非常大，compactness_mean、concavity_mean、concave_points_mean 这三个字段也是相关的，因此我们可以取其中的一个作为代表。</p>
<ol start="4">
<li>进行特征选择</li>
</ol>
<p>特征选择的目的是降维，用少量的特征代表数据的特性，这样也可以增强分类器的泛化能力，避免数据过拟合。</p>
<p>我们能看到 mean、se 和 worst 这三组特征是对同一组内容的不同度量方式，我们可以保留 mean 这组特征，在特征选择中忽略掉 se 和 worst。同时我们能看到 mean 这组特征中，radius_mean、perimeter_mean、area_mean 这三个属性相关性大，compactness_mean、daconcavity_mean、concave points_mean 这三个属性相关性大。我们分别从这 2 类中选择 1 个属性作为代表，比如 radius_mean 和 compactness_mean。</p>
<p>这样我们就可以把原来的 10 个属性缩减为 6 个属性，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 特征选择</span></span><br><span class="line">features_remain = [<span class="string">'radius_mean'</span>,<span class="string">'texture_mean'</span>, <span class="string">'smoothness_mean'</span>,<span class="string">'compactness_mean'</span>,<span class="string">'symmetry_mean'</span>, <span class="string">'fractal_dimension_mean'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对特征进行选择之后，我们就可以准备训练集和测试集：</span></span><br><span class="line"><span class="comment"># 抽取 30% 的数据作为测试集，其余作为训练集</span></span><br><span class="line">train, test = train_test_split(data, test_size = <span class="number">0.3</span>)<span class="comment"># in this our main data is splitted into train and test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 抽取特征选择的数值作为训练和测试数据</span></span><br><span class="line">train_X = train[features_remain]</span><br><span class="line">train_y=train[<span class="string">'diagnosis'</span>]</span><br><span class="line">test_X= test[features_remain]</span><br><span class="line">test_y =test[<span class="string">'diagnosis'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练之前，我们需要对数据进行规范化，这样让数据同在同一个量级上，避免因为维度问题造成数据误差：</span></span><br><span class="line"><span class="comment"># 采用 Z-Score 规范化数据，保证每个特征维度的数据均值为 0，方差为 1</span></span><br><span class="line"><span class="comment"># fit_transform()特征向量转化为特征矩阵</span></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">train_X = ss.fit_transform(train_X)</span><br><span class="line">test_X = ss.transform(test_X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后我们可以让 SVM 做训练和预测了：</span></span><br><span class="line"><span class="comment"># 创建 SVM 分类器</span></span><br><span class="line">model = svm.SVC()</span><br><span class="line"><span class="comment"># 用训练集做训练</span></span><br><span class="line">model.fit(train_X,train_y)</span><br><span class="line"><span class="comment"># 用测试集做预测</span></span><br><span class="line">prediction=model.predict(test_X)</span><br><span class="line">print(<span class="string">'准确率: '</span>, metrics.accuracy_score(prediction,test_y))</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">准确率:  0.9181286549707602</span><br></pre></td></tr></table></figure>

<p>准确率大于 90%，说明训练结果还不错。</p>
<p>完整代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">pd.set_option(<span class="string">'display.max_columns'</span>, <span class="literal">None</span>)</span><br><span class="line">pd.set_option(<span class="string">'display.max_rows'</span>, <span class="literal">None</span>)</span><br><span class="line">pd.set_option(<span class="string">'max_colwidth'</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">'../test_data/breast_cancer_data/data.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(data.columns)</span></span><br><span class="line"><span class="comment"># print(data.info())</span></span><br><span class="line"><span class="comment"># print(data.head(5))</span></span><br><span class="line"><span class="comment"># print(data.describe())</span></span><br><span class="line"></span><br><span class="line">features_mean = list(data.columns[<span class="number">2</span>:<span class="number">12</span>])</span><br><span class="line">features_se = list(data.columns[<span class="number">12</span>:<span class="number">22</span>])</span><br><span class="line">features_worst = list(data.columns[<span class="number">22</span>:<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line">data.drop(<span class="string">"id"</span>, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">data[<span class="string">'diagnosis'</span>] = data[<span class="string">'diagnosis'</span>].map(&#123;<span class="string">'M'</span>: <span class="number">1</span>, <span class="string">'B'</span>: <span class="number">0</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sns.countplot(data['diagnosis'], label="Count")</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># corr = data[features_mean].corr()</span></span><br><span class="line"><span class="comment"># plt.figure(figsize=(14,14))</span></span><br><span class="line"><span class="comment"># sns.heatmap(corr, annot=True)</span></span><br><span class="line"><span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">features_remain = [<span class="string">'radius_mean'</span>,<span class="string">'texture_mean'</span>, <span class="string">'smoothness_mean'</span>, <span class="string">'compactness_mean'</span>, <span class="string">'symmetry_mean'</span>, <span class="string">'fractal_dimension_mean'</span>]</span><br><span class="line"></span><br><span class="line">train, test = train_test_split(data, test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">train_X = train[features_remain]</span><br><span class="line">train_y = train[<span class="string">'diagnosis'</span>]</span><br><span class="line">test_X = test[features_remain]</span><br><span class="line">test_y = test[<span class="string">'diagnosis'</span>]</span><br><span class="line"></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">train_X = ss.fit_transform(train_X)</span><br><span class="line">test_X = ss.transform(test_X)</span><br><span class="line"></span><br><span class="line">model = svm.SVC()</span><br><span class="line">model.fit(train_X, train_y)</span><br><span class="line">prediction = model.predict(test_X)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'准确率: '</span>, metrics.accuracy_score(prediction, test_y))</span><br></pre></td></tr></table></figure>

<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/114.png" alt="114"></p>
<h2 id="19-KNN分类"><a href="#19-KNN分类" class="headerlink" title="19.KNN分类"></a>19.KNN分类</h2><p>KNN 的英文叫 K-Nearest Neighbor，应该算是数据挖掘算法中最简单的一种。</p>
<p>假设，我们想对电影的类型进行分类，统计了电影中打斗次数、接吻次数，当然还有其他的指标也可以被统计到，如下表所示。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/116.png" alt="116" style="zoom:50%;">

<p>我们很容易理解《战狼》《红海行动》《碟中谍 6》是动作片，《前任 3》《春娇救志明》《泰坦尼克号》是爱情片，但是有没有一种方法让机器也可以掌握这个分类的规则，当有一部新电影的时候，也可以对它的类型自动分类呢？</p>
<p>我们可以把打斗次数看成 X 轴，接吻次数看成 Y 轴，然后在二维的坐标轴上，对这几部电影进行标记，如下图所示。对于未知的电影 A，坐标为 (x,y)，我们需要看下离电影 A 最近的都有哪些电影，这些电影中的大多数属于哪个分类，那么电影 A 就属于哪个分类。实际操作中，我们还需要确定一个 K 值，也就是我们要观察离电影 A 最近的电影有多少个。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/117.png" alt="116" style="zoom: 67%;">

<h3 id="一、KNN-的工作原理"><a href="#一、KNN-的工作原理" class="headerlink" title="一、KNN 的工作原理"></a>一、KNN 的工作原理</h3><p>“近朱者赤，近墨者黑”可以说是 KNN 的工作原理。整个计算过程分为三步：</p>
<ol>
<li>计算待分类物体与其他物体之间的距离；</li>
<li>统计距离最近的 K 个邻居；</li>
<li>对于 K 个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类。</li>
</ol>
<h4 id="K-值如何选择"><a href="#K-值如何选择" class="headerlink" title="K 值如何选择"></a>K 值如何选择</h4><p>你能看出整个 KNN 的分类过程，K 值的选择还是很重要的，K 值选择多少是适合的呢？</p>
<p>如果 K 值比较小，就相当于未分类物体与它的邻居非常接近才行。这样产生的一个问题就是，如果邻居点是个噪声点，那么未分类物体的分类也会产生误差，这样 KNN 分类就会产生过拟合。</p>
<p>如果 K 值比较大，相当于距离过远的点也会对未知物体的分类产生影响，虽然这种情况的好处是鲁棒性强，但是不足也很明显，会产生欠拟合情况，也就是没有把未分类物体真正分类出来。</p>
<p>所以 K 值应该是个实践出来的结果，并不是我们事先而定的。在工程上，我们一般采用交叉验证的方式选取 K 值。</p>
<p>交叉验证的思路就是，把样本集中的大部分样本作为训练集，剩余的小部分样本用于预测，来验证分类模型的准确性。所以在 KNN 算法中，我们一般会把 K 值选取在较小的范围内，同时在验证集上准确率最高的那一个最终确定作为 K 值。</p>
<h4 id="距离如何计算"><a href="#距离如何计算" class="headerlink" title="距离如何计算"></a>距离如何计算</h4><p>在 KNN 算法中，还有一个重要的计算就是关于距离的度量。两个样本点之间的距离代表了这两个样本之间的相似度。距离越大，差异性越大；距离越小，相似度越大。</p>
<p>关于距离的计算方式有下面五种方式：</p>
<p>欧氏距离；</p>
<p>曼哈顿距离；</p>
<p>闵可夫斯基距离；</p>
<p>切比雪夫距离；</p>
<p>余弦距离。</p>
<p>其中前三种距离是 KNN 中最常用的距离，现在分别讲解下。</p>
<p><strong>欧氏距离</strong></p>
<p>最常用的距离公式，也叫做欧几里得距离。在二维空间中，两点的欧式距离就是：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/118.png" alt="116" style="zoom: 25%;">

<p>同理，我们也可以求得两点在 n 维空间中的距离：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/119.png" alt="116" style="zoom: 25%;">

<p><strong>曼哈顿距离</strong></p>
<p>在几何空间中用的比较多。曼哈顿距离等于两个点在坐标系上绝对轴距总和。用公式表示就是：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/120.png" alt="116" style="zoom: 25%;">

<p><strong>闵可夫斯基距离</strong></p>
<p>不是一个距离，而是一组距离的定义。对于 n 维空间中的两个点 x(x1,x2,…,xn) 和 y(y1,y2,…,yn) ， x 和 y 两点之间的闵可夫斯基距离为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/121.png" alt="116" style="zoom: 25%;">

<p>其中 p 代表空间的维数，当 p=1 时，就是曼哈顿距离；当 p=2 时，就是欧氏距离；当 p→∞时，就是切比雪夫距离。</p>
<p><strong>切比雪夫距离</strong>：二个点之间的切比雪夫距离就是这两个点坐标数值差的绝对值的最大值，用数学表示就是：max(|x1-y1|,|x2-y2|)。</p>
<p><strong>余弦距离</strong>：实际上计算的是两个向量的夹角，是在方向上计算两者之间的差异，对绝对数值不敏感。在兴趣相关性比较上，角度关系比距离的绝对值更重要，因此余弦距离可以用于衡量用户对内容兴趣的区分度。比如我们用搜索引擎搜索某个关键词，它还会给你推荐其他的相关搜索，这些推荐的关键词就是采用余弦距离计算得出的。</p>
<h4 id="KD-树"><a href="#KD-树" class="headerlink" title="KD 树"></a>KD 树</h4><p>其实从上文你也能看出来，KNN 的计算过程是大量计算样本点之间的距离。为了减少计算距离次数，提升 KNN 的搜索效率，人们提出了 KD 树（K-Dimensional 的缩写）。KD 树是对数据点在 K 维空间中划分的一种数据结构。在 KD 树的构造中，每个节点都是 k 维数值点的二叉树。既然是二叉树，就可以采用二叉树的增删改查操作，这样就大大提升了搜索效率。</p>
<p>在这里，我们不需要对 KD 树的数学原理了解太多，你只需要知道它是一个二叉树的数据结构，方便存储 K 维空间的数据就可以了。而且在 sklearn 中，我们直接可以调用 KD 树，很方便。</p>
<h4 id="用-KNN-做回归"><a href="#用-KNN-做回归" class="headerlink" title="用 KNN 做回归"></a>用 KNN 做回归</h4><p>KNN 不仅可以做<strong>分类</strong>，还可以做<strong>回归</strong>。首先讲下什么是回归。在开头电影这个案例中，如果想要对未知电影进行类型划分，这是一个分类问题。首先看一下要分类的未知电影，离它最近的 K 部电影大多数属于哪个分类，这部电影就属于哪个分类。</p>
<p>如果是一部新电影，已知它是爱情片，想要知道它的打斗次数、接吻次数可能是多少，这就是一个回归问题。</p>
<p>那么 KNN 如何做回归呢？</p>
<p>对于一个新点，我们需要找出这个点的 K 个最近邻居，然后将这些邻居的属性的平均值赋给该点，就可以得到该点的属性。当然不同邻居的影响力权重可以设置成不同的。举个例子，比如一部电影 A，已知它是动作片，当 K=3 时，最近的 3 部电影是《战狼》，《红海行动》和《碟中谍 6》，那么它的打斗次数和接吻次数的预估值分别为 (100+95+105)/3=100 次、(5+3+31)/3=13 次。 </p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/124.png" alt="124" style="zoom:50%;">

<h3 id="二、-如何在-sklearn-中使用-KNN"><a href="#二、-如何在-sklearn-中使用-KNN" class="headerlink" title="二、 如何在 sklearn 中使用 KNN"></a>二、 如何在 sklearn 中使用 KNN</h3><p>接下来，我们先看下如何在 sklearn 中使用 KNN 算法，然后通过 sklearn 中自带的手写数字数据集来进行实战。</p>
<p>在 Python 的 sklearn 工具包中有 KNN 算法。KNN 既可以做分类器，也可以做回归。</p>
<p>如果是做<strong>分类</strong>，你需要引用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br></pre></td></tr></table></figure>

<p>如果是做<strong>回归</strong>，你需要引用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br></pre></td></tr></table></figure>

<p>从名字上你也能看出来 Classifier 对应的是分类，Regressor 对应的是回归。一般来说如果一个算法有 Classifier 类，都能找到相应的 Regressor 类。比如在决策树分类中，你可以使用 DecisionTreeClassifier，也可以使用决策树来做回归 DecisionTreeRegressor。</p>
<p><strong>如何在 sklearn 中创建 KNN 分类器</strong></p>
<p>这里，我们使用构造函数 KNeighborsClassifier(n_neighbors=5, weights=‘uniform’, algorithm=‘auto’, leaf_size=30)，这里有几个比较主要的参数，我分别来讲解下：</p>
<ul>
<li><p>n_neighbors：即 KNN 中的 K 值，代表的是邻居的数量。K 值如果比较小，会造成过拟合。如果 K 值比较大，无法将未知物体分类出来。一般我们使用默认值 5。</p>
</li>
<li><p>weights：是用来确定邻居的权重，有三种方式：</p>
<p>weights=uniform，代表所有邻居的权重相同；</p>
<p>weights=distance，代表权重是距离的倒数，即与距离成反比；</p>
<p>自定义函数，你可以自定义不同距离所对应的权重。大部分情况下不需要自己定义函数。</p>
</li>
<li><p>algorithm：用来规定计算邻居的方法，它有四种方式：</p>
<p>algorithm=auto，根据数据的情况自动选择适合的算法，默认情况选择 auto；</p>
<p>algorithm=kd_tree，也叫作 KD 树，是多维空间的数据结构，方便对关键数据进行检索，不过 KD 树适用于维度少的情况，一般维数不超过 20，如果维数大于 20 之后，效率反而会下降；</p>
<p>algorithm=ball_tree，也叫作球树，它和 KD 树一样都是多维空间的数据结果，不同于 KD 树，球树更适用于维度大的情况；</p>
<p>algorithm=brute，也叫作暴力搜索，它和 KD 树不同的地方是在于采用的是线性扫描，而不是通过构造树结构进行快速检索。当训练集大的时候，效率很低。</p>
</li>
<li><p>leaf_size：代表构造 KD 树或球树时的叶子数，默认是 30，调整 leaf_size 会影响到树的构造和搜索速度。</p>
</li>
</ul>
<p>创建完 KNN 分类器之后，我们就可以输入训练集对它进行训练，这里我们使用 fit() 函数，传入训练集中的样本特征矩阵和分类标识，会自动得到训练好的 KNN 分类器。然后可以使用 predict() 函数来对结果进行预测，这里传入测试集的特征矩阵，可以得到测试集的预测分类结果。</p>
<h3 id="三、-如何用-KNN-对手写数字进行识别分类"><a href="#三、-如何用-KNN-对手写数字进行识别分类" class="headerlink" title="三、 如何用 KNN 对手写数字进行识别分类"></a>三、 如何用 KNN 对手写数字进行识别分类</h3><p>手写数字数据集是个非常有名的用于图像识别的数据集。数字识别的过程就是将这些图片与分类结果 0-9 一一对应起来。完整的手写数字数据集 MNIST 里面包括了 60000 个训练样本，以及 10000 个测试样本。如果你学习深度学习的话，MNIST 基本上是你接触的第一个数据集。</p>
<p>今天我们用 sklearn 自带的手写数字数据集做 KNN 分类，你可以把这个数据集理解成一个简版的 MNIST 数据集，它只包括了 1797 幅数字图像，每幅图像大小是 8*8 像素。</p>
 <br>

<p>先规划下整个 KNN 分类的流程：</p>
<ul>
<li><p>数据加载：我们可以直接从 sklearn 中加载自带的手写数字数据集；</p>
</li>
<li><p>准备阶段：在这个阶段中，我们需要对数据集有个初步的了解，比如样本的个数、图像长什么样、识别结果是怎样的。可以通过<strong>可视化</strong>的方式来查看图像的呈现。通过<strong>数据规范化</strong>可以让数据都在同一个数量级的维度。另外，因为训练集是图像，每幅图像是个 8*8 的矩阵，我们不需要对它进行特征选择，将全部的图像数据作为特征值矩阵即可；</p>
</li>
<li><p>分类阶段：通过训练可以得到分类器，然后用测试集进行准确率的计算。</p>
</li>
</ul>
<ol>
<li>加载数据和对数据的探索：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">digits = load_digits()</span><br><span class="line">data = digits.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据探索</span></span><br><span class="line">print(data.shape)</span><br><span class="line">print(data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看第一幅图像</span></span><br><span class="line">print(digits.images[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 第一幅图像代表的数字含义</span></span><br><span class="line">print(digits.target[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 将第一幅图像显示出来</span></span><br><span class="line">plt.gray()</span><br><span class="line">plt.imshow(digits.images[<span class="number">0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">(1797, 64)</span><br><span class="line">[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.</span><br><span class="line"> 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.</span><br><span class="line">  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.</span><br><span class="line">  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]</span><br><span class="line">[[ 0.  0.  5. 13.  9.  1.  0.  0.]</span><br><span class="line">[ 0.  0. 13. 15. 10. 15.  5.  0.]</span><br><span class="line">[ 0.  3. 15.  2.  0. 11.  8.  0.]</span><br><span class="line">[ 0.  4. 12.  0.  0.  8.  8.  0.]</span><br><span class="line">[ 0.  5.  8.  0.  0.  9.  8.  0.]</span><br><span class="line">[ 0.  4. 11.  0.  1. 12.  7.  0.]</span><br><span class="line">[ 0.  2. 14.  5. 10. 12.  0.  0.]</span><br><span class="line">[ 0.  0.  6. 13. 10.  0.  0.  0.]]</span><br><span class="line">0</span><br></pre></td></tr></table></figure>

<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/123.png" alt="116" style="zoom: 50%;">

<p>我们对原始数据集中的第一幅进行数据可视化，可以看到图像是个 8*8 的像素矩阵，上面这幅图像是一个“0”，从训练集的分类标注中我们也可以看到分类标注为“0”。</p>
<p>sklearn 自带的手写数字数据集一共包括了 1797 个样本，每幅图像都是 8*8 像素的矩阵。因为并没有专门的测试集，所以我们需要对数据集做划分，划分成训练集和测试集。因为 KNN 算法和距离定义相关，我们需要对数据进行规范化处理，采用 Z-Score 规范化，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分割数据，将 25% 的数据作为测试集，其余作为训练集（你也可以指定其他比例的数据作为训练集）</span></span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data, digits.target, test_size=<span class="number">0.25</span>, random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用 Z-Score 规范化</span></span><br><span class="line">ss = preprocessing.StandardScaler()</span><br><span class="line">train_ss_x = ss.fit_transform(train_x)</span><br><span class="line">test_ss_x = ss.transform(test_x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后我们构造一个 KNN 分类器 knn，把训练集的数据传入构造好的 knn，并通过测试集进行结果预测，与测试集的结果进行对比，得到 KNN 分类器准确率，代码如下：</span></span><br><span class="line"><span class="comment"># 创建 KNN 分类器</span></span><br><span class="line">knn = KNeighborsClassifier()</span><br><span class="line">knn.fit(train_ss_x, train_y)</span><br><span class="line">predict_y = knn.predict(test_ss_x)</span><br><span class="line">print(<span class="string">"KNN 准确率: %.4lf"</span> % accuracy_score(predict_y, test_y))</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KNN 准确率: 0.9756</span><br></pre></td></tr></table></figure>

<p>好了，这样我们就构造好了一个 KNN 分类器。之前我们还讲过 SVM、朴素贝叶斯和决策树分类。我们用手写数字数据集一起来训练下这些分类器，然后对比下哪个分类器的效果更好。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建 SVM 分类器</span></span><br><span class="line">svm = SVC()</span><br><span class="line">svm.fit(train_ss_x, train_y)</span><br><span class="line">predict_y=svm.predict(test_ss_x)</span><br><span class="line">print(<span class="string">'SVM 准确率: %0.4lf'</span> % accuracy_score(predict_y, test_y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用 Min-Max 规范化</span></span><br><span class="line">mm = preprocessing.MinMaxScaler()</span><br><span class="line">train_mm_x = mm.fit_transform(train_x)</span><br><span class="line">test_mm_x = mm.transform(test_x)</span><br><span class="line"><span class="comment"># 创建 Naive Bayes 分类器</span></span><br><span class="line">mnb = MultinomialNB()</span><br><span class="line">mnb.fit(train_mm_x, train_y)</span><br><span class="line">predict_y = mnb.predict(test_mm_x)</span><br><span class="line">print(<span class="string">" 多项式朴素贝叶斯准确率: %.4lf"</span> % accuracy_score(predict_y, test_y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 CART 决策树分类器</span></span><br><span class="line">dtc = DecisionTreeClassifier()</span><br><span class="line">dtc.fit(train_mm_x, train_y)</span><br><span class="line">predict_y = dtc.predict(test_mm_x)</span><br><span class="line">print(<span class="string">"CART 决策树准确率: %.4lf"</span> % accuracy_score(predict_y, test_y))</span><br></pre></td></tr></table></figure>

<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SVM 准确率: 0.9867</span><br><span class="line">多项式朴素贝叶斯准确率: 0.8844</span><br><span class="line">CART 决策树准确率: 0.8556</span><br></pre></td></tr></table></figure>

<p>这里需要注意的是，我们在做多项式朴素贝叶斯分类的时候，传入的数据不能有负数。因为 Z-Score 会将数值规范化为一个标准的正态分布，即均值为 0，方差为 1，数值会包含负数。因此我们需要采用 Min-Max 规范化，将数据规范化到 [0,1] 范围内。</p>
<p>好了，我们整理下这 4 个分类器的结果。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/122.png" alt="116" style="zoom: 50%;">

<p>你能看出来 KNN 的准确率还是不错的，和 SVM 不相上下。</p>
<p>你可以自己跑一遍整个代码，在运行前还需要 import 相关的工具包（下面的这些工具包你都会用到，所以都需要引用）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>

<p>代码中，我使用了 train_test_split 做数据集的拆分，使用 matplotlib.pyplot 工具包显示图像，使用 accuracy_score 进行分类器准确率的计算，使用 preprocessing 中的 StandardScaler 和 MinMaxScaler 做数据的规范化。</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/125.png" alt="125"></p>
<h2 id="20-K-Means聚类"><a href="#20-K-Means聚类" class="headerlink" title="20.K-Means聚类"></a>20.K-Means聚类</h2><p>K-Means 是一种非监督学习，解决的是聚类问题。K 代表的是 K 类，Means 代表的是中心，你可以理解这个算法的本质是确定 K 类的中心点，当你找到了这些中心点，也就完成了聚类。</p>
<p>理解K-Means需要掌握的三个问题：</p>
<ol>
<li>如何确定 K 类的中心点？</li>
<li>如何将其他点划分到 K 类中？</li>
<li>如何区分 K-Means 与 KNN？</li>
</ol>
<h3 id="一、K-Means-的工作原理"><a href="#一、K-Means-的工作原理" class="headerlink" title="一、K-Means 的工作原理"></a>一、K-Means 的工作原理</h3><p><strong>如何确定 K 类的中心点</strong></p>
<p>一开始我们是可以随机指派的，当你确认了中心点后，就可以按照距离将其他的点划分到不同的类别中，这也就是 K-Means 的中心思想，就是这么简单直接。你可能会问：如果一开始中心点选错了怎么办？其实不用担心，K-Means 有自我纠正机制，在不断的迭代过程中，会纠正中心点。中心点在整个迭代过程中，并不是唯一的，只是你需要一个初始值，一般算法会随机设置初始的中心点。</p>
<p><strong>K-Means 的工作原理总结：</strong></p>
<ol>
<li>选取 K 个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的；</li>
<li>将每个点分配到最近的类中心点，这样就形成了 K 个类，然后重新计算每个类的中心点；<ul>
<li>解释一下：计算每个类的中心点就是，分别计算每个类别中的所有点的特征值的平均值，将这个平均值作为新的中心点，这就是为什么叫K-Means</li>
</ul>
</li>
<li>重复第二步，直到类不发生变化，或者你也可以设置最大迭代次数，这样即使类中心点发生变化，但是只要达到最大迭代次数就会结束。</li>
</ol>
<h3 id="二、如何给亚洲球队做聚类"><a href="#二、如何给亚洲球队做聚类" class="headerlink" title="二、如何给亚洲球队做聚类"></a>二、如何给亚洲球队做聚类</h3><p>其中 2019 年国际足联的世界排名，2015 年亚洲杯排名均为实际排名。2018 年世界杯中，很多球队没有进入到决赛圈，所以只有进入到决赛圈的球队才有实际的排名。如果是亚洲区预选赛 12 强的球队，排名会设置为 40。如果没有进入亚洲区预选赛 12 强，球队排名会设置为 50。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/126.png" alt="126" style="zoom: 67%;">

<p>针对上面的排名，我们首先需要做的是数据规范化。你可以把这些值划分到 [0,1] ，或者按照均值为 0，方差为 1 的正态分布进行规范化。</p>
<p>把数值都规范化到 [0,1] 的空间中，得到了以下的数值表：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/127.png" alt="126" style="zoom: 67%;">

<p>如果我们随机选取中国、日本、韩国为三个类的中心点，我们就需要看下这些球队到中心点的距离。</p>
<p>距离有多种计算的方式，有关距离的计算在 KNN 算法中讲过。这里我选择欧氏距离作为距离的标准，计算每个队伍分别到中国、日本、韩国的距离，然后根据距离远近来划分。我们看到大部分的队和中国队聚类到一起。这里我整理了距离的计算过程，比如中国和中国的欧氏距离为 0，中国和日本的欧式距离为 0.732003。如果按照中国、日本、韩国为 3 个分类的中心点，欧氏距离的计算结果如下表所示：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/128.png" alt="126" style="zoom: 67%;">

<p>然后我们再重新计算这三个类的中心点，如何计算呢？最简单的方式就是取平均值，然后根据新的中心点按照距离远近重新分配球队的分类，再根据球队的分类更新中心点的位置。计算过程这里不展开，最后一直迭代（重复上述的计算过程：计算中心点和划分分类）到分类不再发生变化，可以得到以下的分类结果：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/129.png" alt="126" style="zoom: 67%;">

<h3 id="三、如何使用-sklearn-中的-K-Means-算法"><a href="#三、如何使用-sklearn-中的-K-Means-算法" class="headerlink" title="三、如何使用 sklearn 中的 K-Means 算法"></a>三、如何使用 sklearn 中的 K-Means 算法</h3><p>sklearn 是 Python 的机器学习工具库，如果从功能上来划分，sklearn 可以实现分类、聚类、回归、降维、模型选择和预处理等功能。这里我们使用的是 sklearn 的聚类函数库，因此需要引用工具包，具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br></pre></td></tr></table></figure>

<p>当然 K-Means 只是 sklearn.cluster 中的一个聚类库，实际上包括 K-Means 在内，sklearn.cluster 一共提供了 9 种聚类方法，比如 Mean-shift，DBSCAN，Spectral clustering（谱聚类）等。这些聚类方法的原理和 K-Means 不同，这里不做介绍。</p>
<p>K-Means 创建：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KMeans(n_clusters=<span class="number">8</span>, init=<span class="string">'k-means++'</span>, n_init=<span class="number">10</span>, max_iter=<span class="number">300</span>, tol=<span class="number">0.0001</span>, precompute_distances=<span class="string">'auto'</span>, verbose=<span class="number">0</span>, random_state=<span class="literal">None</span>, copy_x=<span class="literal">True</span>, n_jobs=<span class="number">1</span>, algorithm=<span class="string">'auto'</span>)</span><br></pre></td></tr></table></figure>

<p>我们能看到在 K-Means 类创建的过程中，有一些主要的参数：</p>
<ul>
<li><p>n_clusters : 即 <em>K 值</em>，一般需要多试一些 K 值来保证更好的聚类效果。你可以随机设置一些 K 值，然后选择聚类效果最好的作为最终的 K 值；</p>
</li>
<li><p>max_iter ： <em>最大迭代次数</em>，如果聚类很难收敛的话，设置最大迭代次数可以让我们及时得到反馈结果，否则程序运行时间会非常长；</p>
</li>
<li><p>n_init ：<em>初始化中心点的运算次数</em>，默认是 10。程序是否能快速收敛和中心点的选择关系非常大，所以在中心点选择上多花一些时间，来争取整体时间上的快速收敛还是非常值得的。由于每一次中心点都是随机生成的，这样得到的结果就有好有坏，非常不确定，所以要运行 n_init 次, 取其中最好的作为初始的中心点。</p>
<p>如果 K 值比较大的时候，你可以适当增大 n_init 这个值；</p>
</li>
<li><p>algorithm ：k-means 的实现算法，有“auto” “full”“elkan”三种。一般来说建议直接用默认的”auto”。简单说下这三个取值的区别，如果你选择”full”采用的是传统的 K-Means 算法，“auto”会根据数据的特点自动选择是选择“full”还是“elkan”。我们一般选择默认的取值，即“auto” 。</p>
</li>
</ul>
<p>在创建好 K-Means 类之后，就可以使用它的方法，最常用的是 fit 和 predict 这个两个函数。你可以单独使用 fit 函数和 predict 函数，也可以合并使用 fit_predict 函数。其中 fit(data) 可以对data 数据进行 k-Means 聚类。 predict(data) 可以针对 data 中的每个样本，计算最近的类。</p>
<p><a href="https://github.com/cystanford/kmeans" target="_blank" rel="noopener">数据下载</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="comment"># coding: utf-8``from` `sklearn.cluster ``import` `KMeans``from` `sklearn ``import` `preprocessing``import` `pandas as pd``import` `numpy as np``# 输入数据``data ``=` `pd.read_csv(``'data.csv'``, encoding``=``'gbk'``)``train_x ``=` `data[[``"2019 年国际排名 "``,``"2018 世界杯 "``,``"2015 亚洲杯 "``]]``df ``=` `pd.DataFrame(train_x)``kmeans ``=` `KMeans(n_clusters``=``3``)``# 规范化到 [0,1] 空间``min_max_scaler``=``preprocessing.MinMaxScaler()``train_x``=``min_max_scaler.fit_transform(train_x)``# kmeans 算法``kmeans.fit(train_x)``predict_y ``=` `kmeans.predict(train_x)``# 合并聚类结果，插入到原数据中``result ``=` `pd.concat((data,pd.DataFrame(predict_y)),axis``=``1``)``result.rename(&#123;``0``:u``'聚类'``&#125;,axis``=``1``,inplace``=``True``)``print``(result)`  `# 运行结果：``国家  ``2019` `年国际排名  ``2018` `世界杯  ``2015` `亚洲杯  聚类``0`       `中国         ``73`       `40`        `7`   `2``1`       `日本         ``60`       `15`        `5`   `0``2`       `韩国         ``61`       `19`        `2`   `0``3`       `伊朗         ``34`       `18`        `6`   `0``4`       `沙特         ``67`       `26`       `10`   `0``5`      `伊拉克         ``91`       `40`        `4`   `2``6`      `卡塔尔        ``101`       `40`       `13`   `1``7`      `阿联酋         ``81`       `40`        `6`   `2``8`   `乌兹别克斯坦         ``88`       `40`        `8`   `2``9`       `泰国        ``122`       `40`       `17`   `1``10`      `越南        ``102`       `50`       `17`   `1``11`      `阿曼         ``87`       `50`       `12`   `1``12`      `巴林        ``116`       `50`       `11`   `1``13`      `朝鲜        ``110`       `50`       `14`   `1``14`      `印尼        ``164`       `50`       `17`   `1``15`      `澳洲         ``40`       `30`        `1`   `0``16`     `叙利亚         ``76`       `40`       `17`   `1``17`      `约旦        ``118`       `50`        `9`   `1``18`     `科威特        ``160`       `50`       `15`   `1``19`    `巴勒斯坦         ``96`       `50`       `16`   `1`</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">        国家  2019年国际排名  2018世界杯  2015亚洲杯  聚类</span><br><span class="line">0       中国         73       40        7   0</span><br><span class="line">1       日本         60       15        5   2</span><br><span class="line">2       韩国         61       19        2   2</span><br><span class="line">3       伊朗         34       18        6   2</span><br><span class="line">4       沙特         67       26       10   2</span><br><span class="line">5      伊拉克         91       40        4   0</span><br><span class="line">6      卡塔尔        101       40       13   1</span><br><span class="line">7      阿联酋         81       40        6   0</span><br><span class="line">8   乌兹别克斯坦         88       40        8   0</span><br><span class="line">9       泰国        122       40       17   1</span><br><span class="line">10      越南        102       50       17   1</span><br><span class="line">11      阿曼         87       50       12   1</span><br><span class="line">12      巴林        116       50       11   1</span><br><span class="line">13      朝鲜        110       50       14   1</span><br><span class="line">14      印尼        164       50       17   1</span><br><span class="line">15      澳洲         40       30        1   2</span><br><span class="line">16     叙利亚         76       40       17   1</span><br><span class="line">17      约旦        118       50        9   1</span><br><span class="line">18     科威特        160       50       15   1</span><br><span class="line">19    巴勒斯坦         96       50       16   1</span><br></pre></td></tr></table></figure>

<p>如何区分K-Means和KNN这两种算法呢？刚学过K-Means和KNN算法的同学应该能知道两者的区别，但往往过了一段时间，就容易混淆。所以我们可以从三个维度来区分K-Means和KNN这两个算法：</p>
<p>首先，这两个算法解决数据挖掘的两类问题。K-Means是聚类算法，KNN是分类算法。</p>
<p>这两个算法分别是两种不同的学习方式。K-Means是非监督学习，也就是不需要事先给出分类标签，而KNN是有监督学习，需要我们给出训练数据的分类标识。</p>
<p>最后，K值的含义不同。K-Means中的K值代表K类。KNN中的K值代表K个最接近的邻居。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/130.png">

<h3 id="四、用KMeans对图像进行分割"><a href="#四、用KMeans对图像进行分割" class="headerlink" title="四、用KMeans对图像进行分割"></a>四、用KMeans对图像进行分割</h3><p>图像分割就是利用图像自身的信息，比如颜色、纹理、形状等特征进行划分，将图像分割成不同的区域，划分出来的每个区域就相当于是对图像中的像素进行了聚类。单个区域内的像素之间的相似度大，不同区域间的像素差异性大。这个特性正好符合聚类的特性，所以你可以把图像分割看成是将图像中的信息进行聚类。当然聚类只是分割图像的一种方式，除了聚类，我们还可以基于图像颜色的阈值进行分割，或者基于图像边缘的信息进行分割等。</p>
<p><strong>将微信开屏封面进行分割</strong></p>
<p>我们现在用 K-Means 算法对微信页面进行分割。微信开屏图如下所示：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/weixin.jpg" alt="weixin" style="zoom: 200%;">

<p>我们先设定下聚类的流程，聚类的流程和分类差不多，如图所示：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/131.png" alt="126" style="zoom: 67%;">
在准备阶段里，我们需要对数据进行加载。

<p>因为处理的是图像信息，我们除了要获取图像数据以外，还需要获取图像的尺寸和通道数，然后基于图像中每个通道的数值进行数据规范化。这里我们需要定义个函数 load_data，来帮我们进行图像加载和数据规范化。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载图像，并对数据进行规范化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(filePath)</span>:</span></span><br><span class="line">    <span class="comment"># 读文件</span></span><br><span class="line">    f = open(filePath,<span class="string">'rb'</span>)</span><br><span class="line">    data = []</span><br><span class="line">    <span class="comment"># 得到图像的像素值</span></span><br><span class="line">    img = image.open(f)</span><br><span class="line">    <span class="comment"># 得到图像尺寸</span></span><br><span class="line">    width, height = img.size</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(width):</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(height):</span><br><span class="line">            <span class="comment"># 得到点(x,y)的三个通道值，存入data</span></span><br><span class="line">            c1, c2, c3 = img.getpixel((x, y))</span><br><span class="line">            data.append([c1, c2, c3])</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="comment"># 采用Min-Max规范化</span></span><br><span class="line">    <span class="comment"># np.mat()将[[,,],[,,],...]序列，转化为n*3数组矩阵</span></span><br><span class="line">    mm = preprocessing.MinMaxScaler()</span><br><span class="line">    data = mm.fit_transform(data)</span><br><span class="line">    <span class="keyword">return</span> np.mat(data), width, height</span><br></pre></td></tr></table></figure>

<p>因为 jpg 格式的图像是三个通道 (R,G,B)，也就是一个像素点具有 3 个特征值。这里我们用 c1、c2、c3 来获取平面坐标点 (x,y) 的三个特征值，特征值是在 0-255 之间。</p>
<p>为了加快聚类的收敛，我们需要采用 Min-Max 规范化对数据进行规范化。我们定义的 load_data 函数返回的结果包括了针对 (R,G,B) 三个通道规范化的数据，以及图像的尺寸信息。在定义好 load_data 函数后，我们直接调用就可以得到相关信息，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载图像，得到规范化的结果img，以及图像尺寸</span></span><br><span class="line">img, width, height = load_data(<span class="string">'../test_data/kmeans/weixin.jpg'</span>)</span><br></pre></td></tr></table></figure>

<p>假设我们想要对图像分割成 2 部分，在聚类阶段，我们可以将聚类数设置为 2，这样图像就自动聚成 2 类。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用K-Means对图像进行2聚类</span></span><br><span class="line">kmeans =KMeans(n_clusters=<span class="number">2</span>)</span><br><span class="line">kmeans.fit(img)</span><br><span class="line">label = kmeans.predict(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图像聚类结果，转化成图像尺寸的矩阵</span></span><br><span class="line">label = label.reshape([width, height])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建个新图像pic_mark，用来保存图像聚类的结果，并设置不同的灰度值</span></span><br><span class="line">pic_mark = image.new(<span class="string">"L"</span>, (width, height))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(width):</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(height):</span><br><span class="line">        <span class="comment"># 根据类别设置图像灰度, 类别0 灰度值为255， 类别1 灰度值为127（label[x][y]=0/1</span></span><br><span class="line">        pic_mark.putpixel((x, y), int(<span class="number">256</span>/(label[x][y]+<span class="number">1</span>))<span class="number">-1</span>)</span><br><span class="line">pic_mark.save(<span class="string">"../test_data/kmeans/weixin_mark.jpg"</span>, <span class="string">"JPEG"</span>)</span><br></pre></td></tr></table></figure>

<p>代码中有一些参数，下面说一下这些参数的作用和设置方法：</p>
<ul>
<li><p>我们使用了 fit 和 predict 这两个函数来做数据的训练拟合和预测，因为传入的参数是一样的，我们可以同时进行 fit 和 predict 操作，这样我们可以直接使用 fit_predict(data) 得到聚类的结果。得到聚类的结果 label 后，实际上是一个一维的向量，我们需要把它转化成图像尺寸的矩阵。label 的聚类结果是从 0 开始统计的，当聚类数为 2 的时候，聚类的标识 label=0 或者 1。</p>
</li>
<li><p>如果你想对图像聚类的结果进行可视化，直接看 0 和 1 是看不出来的，还需要将 0 和 1 转化为灰度值。灰度值一般是在 0-255 的范围内，我们可以将 label=0 设定为灰度值 255，label=1 设定为灰度值 127。具体方法是用 <code>int(256/(label[x][y]+1))-1</code>。可视化的时候，主要是通过设置图像的灰度值进行显示。所以我们把聚类 label=0 的像素点都统一设置灰度值为 255，把聚类 label=1 的像素点都统一设置灰度值为 127。原来图像的灰度值是在 0-255 之间，现在就只有 2 种颜色（也就是灰度为 255，和灰度 127）。</p>
</li>
<li><p>有了这些灰度信息，我们就可以用 image.new 创建一个新的图像，用 putpixel 函数对新图像的点进行灰度值的设置，最后用 save 函数保存聚类的灰度图像。这样你就可以看到聚类的可视化结果了，如下图所示：</p>
</li>
</ul>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/weixin_mark.jpg" alt="weixin_mark" style="zoom:200%;">

<p>如果我们想要分割成 16 个部分，该如何对不同分类设置不同的颜色值呢？这里需要用到 skimage 工具包，它是图像处理工具包。你需要使用 pip install scikit-image 来进行安装。</p>
<p>这段代码可以将聚类标识矩阵转化为不同颜色的矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> color</span><br><span class="line"><span class="comment"># 参数改为16，16聚类</span></span><br><span class="line"><span class="comment"># kmeans = KMeans(n_clusters=16)</span></span><br><span class="line"><span class="comment"># 将聚类标识矩阵转化为不同颜色的矩阵</span></span><br><span class="line">label_color = (color.label2rgb(label)*<span class="number">255</span>).astype(np.uint8)</span><br><span class="line">label_color = label_color.transpose(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>)</span><br><span class="line">images = image.fromarray(label_color)</span><br><span class="line">images.save(<span class="string">'../test_data/kmeans/weixin_mark_color.jpg'</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>代码中，我使用 skimage 中的 label2rgb 函数来将 label 分类标识转化为颜色数值，因为我们的颜色值范围是[0,255]，所以还需要乘以 255 进行转化，最后再转化为 np.uint8 类型。unit8 类型代表无符号整数，范围是 0-255 之间。</p>
</li>
<li><p>得到颜色矩阵后，你可以把它输出出来，这时你发现输出的图像是颠倒的，原因可能是图像源拍摄的时候本身是倒置的。我们需要设置三维矩阵的转置，让第一维和第二维颠倒过来，也就是使用 transpose(1,0,2)，将原来的 (0,1,2）顺序转化为 (1,0,2) 顺序，即第一维和第二维互换。</p>
</li>
<li><p>最后我们使用 fromarray 函数，它可以通过矩阵来生成图片，并使用 save 进行保存。最后得到的分类标识颜色化图像是这样的：</p>
</li>
</ul>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/weixin_mark_color.jpg" alt="weixin_mark_color" style="zoom:200%;">

<p>刚才我们做的是聚类的可视化。如果我们想要看到对应的原图，可以将每个簇（即每个类别）的点的 RGB 值设置为该簇质心点的 RGB 值，也就是簇内的点的特征均为质心点的特征。</p>
<p>我给出了完整的代码，代码中，我可以把范围为 0-255 的数值投射到 1-256 数值之间，方法是对每个数值进行加 1，你可以自己来运行下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 使用K-means对图像进行聚类，并显示聚类压缩后的图像</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> image</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mpimg</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载图像，并对数据进行规范化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(filePath)</span>:</span></span><br><span class="line">    <span class="comment"># 读文件</span></span><br><span class="line">    f = open(filePath,<span class="string">'rb'</span>)</span><br><span class="line">    data = []</span><br><span class="line">    <span class="comment"># 得到图像的像素值</span></span><br><span class="line">    img = image.open(f)</span><br><span class="line">    <span class="comment"># 得到图像尺寸</span></span><br><span class="line">    width, height = img.size</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(width):</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(height):</span><br><span class="line">            <span class="comment"># 得到点(x,y)的三个通道值</span></span><br><span class="line">            c1, c2, c3 = img.getpixel((x, y))</span><br><span class="line">            data.append([(c1+<span class="number">1</span>)/<span class="number">256.0</span>, (c2+<span class="number">1</span>)/<span class="number">256.0</span>, (c3+<span class="number">1</span>)/<span class="number">256.0</span>])</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> np.mat(data), width, height</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 加载图像，得到规范化的结果imgData，以及图像尺寸</span></span><br><span class="line">img, width, height = load_data(<span class="string">'../test_data/kmeans/weixin.jpg'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用K-Means对图像进行16聚类</span></span><br><span class="line">kmeans =KMeans(n_clusters=<span class="number">16</span>)</span><br><span class="line">label = kmeans.fit_predict(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图像聚类结果，转化成图像尺寸的矩阵</span></span><br><span class="line">label = label.reshape([width, height])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前面代码和之前的一样，只是保存可视化图片部分不一样</span></span><br><span class="line"><span class="comment"># 创建个新图像img，用来保存图像聚类压缩后的结果</span></span><br><span class="line">img=image.new(<span class="string">'RGB'</span>, (width, height))</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(width):</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> range(height):</span><br><span class="line">        c1 = kmeans.cluster_centers_[label[x, y], <span class="number">0</span>]</span><br><span class="line">        c2 = kmeans.cluster_centers_[label[x, y], <span class="number">1</span>]</span><br><span class="line">        c3 = kmeans.cluster_centers_[label[x, y], <span class="number">2</span>]</span><br><span class="line">        img.putpixel((x, y), (int(c1*<span class="number">256</span>)<span class="number">-1</span>, int(c2*<span class="number">256</span>)<span class="number">-1</span>, int(c3*<span class="number">256</span>)<span class="number">-1</span>))</span><br><span class="line">img.save(<span class="string">'../test_data/kmeans/weixin_new.jpg'</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>可以看到没有用到 sklearn 自带的 MinMaxScaler，而是自己写了 Min-Max 规范化的公式。这样做的原因是我们知道 RGB 每个通道的数值在[0,255]之间，所以我们可以用每个通道的数值 +1/256，这样数值就会在[0,1]之间。（用MinMaxScaler效果也是一样的）</p>
</li>
<li><p>对图像做了 Min-Max 空间变换之后，还可以对其进行反变换，还原出对应原图的通道值。对于点 (x,y)，我们找到它们所属的簇 label[x,y]，然后得到这个簇的质心特征，用 c1,c2,c3 表示：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c1 = kmeans.cluster_centers_[label[x, y], <span class="number">0</span>]</span><br><span class="line">c2 = kmeans.cluster_centers_[label[x, y], <span class="number">1</span>]</span><br><span class="line">c3 = kmeans.cluster_centers_[label[x, y], <span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<ul>
<li>因为 c1, c2, c3 对应的是数据规范化的数值，因此我们还需要进行反变换，即：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c1=int(c1*<span class="number">256</span>)<span class="number">-1</span></span><br><span class="line">c2=int(c2*<span class="number">256</span>)<span class="number">-1</span></span><br><span class="line">c3=int(c3*<span class="number">256</span>)<span class="number">-1</span></span><br></pre></td></tr></table></figure>

<ul>
<li>然后用 img.putpixel 设置点 (x,y) 反变换后得到的特征值。最后用 img.save 保存图像。</li>
</ul>
<p>结果如下：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/weixin_new.jpg" alt="weixin_new" style="zoom:200%;">

<h3 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h3><p>好了，写到这关于KMeans，就要结束了。 下面快速的回顾一下：</p>
<p>首先，通过足球队聚类的例子引出了KMeans聚类的工作原理，简单来说两步，你可以回忆回忆。</p>
<p>然后，通过KMeans实现了对图像分割的实战，另外我们还学习了如何在 Python 中如何对图像进行读写，具体的代码如下，上文中也有相应代码，你也可以自己对应下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> image</span><br><span class="line"><span class="comment"># 得到图像的像素值</span></span><br><span class="line">img = image.open(f)</span><br><span class="line"><span class="comment"># 得到图像尺寸</span></span><br><span class="line">width, height = img.size</span><br></pre></td></tr></table></figure>

<p>这里会使用 PIL 这个工具包，它的英文全称叫 Python Imaging Library，顾名思义，它是 Python 图像处理标准库。同时我们也使用到了 skimage 工具包（scikit-image），它也是图像处理工具包。用过 Matlab 的同学知道，Matlab 处理起图像来非常方便。skimage 可以和它相媲美，集成了很多图像处理函数，其中对不同分类标识显示不同的颜色。在 Python 中图像处理工具包，我们用的是 skimage 工具包。</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/132.png" alt="132"></p>
<h2 id="21-EM聚类"><a href="#21-EM聚类" class="headerlink" title="21.EM聚类"></a>21.EM聚类</h2><p>EM 的英文是 Expectation Maximization，所以 EM 算法也叫<strong>最大期望算法</strong>。</p>
<p>先看一个简单的场景：假设你炒了一份菜，想要把它平均分到两个碟子里，该怎么分？</p>
<p>很少有人用称对菜进行称重，再计算一半的分量进行平分。大部分人的方法是先分一部分到碟子 A 中，然后再把剩余的分到碟子 B 中，再来观察碟子 A 和 B 里的菜是否一样多，哪个多就匀一些到少的那个碟子里，然后再观察碟子 A 和 B 里的是否一样多……整个过程一直重复下去，直到份量不发生变化为止。</p>
<p>你能从这个例子中看到三个主要的步骤：初始化参数、观察预期、重新估计。首先是先给每个碟子初始化一些菜量，然后再观察预期，这两个步骤实际上就是期望步骤（Expectation）。如果结果存在偏差就需要重新估计参数，这个就是最大化步骤（Maximization）。这两个步骤加起来也就是 EM 算法的过程。</p>
<h3 id="一、EM-算法的工作原理"><a href="#一、EM-算法的工作原理" class="headerlink" title="一、EM 算法的工作原理"></a>一、EM 算法的工作原理</h3><p>说到 EM 算法，我们先来看一个概念“最大似然”，英文是 Maximum Likelihood，Likelihood 代表可能性，所以最大似然也就是最大可能性的意思。</p>
<p>举个例子，有一男一女两个同学，现在要对他俩进行身高的比较，谁会更高呢？根据我们的经验，相同年龄下男性的平均身高比女性的高一些，所以男同学高的可能性会很大。这里运用的就是<strong>最大似然</strong>的概念。</p>
<p><strong>最大似然估计，</strong>指的就是一件事情已经发生了，然后反推更有可能是什么因素造成的。还是用一男一女比较身高为例，假设有一个人比另一个人高，反推他可能是男性。最大似然估计是一种通过已知结果，估计参数的方法。</p>
<p>EM 算法和最大似然估计的关系：EM 算法是一种求解最大似然估计的方法，通过观测样本，来找出样本的模型参数。</p>
<p>再回过来看下分菜的这个例子，实际上最终我们想要的是碟子 A 和碟子 B 中菜的份量，你可以把它们理解为想要求得的 模型参数 。然后我们通过 EM 算法中的 E 步来进行观察，然后通过 M 步来进行调整 A 和 B 的参数，最后让碟子 A 和碟子 B 的参数不再发生变化为止。</p>
<p>实际我们遇到的问题，比分菜复杂。举个一个投掷硬币的例子，假设我们有 A 和 B 两枚硬币，我们做了 5 组实验，每组实验投掷 10 次，然后统计出现正面的次数，实验结果如下：</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/133.png" alt="img"></p>
<p>投掷硬币这个过程中存在隐含的数据，即我们事先并不知道每次投掷的硬币是 A 还是 B。假设我们知道这个隐含的数据，并将它完善，可以得到下面的结果：</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/134.png" alt="img"></p>
<p>我们现在想要求得硬币 A 和 B 出现正面次数的概率，可以直接求得：</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/135.png" alt="img"></p>
<p>而实际情况是我不知道每次投掷的硬币是 A 还是 B，那么如何求得硬币 A 和硬币 B 出现正面的概率呢？</p>
<p>这里就需要采用 EM 算法的思想。</p>
<ol>
<li><p>初始化参数。我们假设硬币 A 和 B 的正面概率（随机指定）是θA=0.5 和θB=0.9。</p>
</li>
<li><p>计算期望值。假设实验 1 投掷的是硬币 A，那么正面次数为 5 的概率为：</p>
</li>
</ol>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/136.png" alt="img"></p>
<p>公式中的 C(10,5) 代表的是 10 个里面取 5 个的组合方式，也就是排列组合公式，0.5 的 5 次方乘以 0.5 的 5 次方代表的是其中一次为 5 次为正面，5 次为反面的概率，然后再乘以 C(10,5) 等于正面次数为 5 的概率。</p>
<p>假设实验 1 是投掷的硬币 B ，那么正面次数为 5 的概率为：</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/137.png" alt="img"></p>
<p>所以实验 1 更有可能投掷的是硬币 A。</p>
<p>然后我们对实验 2~5 重复上面的计算过程，可以推理出来硬币顺序应该是{A，A，B，B，A}。</p>
<p>这个过程实际上是通过假设的参数来估计未知参数，即“每次投掷是哪枚硬币”。</p>
<ol start="3">
<li>通过猜测的结果{A, A, B, B, A}来完善初始化的参数θA 和θB。</li>
</ol>
<p>然后一直重复第二步和第三步，直到参数不再发生变化。</p>
<p>简单总结下上面的步骤，你能看出 EM 算法中的 E 步骤就是通过旧的参数来计算隐藏变量。然后在 M 步骤中，通过得到的隐藏变量的结果来重新估计参数。直到参数不再发生变化，得到我们想要的结果。</p>
<h3 id="二、EM-聚类的工作原理"><a href="#二、EM-聚类的工作原理" class="headerlink" title="二、EM 聚类的工作原理"></a>二、EM 聚类的工作原理</h3><p>上面你能看到 EM 算法最直接的应用就是求参数估计。如果我们把潜在类别当做隐藏变量，样本看做观察值，就可以把聚类问题转化为参数估计问题。这也就是 EM 聚类的原理。</p>
<p>相比于 K-Means 算法，EM 聚类更加灵活，比如下面这两种情况，K-Means 会得到下面的聚类结果。</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/138.jpg" alt="img"></p>
<p>因为 K-Means 是通过距离来区分样本之间的差别的，且每个样本在计算的时候只能属于一个分类，称之为是硬聚类算法。而 EM 聚类在求解的过程中，实际上每个样本都有一定的概率和每个聚类相关，叫做<strong>软聚类算法</strong>。</p>
<p>你可以把 EM 算法理解成为是一个<strong>框架</strong>，在这个框架中可以采用不同的模型来用 EM 进行求解。</p>
<p>常用的 EM 聚类有 GMM 高斯混合模型和 HMM 隐马尔科夫模型。<strong>GMM（高斯混合模型）聚类</strong>就是 EM 聚类的一种。比如上面这两个图，可以采用 GMM 来进行聚类。</p>
<p><strong>E 步骤</strong>：和 K-Means 一样，我们事先知道聚类的个数，但是不知道每个样本分别属于哪一类。通常，我们可以假设样本是符合高斯分布的（也就是正态分布）。每个高斯分布都属于这个模型的组成部分（component），要分成 K 类就相当于是 K 个组成部分。这样我们可以先初始化每个组成部分的高斯分布的参数，然后再看来每个样本是属于哪个组成部分。这也就是 E 步骤。</p>
<p><strong>M 步骤</strong>：再通过得到的这些隐含变量结果，反过来求每个组成部分高斯分布的参数，即 M 步骤。</p>
<p><strong>反复</strong> EM 步骤，直到每个组成部分的高斯分布参数不变为止。</p>
<p>这样也就相当于将样本按照 GMM 模型进行了 EM 聚类。</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/139.jpg" alt="img"></p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/143.png" alt="img"></p>
<h3 id="三、-如何使用-EM-工具包"><a href="#三、-如何使用-EM-工具包" class="headerlink" title="三、 如何使用 EM 工具包"></a>三、 如何使用 EM 工具包</h3><p>在 Python 中有第三方的 EM 算法工具包。由于 EM 算法是一个聚类框架，所以你需要明确你要用的具体算法，比如是采用 GMM 高斯混合模型，还是 HMM 隐马尔科夫模型。</p>
<p>我们主要讲解 GMM 的使用，在使用前你需要引入工具包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br></pre></td></tr></table></figure>

<p>如何在 sklearn 中创建 GMM 聚类。</p>
<p>首先我们使用 gmm = GaussianMixture(n_components=1, covariance_type=‘full’, max_iter=100) 来创建 GMM 聚类，其中有几个比较主要的参数（GMM 类的构造参数比较多，我筛选了一些主要的进行讲解），我分别来讲解下：</p>
<ul>
<li><p>n_components：即高斯混合模型的个数，也就是我们要聚类的个数，默认值为 1。如果你不指定 n_components，最终的聚类结果都会为同一个值。</p>
</li>
<li><p>covariance_type：代表协方差类型。一个高斯混合模型的分布是由均值向量和协方差矩阵决定的，所以协方差的类型也代表了不同的高斯混合模型的特征。协方差类型有 4 种取值：</p>
<p>covariance_type=full，代表完全协方差，也就是元素都不为 0；</p>
<p>covariance_type=tied，代表相同的完全协方差；</p>
<p>covariance_type=diag，代表对角协方差，也就是对角不为 0，其余为 0；</p>
<p>covariance_type=spherical，代表球面协方差，非对角为 0，对角完全相同，呈现球面的特性。</p>
</li>
<li><p>max_iter：代表最大迭代次数，EM 算法是由 E 步和 M 步迭代求得最终的模型参数，这里可以指定最大迭代次数，默认值为 100。</p>
</li>
</ul>
<p>创建完 GMM 聚类器之后，我们就可以传入数据让它进行迭代拟合。</p>
<p>我们使用 fit 函数，传入样本特征矩阵，模型会自动生成聚类器，然后使用 prediction=gmm.predict(data) 来对数据进行聚类，传入你想进行聚类的数据，可以得到聚类结果 prediction。</p>
<p>能看出来拟合训练和预测可以传入相同的特征矩阵，这是因为聚类是<strong>无监督学习</strong>，你不需要事先指定聚类的结果，也无法基于先验的结果经验来进行学习。只要在训练过程中传入特征值矩阵，机器就会按照特征值矩阵生成聚类器，然后就可以使用这个聚类器进行聚类了。</p>
<h3 id="四、-如何用-EM-算法对王者荣耀数据进行聚类"><a href="#四、-如何用-EM-算法对王者荣耀数据进行聚类" class="headerlink" title="四、 如何用 EM 算法对王者荣耀数据进行聚类"></a>四、 如何用 EM 算法对王者荣耀数据进行聚类</h3><p>了解了 GMM 聚类工具之后，我们看下如何对王者荣耀的英雄数据进行聚类。</p>
<p>首先我们知道聚类的原理是“人以群分，物以类聚”。通过聚类算法把特征值相近的数据归为一类，不同类之间的差异较大，这样就可以对原始数据进行降维。通过分成几个组（簇），来研究每个组之间的特性。或者我们也可以把组（簇）的数量适当提升，这样就可以找到可以互相替换的英雄，比如你的对手选择了你擅长的英雄之后，你可以选择另一个英雄作为备选。</p>
<p>我们先看下数据长什么样子：</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/140.png" alt="img"></p>
<p>这里我们收集了 69 名英雄的 20 个特征属性，这些属性分别是最大生命、生命成长、初始生命、最大法力、法力成长、初始法力、最高物攻、物攻成长、初始物攻、最大物防、物防成长、初始物防、最大每 5 秒回血、每 5 秒回血成长、初始每 5 秒回血、最大每 5 秒回蓝、每 5 秒回蓝成长、初始每 5 秒回蓝、最大攻速和攻击范围等。</p>
<p>数据下载： <a href="https://github.com/cystanford/EM_data" target="_blank" rel="noopener">https://github.com/cystanford/EM_data</a> 。</p>
<p>现在我们需要对王者荣耀的英雄数据进行聚类，我们先设定项目的执行流程：</p>
<ol>
<li><p>加载数据源；</p>
</li>
<li><p>在准备阶段，我们需要对数据进行探索，包括采用数据可视化技术，让我们对英雄属性以及这些属性之间的关系理解更加深刻，然后对数据质量进行评估，是否进行数据清洗，最后进行特征选择方便后续的聚类算法；</p>
</li>
<li><p>聚类阶段：选择适合的聚类模型，这里我们采用 GMM 高斯混合模型进行聚类，并输出聚类结果，对结果进行分析。</p>
</li>
</ol>
<p>按照上面的步骤，我们来编写下代码。完整的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载，避免中文乱码问题</span></span><br><span class="line">data_ori = pd.read_csv(<span class="string">'../test_data/EM_data/heros.csv'</span>, encoding=<span class="string">'gb18030'</span>)</span><br><span class="line">features = [<span class="string">u'最大生命'</span>, <span class="string">u'生命成长'</span>, <span class="string">u'初始生命'</span>, <span class="string">u'最大法力'</span>, <span class="string">u'法力成长'</span>, <span class="string">u'初始法力'</span>, <span class="string">u'最高物攻'</span>, <span class="string">u'物攻成长'</span>, <span class="string">u'初始物攻'</span>,<span class="string">u'最大物防'</span>, <span class="string">u'物防成长'</span>, <span class="string">u'初始物防'</span>, <span class="string">u'最大每5秒回血'</span>, <span class="string">u'每5秒回血成长'</span>, <span class="string">u'初始每5秒回血'</span>, <span class="string">u'最大每5秒回蓝'</span>,<span class="string">u'每5秒回蓝成长'</span>, <span class="string">u'初始每5秒回蓝'</span>, <span class="string">u'最大攻速'</span>, <span class="string">u'攻击范围'</span>]</span><br><span class="line">data = data_ori[features]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对英雄属性之间的关系进行可视化分析</span></span><br><span class="line"><span class="comment"># 设置 plt 正确显示中文(我的matplotlib已经配置过，配置方法见前面可视化蜘蛛图那里)</span></span><br><span class="line"><span class="comment"># plt.rcParams['font.sans-serif']=['SimHei'] # 用来正常显示中文标签</span></span><br><span class="line"><span class="comment"># plt.rcParams['axes.unicode_minus']=False # 用来正常显示负号</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用热力图呈现 features_mean 字段之间的相关性</span></span><br><span class="line"><span class="comment"># corr是DataFrame函数，相关系数，结合热力图使用，SVM分类实例中也有使用</span></span><br><span class="line">corr = data[features].corr()</span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>, <span class="number">14</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># annot=True 显示每个方格的数据</span></span><br><span class="line">sns.heatmap(corr, annot=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据相关系数热力图，相关性大的属性保留一个，因此可以对属性进行降维</span></span><br><span class="line">features_remain = [<span class="string">u'最大生命'</span>, <span class="string">u'初始生命'</span>, <span class="string">u'最大法力'</span>, <span class="string">u'最高物攻'</span>, <span class="string">u'初始物攻'</span>, <span class="string">u'最大物防'</span>, <span class="string">u'初始物防'</span>, <span class="string">u'最大每 5 秒回血'</span>, <span class="string">u'最大每 5 秒回蓝'</span>, <span class="string">u'初始每 5 秒回蓝'</span>, <span class="string">u'最大攻速'</span>, <span class="string">u'攻击范围'</span>]</span><br><span class="line">data = data_ori[features_remain]</span><br><span class="line"></span><br><span class="line"><span class="comment"># strip可以删除两边空格，这么用可以删除特殊符号</span></span><br><span class="line">data[<span class="string">u'最大攻速'</span>] = data[<span class="string">u'最大攻速'</span>].apply(<span class="keyword">lambda</span> x: float(x.strip(<span class="string">'%'</span>))/<span class="number">100</span>)</span><br><span class="line">data[<span class="string">u'攻击范围'</span>]=data[<span class="string">u'攻击范围'</span>].map(&#123;<span class="string">'远程'</span>:<span class="number">1</span>,<span class="string">'近战'</span>:<span class="number">0</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用 Z-Score 规范化数据，保证每个特征维度的数据均值为 0，方差为 1</span></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">data = ss.fit_transform(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造 GMM 聚类</span></span><br><span class="line">gmm = GaussianMixture(n_components=<span class="number">30</span>, covariance_type=<span class="string">'full'</span>)</span><br><span class="line">gmm.fit(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练数据</span></span><br><span class="line">prediction = gmm.predict(data)</span><br><span class="line">print(prediction)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将分组结果输出到 CSV 文件中</span></span><br><span class="line"><span class="comment"># insert增加列，0插入位置</span></span><br><span class="line">data_ori.insert(<span class="number">0</span>, <span class="string">'分组'</span>, prediction)</span><br><span class="line">data_ori.to_csv(<span class="string">'./hero_out.csv'</span>, index=<span class="literal">False</span>, sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure>

<p>运行结果如下：</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/141.png" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[28 14  8  9  5  5 15  8  3 14 18 14  9  7 16 18 13  3  5  4 19 12  4 12</span><br><span class="line">12 12  4 17 24  2  7  2  2 24  2  2 24  6 20 22 22 24 24  2  2 22 14 20</span><br><span class="line">14 24 26 29 27 25 25 28 11  1 23  5 11  0 10 28 21 29 29 29 17]</span><br></pre></td></tr></table></figure>

<p>(每次聚类标号可能不同，但分类结果是一样的)</p>
<p>同时你也能看到输出的聚类结果文件 hero_out.csv（它保存在你本地运行的文件夹里，程序会自动输出这个文件，你可以自己看下）。</p>
<p>讲解下程序的几个模块。</p>
<ul>
<li><p>数据可视化的探索</p>
<p>你能看到我们将 20 个英雄属性之间的关系用热力图呈现了出来，中间的数字代表两个属性之间的关系系数，最大值为 1，代表完全正相关，关系系数越大代表相关性越大。从图中你能看出来“最大生命”“生命成长”和“初始生命”这三个属性的相关性大，我们只需要保留一个属性即可。同理我们也可以对其他相关性大的属性进行筛选，保留一个。你在代码中可以看到，我用 features_remain 数组保留了特征选择的属性，这样就将原本的 20 个属性降维到了 13 个属性。</p>
</li>
<li><p>关于数据规范化</p>
<p>“最大攻速”这个属性值是百分数，不适合做矩阵运算，因此需要将百分数转化为小数。“攻击范围”这个字段的取值为远程或者近战，也不适合矩阵运算，将取值做个映射，用 1 代表远程，0 代表近战。然后采用 Z-Score 规范化，对特征矩阵进行规范化。</p>
</li>
<li><p>在聚类阶段</p>
<p>我们采用了 GMM 高斯混合模型，并将结果输出到 CSV 文件中。</p>
<p>这里将输出的结果截取了一段（设置聚类个数为 30）：</p>
</li>
</ul>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/142.png" alt="img"></p>
<p>第一列代表的是分组（簇），我们能看到张飞、程咬金分到了一组，牛魔、白起是一组，老夫子自己是一组，达摩、典韦是一组。</p>
<p>聚类的特点是相同类别之间的属性值相近，不同类别的属性值差异大。因此如果你擅长用典韦这个英雄，不防试试达摩这个英雄。同样你也可以在张飞和程咬金中进行切换。这样就算你的英雄被别人选中了，你依然可以有备选的英雄可以使用。</p>
<p>聚类和分类不一样，聚类是无监督的学习方式，也就是我们没有实际的结果可以进行比对，所以聚类的结果评估不像分类准确率一样直观，那么有没有聚类结果的评估方式呢？这里我们可以采用 Calinski-Harabaz 指标，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> calinski_harabaz_score</span><br><span class="line">print(calinski_harabaz_score(data, prediction))</span><br></pre></td></tr></table></figure>

<p>指标分数越高，代表聚类效果越好，也就是相同类中的差异性小，不同类之间的差异性大。当然具体聚类的结果含义，我们需要人工来分析，也就是当这些数据被分成不同的类别之后，具体每个类表代表的含义。</p>
<p>另外聚类算法也可以作为其他数据挖掘算法的预处理阶段，这样我们就可以将数据进行降维了。<br><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/144.png" alt="img"></p>
<h2 id="22-关联规则挖掘（Apriori、FP-Growth算法）"><a href="#22-关联规则挖掘（Apriori、FP-Growth算法）" class="headerlink" title="22.关联规则挖掘（Apriori、FP-Growth算法）"></a>22.关联规则挖掘（Apriori、FP-Growth算法）</h2><h3 id="一、-关联规则中的几个概念"><a href="#一、-关联规则中的几个概念" class="headerlink" title="一、 关联规则中的几个概念"></a>一、 关联规则中的几个概念</h3><p>关联规则这个概念，最早是由 Agrawal 等人在 1993 年提出的。在 1994 年 Agrawal 等人又提出了基于<strong>关联规则</strong>的 Apriori 算法，至今 Apriori 仍是关联规则挖掘的重要算法。</p>
<p>关联规则挖掘可以让我们从数据集中发现项与项（item 与 item）之间的关系，它在我们的生活中有很多应用场景，“购物篮分析”就是一个常见的场景，这个场景可以从消费者交易记录中发掘商品与商品之间的关联关系，进而通过商品捆绑销售或者相关推荐的方式带来更多的销售量。所以说，关联规则挖掘是个非常有用的技术。</p>
<p>举一个超市购物的例子，下面是几名客户购买的商品列表：</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/145.png" alt="img"></p>
<p><strong>支持度</strong>:</p>
<p>支持度是个百分比，它指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大。</p>
<p>在这个例子中，我们能看到“牛奶”出现了 4 次，那么这 5 笔订单中“牛奶”的支持度就是 4/5=0.8。</p>
<p>同样“牛奶 + 面包”出现了 3 次，那么这 5 笔订单中“牛奶 + 面包”的支持度就是 3/5=0.6。</p>
<p><strong>置信度:</strong></p>
<p>它指的就是当你购买了商品 A，会有多大的概率购买商品 B，在上面这个例子中：</p>
<p>置信度（牛奶→啤酒）=2/4=0.5，代表如果你购买了牛奶，有多大的概率会购买啤酒？</p>
<p>置信度（啤酒→牛奶）=2/3=0.67，代表如果你购买了啤酒，有多大的概率会购买牛奶？</p>
<p>我们能看到，在 4 次购买了牛奶的情况下，有 2 次购买了啤酒，所以置信度 (牛奶→啤酒)=0.5，而在 3 次购买啤酒的情况下，有 2 次购买了牛奶，所以置信度（啤酒→牛奶）=0.67。</p>
<p>所以说置信度是个条件概念，就是说在 A 发生的情况下，B 发生的概率是多少。</p>
<p><strong>提升度:</strong></p>
<p>我们在做商品推荐的时候，重点考虑的是提升度，因为提升度代表的是“商品 A 的出现，对商品 B 的出现概率提升的”程度。</p>
<p>还是看上面的例子，如果我们单纯看置信度 (可乐→尿布)=1，也就是说可乐出现的时候，用户都会购买尿布，那么当用户购买可乐的时候，我们就需要推荐尿布么？</p>
<p>实际上，就算用户不购买可乐，也会直接购买尿布的，所以用户是否购买可乐，对尿布的提升作用并不大。我们可以用下面的公式来计算商品 A 对商品 B 的提升度：</p>
<p>提升度 (A→B)= 置信度 (A→B)/ 支持度 (B)</p>
<p>这个公式是用来衡量 A 出现的情况下，是否会对 B 出现的概率有所提升。</p>
<p>所以提升度有三种可能：</p>
<p>提升度 (A→B)&gt;1：代表有提升；</p>
<p>提升度 (A→B)=1：代表有没有提升，也没有下降；</p>
<p>提升度 (A→B)&lt;1：代表有下降。</p>
<h3 id="二、-Apriori-的工作原理"><a href="#二、-Apriori-的工作原理" class="headerlink" title="二、 Apriori 的工作原理"></a>二、 Apriori 的工作原理</h3><p>首先我们把上面案例中的商品用 ID 来代表，牛奶、面包、尿布、可乐、啤酒、鸡蛋的商品 ID 分别设置为 1-6，上面的数据表可以变为：</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/146.png" alt="img"></p>
<p>Apriori 算法其实就是查找<strong>频繁项集</strong> (frequent itemset) 的过程，所以首先我们需要定义什么是频繁项集。</p>
<p>频繁项集就是支持度大于等于最小支持度 (Min Support) 阈值的项集，所以小于最小值支持度的项目就是非频繁项集，而大于等于最小支持度的项集就是频繁项集。</p>
<p>项集这个概念，英文叫做 itemset，它可以是单个的商品，也可以是商品的组合。</p>
<p>再来看这个例子，假设我随机指定最小支持度是 50%，也就是 0.5，Apriori 算法是如何运算的。</p>
<ol>
<li>首先，计算单个商品的支持度，也就是得到 K=1 项的支持度：</li>
</ol>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/147.png" alt="img"></p>
<ol start="2">
<li>因为最小支持度是 0.5，所以商品 4、6 是不符合最小支持度的，不属于频繁项集，于是经过筛选商品的频繁项集就变成：</li>
</ol>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/148.png" alt="img"></p>
<ol start="3">
<li>在这个基础上，将商品两两组合，得到 k=2 项的支持度：</li>
</ol>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/149.png" alt="img"></p>
<ol start="4">
<li>再筛掉小于最小值支持度的商品组合，可以得到：</li>
</ol>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/150.png" alt="img"></p>
<ol start="5">
<li>再将商品进行 K=3 项的商品组合，可以得到：</li>
</ol>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/151.png" alt="img"></p>
<ol start="6">
<li>再筛掉小于最小值支持度的商品组合，可以得到：</li>
</ol>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/152.png" alt="img"></p>
<p>到这里，模拟了一遍整个 Apriori 算法的流程，总结下 Apriori 算法的递归流程：</p>
<ol>
<li><p>K=1，计算 K 项集的支持度；</p>
</li>
<li><p>筛选掉小于最小支持度的项集；</p>
</li>
<li><p>如果项集为空，则对应 K-1 项集的结果为最终结果。</p>
<p>否则 K=K+1，重复 1-3 步。</p>
</li>
</ol>
<h3 id="三、-Apriori-的改进算法：FP-Growth-算法"><a href="#三、-Apriori-的改进算法：FP-Growth-算法" class="headerlink" title="三、 Apriori 的改进算法：FP-Growth 算法"></a>三、 Apriori 的改进算法：FP-Growth 算法</h3><p>Apriori 在计算的过程中有以下几个<strong>缺点</strong>：</p>
<ul>
<li><p>可能产生大量的候选集。因为采用排列组合的方式，把可能的项集都组合出来了；</p>
</li>
<li><p>每次计算都需要重新扫描数据集，来计算每个项集的支持度。</p>
</li>
</ul>
<br>

<p>所以 Apriori 算法会浪费很多计算空间和计算时间，为此人们提出了 <strong>FP-Growth 算法</strong>，它的特点是：</p>
<p>创建了一棵 FP 树来存储频繁项集。在创建前对不满足最小支持度的项进行删除，减少了存储空间。我稍后会讲解如何构造一棵 FP 树；</p>
<p>整个生成过程只遍历数据集 2 次，大大减少了计算量。</p>
<p>所以在实际工作中，我们<strong>常用</strong> FP-Growth 来做频繁项集的挖掘，下面我给你简述下 FP-Growth 的原理。</p>
<p><strong>1. 创建项头表（item header table）</strong></p>
<p>创建项头表的作用是为 FP 构建及频繁项集挖掘提供索引。</p>
<p><strong>流程</strong>：先扫描一遍数据集，对于<strong>满足最小支持度</strong>的单个项（K=1 项集）按照支持度从高到低进行排序，这个过程中删除了不满足最小支持度的项。</p>
<p>项头表包括了项目、支持度，以及该项在 FP 树中的链表。初始的时候链表为空。</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/153.png" alt="img"></p>
<p><strong>2. 构造 FP 树</strong></p>
<p>FP 树的根节点记为 NULL 节点。</p>
<p><strong>流程</strong>：需要再次扫描数据集，对于每一条数据，按照项头表中项目、支持度从高到低的顺序进行创建节点（也就是第一步中项头表中的排序结果），节点如果存在就将计数 count+1，如果不存在就进行创建。同时在创建的过程中，需要更新项头表的链表。（5个尿布订单中–&gt;4个牛奶订单中–&gt;3个面包订单，还有1个面包订单中–&gt;1个啤酒订单，还有1个啤酒，还有1个啤酒）</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/154.png" alt="img"></p>
<p><strong>3. 通过 FP 树挖掘频繁项集</strong></p>
<p>到这里，我们就得到了一个存储频繁项集的 FP 树，以及一个项头表。我们可以通过项头表来挖掘出<strong>每个频繁项集</strong>。</p>
<p>具体的操作会用到一个概念，叫“<strong>条件模式基</strong>”，它指的是以要挖掘的节点为叶子节点，自底向上求出 FP 子树，然后将 FP 子树的祖先节点设置为叶子节点之和。</p>
<p>我以“啤酒”的节点为例，从 FP 树中可以得到一棵 FP 子树，将祖先节点的支持度记为叶子节点之和，得到：</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/155.png" alt="img"></p>
<p>相比于原来的 FP 树，尿布和牛奶的频繁项集数减少了。这是因为我们求得的是以“啤酒”为节点的 FP 子树，也就是说，在频繁项集中一定要含有“啤酒”这个项。你可以再看下原始的数据，其中订单 1{牛奶、面包、尿布}和订单 5{牛奶、面包、尿布、可乐}并不存在“啤酒”这个项，所以针对订单 1，尿布→牛奶→面包这个项集就会从 FP 树中去掉，针对订单 5 也包括了尿布→牛奶→面包这个项集也会从 FP 树中去掉，所以你能看到以“啤酒”为节点的 FP 子树，尿布、牛奶、面包项集上的计数比原来少了 2。</p>
<p>条件模式基不包括“啤酒”节点，而且祖先节点如果小于最小支持度就会被剪枝，所以“啤酒”的条件模式基为空。</p>
<p>同理，我们可以求得“面包”的条件模式基为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/156.png" alt="img" style="zoom:50%;">

<p>所以可以求得面包的频繁项集为{尿布，面包}，{尿布，牛奶，面包}。同样，我们还可以求得牛奶，尿布的频繁项集，这里就不再计算展示。</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/160.png" alt="img"></p>
<h3 id="四、-使用-Apriori-工具包"><a href="#四、-使用-Apriori-工具包" class="headerlink" title="四、 使用 Apriori 工具包"></a>四、 使用 Apriori 工具包</h3><p>Apriori 虽然是十大算法之一，不过在 sklearn 工具包中并没有它，也没有 FP-Growth 算法。这里教你个方法，来选择 Python 中可以使用的工具包，你可以通过 <a href="https://pypi.org/" target="_blank" rel="noopener">https://pypi.org/</a> 搜索工具包。</p>
<p>这个网站提供的工具包都是 Python 语言的，能找到 8 个 Python 语言的 Apriori 工具包，具体选择哪个呢？建议你使用第二个工具包，即 efficient-apriori。后面会讲到为什么推荐这个工具包。</p>
<p>可以通过 pip install efficient-apriori 安装这个工具包。</p>
<p>（实际上pycharm可以直接安装）</p>
<br>

<p>然后看下如何使用它，核心的代码就是这一行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">itemsets, rules = apriori(data, min_support,  min_confidence)</span><br></pre></td></tr></table></figure>

<ul>
<li>data 是我们要提供的数据集，它是一个 <strong>list 数组类型</strong></li>
<li>min_support 参数为<strong>最小支持度</strong>，在 efficient-apriori 工具包中用 0 到 1 的数值代表百分比，比如 0.5 代表最小支持度为 50%。</li>
<li>min_confidence 是<strong>最小置信度</strong>，数值也代表百分比，比如 1 代表 100%。</li>
</ul>
<p>接下来我们用这个工具包，跑一下前面讲到的超市购物的例子。下面是客户购买的商品列表：</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/157.png" alt="img"></p>
<p>具体实现的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> efficient_apriori <span class="keyword">import</span> apriori</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置数据集</span></span><br><span class="line">data = [(<span class="string">'牛奶'</span>, <span class="string">'面包'</span>, <span class="string">'尿布'</span>),</span><br><span class="line">        (<span class="string">'可乐'</span>, <span class="string">'面包'</span>, <span class="string">'尿布'</span>, <span class="string">'啤酒'</span>),</span><br><span class="line">        (<span class="string">'牛奶'</span>, <span class="string">'尿布'</span>, <span class="string">'啤酒'</span>, <span class="string">'鸡蛋'</span>),</span><br><span class="line">        (<span class="string">'面包'</span>, <span class="string">'牛奶'</span>, <span class="string">'尿布'</span>, <span class="string">'啤酒'</span>),</span><br><span class="line">        (<span class="string">'面包'</span>, <span class="string">'牛奶'</span>, <span class="string">'尿布'</span>, <span class="string">'可乐'</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挖掘频繁项集和频繁规则</span></span><br><span class="line">itemsets, rules = apriori(data, min_support=<span class="number">0.5</span>,  min_confidence=<span class="number">1</span>)</span><br><span class="line">print(itemsets)</span><br><span class="line">print(rules)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;1: &#123;(&apos;牛奶&apos;,): 4, (&apos;尿布&apos;,): 5, (&apos;面包&apos;,): 4, (&apos;啤酒&apos;,): 3&#125;, 2: &#123;(&apos;尿布&apos;, &apos;牛奶&apos;): 4, (&apos;尿布&apos;, &apos;面包&apos;): 4, (&apos;牛奶&apos;, &apos;面包&apos;): 3, (&apos;啤酒&apos;, &apos;尿布&apos;): 3&#125;, 3: &#123;(&apos;尿布&apos;, &apos;牛奶&apos;, &apos;面包&apos;): 3&#125;&#125;</span><br><span class="line">[&#123;牛奶&#125; -&gt; &#123;尿布&#125;, &#123;面包&#125; -&gt; &#123;尿布&#125;, &#123;啤酒&#125; -&gt; &#123;尿布&#125;, &#123;牛奶, 面包&#125; -&gt; &#123;尿布&#125;]</span><br></pre></td></tr></table></figure>

<p>你能从代码中看出来，data 是个 List 数组类型，其中每个值都可以是一个集合。实际上你也可以把 data 数组中的每个值设置为 List 数组类型，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="string">'牛奶'</span>,<span class="string">'面包'</span>,<span class="string">'尿布'</span>],</span><br><span class="line">        [<span class="string">'可乐'</span>,<span class="string">'面包'</span>, <span class="string">'尿布'</span>, <span class="string">'啤酒'</span>],</span><br><span class="line">        [<span class="string">'牛奶'</span>,<span class="string">'尿布'</span>, <span class="string">'啤酒'</span>, <span class="string">'鸡蛋'</span>],</span><br><span class="line">        [<span class="string">'面包'</span>, <span class="string">'牛奶'</span>, <span class="string">'尿布'</span>, <span class="string">'啤酒'</span>],</span><br><span class="line">        [<span class="string">'面包'</span>, <span class="string">'牛奶'</span>, <span class="string">'尿布'</span>, <span class="string">'可乐'</span>]]</span><br></pre></td></tr></table></figure>

<p>两者的运行结果是一样的，efficient-apriori 工具包把每一条数据集里的项式都放到了一个集合中进行运算，并没有考虑它们之间的先后顺序。因为实际情况下，同一个购物篮中的物品也不需要考虑购买的先后顺序。</p>
<p>而其他的 Apriori 算法可能会因为考虑了先后顺序，出现计算频繁项集结果不对的情况。所以这里采用的是 efficient-apriori 这个工具包。</p>
<h3 id="五、-挖掘导演是如何选择演员的"><a href="#五、-挖掘导演是如何选择演员的" class="headerlink" title="五、 挖掘导演是如何选择演员的"></a>五、 挖掘导演是如何选择演员的</h3><p>在实际工作中，数据集是需要自己来准备的，比如我们要挖掘导演是如何选择演员的数据情况，但是并没有公开的数据集可以直接使用。因此我们需要使用之前讲到的 <strong>Python 爬虫</strong>进行数据采集。</p>
<p>不同导演选择演员的规则是不同的，因此我们需要先指定导演。数据源我们选用豆瓣电影。</p>
<ol>
<li>采集的工作流程:</li>
</ol>
<p>首先我们先在 <a href="https://movie.douban.com/" target="_blank" rel="noopener">https://movie.douban.com</a> 搜索框中输入导演姓名，比如“宁浩”。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/158.png" alt="img" style="zoom: 33%;">

<p>页面会呈现出来导演之前的所有电影，然后对页面进行观察，你能观察到以下几个现象：</p>
<p>页面默认是 15 条数据反馈，第一页会返回 16 条。因为第一条数据实际上这个导演的概览，你可以理解为是一条广告的插入，下面才是真正的返回结果。</p>
<p>每条数据的最后一行是电影的演出人员的信息，第一个人员是导演，其余为演员姓名。姓名之间用“/”分割。</p>
<p>有了这些观察之后，我们就可以编写抓取程序了。在代码讲解中你能看出这两点观察的作用。抓取程序的目的是为了生成宁浩导演（你也可以抓取其他导演）的数据集，结果会保存在 csv 文件中。完整的抓取代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 下载某个导演的电影数据集</span></span><br><span class="line"><span class="keyword">from</span> efficient_apriori <span class="keyword">import</span> apriori</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">driver = webdriver.Edge()</span><br><span class="line"><span class="comment"># 设置想要下载的导演 数据集</span></span><br><span class="line">director = <span class="string">u'宁浩'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写 CSV 文件</span></span><br><span class="line">file_name = <span class="string">'./'</span> + director + <span class="string">'.csv'</span></span><br><span class="line">base_url = <span class="string">'https://movie.douban.com/subject_search?search_text='</span>+director+<span class="string">'&amp;cat=1002&amp;start='</span></span><br><span class="line">out = open(file_name,<span class="string">'w'</span>, newline=<span class="string">''</span>, encoding=<span class="string">'utf-8-sig'</span>)</span><br><span class="line">csv_write = csv.writer(out, dialect=<span class="string">'excel'</span>)</span><br><span class="line">flags=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载指定页面的数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(request_url)</span>:</span></span><br><span class="line">	driver.get(request_url)</span><br><span class="line">	time.sleep(<span class="number">1</span>)</span><br><span class="line">	html = driver.find_element_by_xpath(<span class="string">"//*"</span>).get_attribute(<span class="string">"outerHTML"</span>)</span><br><span class="line">	html = etree.HTML(html)</span><br><span class="line">  </span><br><span class="line">	<span class="comment"># 设置电影名称，导演演员 的 XPATH</span></span><br><span class="line">	movie_lists = html.xpath(<span class="string">"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']"</span>)</span><br><span class="line">	name_lists = html.xpath(<span class="string">"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='meta abstract_2']"</span>)</span><br><span class="line">  </span><br><span class="line">	<span class="comment"># 获取返回的数据个数</span></span><br><span class="line">	num = len(movie_lists)</span><br><span class="line">	<span class="keyword">if</span> num &gt; <span class="number">15</span>: <span class="comment"># 第一页会有 16 条数据</span></span><br><span class="line">		<span class="comment"># 默认第一个不是，所以需要去掉</span></span><br><span class="line">		movie_lists = movie_lists[<span class="number">1</span>:]</span><br><span class="line">		name_lists = name_lists[<span class="number">1</span>:]</span><br><span class="line">	<span class="keyword">for</span> (movie, name_list) <span class="keyword">in</span> zip(movie_lists, name_lists):</span><br><span class="line">		<span class="comment"># 会存在数据为空的情况</span></span><br><span class="line">		<span class="keyword">if</span> name_list.text <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		<span class="comment"># 显示下演员名称</span></span><br><span class="line">		print(name_list.text)</span><br><span class="line">		names = name_list.text.split(<span class="string">'/'</span>)</span><br><span class="line">		<span class="comment"># 判断导演是否为指定的 director</span></span><br><span class="line">		<span class="keyword">if</span> names[<span class="number">0</span>].strip() == director <span class="keyword">and</span> movie.text <span class="keyword">not</span> <span class="keyword">in</span> flags:</span><br><span class="line">			<span class="comment"># 将第一个字段设置为电影名称</span></span><br><span class="line">			names[<span class="number">0</span>] = movie.text</span><br><span class="line">			flags.append(movie.text)</span><br><span class="line">			csv_write.writerow(names)</span><br><span class="line">	print(<span class="string">'OK'</span>) <span class="comment"># 代表这页数据下载成功</span></span><br><span class="line">	print(num)</span><br><span class="line">	<span class="keyword">if</span> num &gt;= <span class="number">14</span>: <span class="comment"># 有可能一页会有 14 个电影</span></span><br><span class="line">		<span class="comment"># 继续下一页</span></span><br><span class="line">		<span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="comment"># 没有下一页</span></span><br><span class="line">		<span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始的 ID 为 0，每页增加 15</span></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> start&lt;<span class="number">10000</span>: <span class="comment"># 最多抽取 1 万部电影</span></span><br><span class="line">	request_url = base_url + str(start)</span><br><span class="line">	<span class="comment"># 下载数据，并返回是否有下一页</span></span><br><span class="line">	flag = download(request_url)</span><br><span class="line">	<span class="keyword">if</span> flag:</span><br><span class="line">		start = start + <span class="number">15</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">out.close()</span><br><span class="line">print(<span class="string">'finished'</span>)</span><br></pre></td></tr></table></figure>

<p>（此处爬虫未跑通，待学习FileNotFoundError: [Errno 2] No such file or directory: ‘MicrosoftWebDriver.exe’: ‘MicrosoftWebDriver.exe’）</p>
<p>代码中涉及到了几个模块，简单讲解：</p>
<ul>
<li><p>在引用包这一段，使用 csv 工具包读写 CSV 文件，用 efficient_apriori 完成 Apriori 算法，用 lxml 进行 XPath 解析，time 工具包可以让我们在模拟后有个适当停留，代码中设置为 1 秒钟，等 HTML 数据完全返回后再进行 HTML 内容的获取。使用 selenium 的 webdriver 来模拟浏览器的行为。</p>
</li>
<li><p>在读写文件这一块，需要事先告诉 python 的 open 函数，文件的编码是 utf-8-sig（对应代码：encoding=‘utf-8-sig’），这是因为会用到中文，为了避免编码混乱。</p>
</li>
<li><p>编写 download 函数，参数传入要采集的页面地址（request_url）。针对返回的 HTML，需要用到之前讲到的 Chrome 浏览器的 XPath Helper 工具，来获取电影名称以及演出人员的 XPath。用页面返回的数据个数来判断当前所处的页面序号。如果数据个数 &gt;15，也就是第一页，第一页的第一条数据是广告，需要忽略。如果数据个数 =15，代表是中间页，需要点击“下一页”，也就是翻页。如果数据个数 &lt;15，代表最后一页，没有下一页。</p>
</li>
<li><p>在程序主体部分，设置 start 代表抓取的 ID，从 0 开始最多抓取 1 万部电影的数据（一个导演不会超过 1 万部电影），每次翻页 start 自动增加 15，直到 flag=False 为止，也就是不存在下一页的情况。</p>
</li>
</ul>
<p>你可以模拟下抓取的流程，获得指定导演的数据，比如我上面抓取的宁浩的数据。这里需要注意的是，豆瓣的电影数据可能是不全的，但基本上够我们用。</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/159.png" alt="img"></p>
<p>有了数据之后，我们就可以用 Apriori 算法来挖掘频繁项集和关联规则，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> efficient_apriori <span class="keyword">import</span> apriori</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">director = <span class="string">u'宁浩'</span></span><br><span class="line">file_name = <span class="string">'./'</span>+director+<span class="string">'.csv'</span></span><br><span class="line">lists = csv.reader(open(file_name, <span class="string">'r'</span>, encoding=<span class="string">'utf-8-sig'</span>))</span><br><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">data = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历获得[[,,],[,,]]</span></span><br><span class="line"><span class="comment"># 遍历得到每一行</span></span><br><span class="line"><span class="keyword">for</span> names <span class="keyword">in</span> lists:</span><br><span class="line">     name_new = []</span><br><span class="line">     <span class="comment"># 遍历获得新的去除空格的每一行</span></span><br><span class="line">     <span class="keyword">for</span> name <span class="keyword">in</span> names:</span><br><span class="line">           <span class="comment"># 去掉演员数据中的空格</span></span><br><span class="line">           name_new.append(name.strip())</span><br><span class="line">     <span class="comment"># 新的行[,,]存入data，但不要第一项电影名</span></span><br><span class="line">     data.append(name_new[<span class="number">1</span>:])</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 挖掘频繁项集和关联规则</span></span><br><span class="line">itemsets, rules = apriori(data, min_support=<span class="number">0.5</span>,  min_confidence=<span class="number">1</span>)</span><br><span class="line">print(itemsets)</span><br><span class="line">print(rules)</span><br></pre></td></tr></table></figure>

<p>代码中使用的 apriori 方法和开头中用 Apriori 获取购物篮规律的方法类似，比如代码中都设定了最小支持度和最小置信系数，这样我们可以找到支持度大于 50%，置信系数为 1 的频繁项集和关联规则。</p>
<p>这是最后的运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;1: &#123;(&apos;徐峥&apos;,): 5, (&apos;黄渤&apos;,): 6&#125;, 2: &#123;(&apos;徐峥&apos;, &apos;黄渤&apos;): 5&#125;&#125;</span><br><span class="line">[&#123;徐峥&#125; -&gt; &#123;黄渤&#125;]</span><br></pre></td></tr></table></figure>

<p> 能看出来，宁浩导演喜欢用徐峥和黄渤，并且有徐峥的情况下，一般都会用黄渤。也可以用上面的代码来挖掘下其他导演选择演员的规律。</p>
<p><strong>总结：</strong><br>实战最主要的是爬取数据，尤其是环境的搭建和最后编码部分。</p>
<p>算法有现成的包efficient-apriori 可以使用</p>
<p> Apriori 算法中的最小支持度和最小置信度，一般设置为多少比较合理？</p>
<p>首先，这两个值与数据集大小特征相关。一般来说最小支持度常见的取值有0.5，0.1, 0.05。最小置信度常见的取值有1.0, 0.9, 0.8。可以通过尝试一些取值，然后观察关联结果的方式来调整最小值尺度和最小置信度的取值。</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/161.png" alt="img"></p>
<h2 id="23-PageRank算法-重要性评估"><a href="#23-PageRank算法-重要性评估" class="headerlink" title="23.PageRank算法(重要性评估)"></a>23.PageRank算法(重要性评估)</h2><p>大家一般都有从众心理：</p>
<p>比如，当你某一家饭店，或者理发店，或者小吃店里面的顾客非常多，每天爆满的时候，心理一定会想，这家店想必不错，要不然不可能这么多人，我也进去试试。 或者，在淘宝上买某个商品的时候，肯定是喜欢挑人多的店铺，好评量高的店铺买的放心等等吧。 所以当我们在生活中遇到艰难选择的时候，往往喜欢看看别人是怎么做的，一般都会选大部分人的选择。 这其实就是一种从众。</p>
<p>这些店铺也好，选择也罢，其实都是通过很多人的投票进而提高了自己的影响力，再比如说，微博上如何去衡量一个人的影响力呢？ 我们习惯看他的粉丝，如果他的粉丝多，并且里面都是一些大V，明星的话，很可能这个人的影响力会比较强。 再比如说，职场上如何衡量一个人的影响力呢？ 我们习惯看与他交往的人物， 如果和他交往的都是像马云，王健林，马化腾这样的人物，那么这个人的影响力估计也小不了。 再比如说，如何判断一篇论文好？ 我们习惯看他的引用次数，或者影响因子，高的论文就比较好等等。</p>
<p>但是你只知道吗？ 其实我们的这种方式就在用PageRank算法的思想了，只不过我们没有发觉罢了，所谓的算法来源于生活，并服务于生活就是这个道理。</p>
<h3 id="PageRank原理"><a href="#PageRank原理" class="headerlink" title="PageRank原理"></a>PageRank原理</h3><p><strong>PageRank来源：</strong><br>想必大家上网的时候，都用过搜索引擎，现在已经非常好用了，基本上输入关键词，都能找到匹配的内容，质量还不错。但在 1998 年之前，搜索引擎的体验并不好。早期的搜索引擎，会遇到下面的两类问题：</p>
<p>返回结果质量不高：搜索结果不考虑网页的质量，而是通过时间顺序进行检索；<br>容易被人钻空子：搜索引擎是基于检索词进行检索的，页面中检索词出现的频次越高，匹配度越高，这样就会出现网页作弊的情况。有些网页为了增加搜索引擎的排名，故意增加某个检索词的频率。 </p>
<p>基于这些缺陷，当时 Google 的创始人拉里·佩奇提出了 PageRank 算法，目的就是要找到优质的网页，这样 Google 的排序结果不仅能找到用户想要的内容，而且还会从众多网页中筛选出权重高的呈现给用户。其灵感就是论文影响力因子的启发。</p>
<p><strong>PageRank的简化模型：</strong></p>
<p>假设一共有 4 个网页 A、B、C、D。它们之间的链接信息如图所示：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/162.png" style="zoom: 33%;">

<p>首先先知道两个概念：</p>
<p>出链指的是链接出去的链接。入链指的是链接进来的链接。比如图中 A 有 2 个入链，3 个出链。</p>
<p>那么我们如何计算一个网页的影响力或者重要程度呢？</p>
<p>简单来说，一个网页的影响力 = 所有入链集合的页面的加权影响力之和，用公式表示为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/163.png" style="zoom: 50%;">

<p>u 为待评估的页面，Bu 为页面 u 的入链集合。针对入链集合中的任意页面 v，它能给 u 带来的影响力是其自身的影响力 PR(v) 除以 v 页面的出链数量，即页面 v 把影响力 PR(v) 平均分配给了它的出链，这样统计所有能给 u 带来链接的页面 v，得到的总和就是网页 u 的影响力，即为 PR(u)。</p>
<p>所以你能看到，出链会给被链接的页面赋予影响力，当我们统计了一个网页链出去的数量，也就是统计了这个网页的跳转概率。</p>
<p>在这个例子中，你能看到 A 有三个出链分别链接到了 B、C、D 上。那么当用户访问 A 的时候，就有跳转到 B、C 或者 D 的可能性，跳转概率均为 1/3。</p>
<p>B 有两个出链，链接到了 A 和 D 上，跳转概率为 1/2。</p>
<p>这样，我们可以得到 A、B、C、D 这四个网页的转移矩阵 M：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/164.png" style="zoom: 67%;">

<p>这个转移矩阵，每一行代表了每个节点入链上的权重（大家浏览别的页面的时候，有多大的概率能跳到我这来）。 每一列代表了每个节点对其他页面的影响力的赋予程度（也就是大家浏览我这，有多大的概率跳到别的页面上去）。</p>
<p>我们假设 A、B、C、D 四个页面的初始影响力都是相同的，即：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/165.png" style="zoom: 67%;">

<p>当进行第一次转移之后，各页面的影响力 w1 变为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/166.png" style="zoom: 67%;">

<p>然后我们再用转移矩阵乘以 w1 得到 w2 结果，直到第 n 次迭代后 wn 影响力不再发生变化，可以收敛到 (0.3333，0.2222，0.2222，0.2222），也就是对应着 A、B、C、D 四个页面最终平衡状态下的影响力。</p>
<p>能看出 A 页面相比于其他页面来说权重更大，也就是 PR 值更高。而 B、C、D 页面的 PR 值相等。</p>
<p>至此，我们模拟了一个简化的 PageRank 的计算过程，也就是PageRank简化模型的原理了。实际情况更复杂，可能有两个问题：</p>
<ul>
<li><strong>等级泄露</strong>（Rank Leak）：如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的 PR 值为 0。</li>
</ul>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/167.png" style="zoom: 33%;">

<ul>
<li><strong>等级沉没</strong>（Rank Sink）：如果一个网页只有出链，没有入链（如下图所示），计算的过程迭代下来，会导致这个网页的 PR 值为 0（也就是不存在公式中的 V）。</li>
</ul>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/168.png" style="zoom: 33%;">

<p>那么如何有效解决呢？</p>
<p><strong>PageRank 的随机浏览模型</strong></p>
<p>为了解决简化模型中存在的等级泄露和等级沉没的问题，拉里·佩奇提出了 PageRank 的随机浏览模型。他假设了这样一个场景：用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面，比如说用户就是要直接输入网址访问其他页面，虽然这个概率比较小。</p>
<p>所以他定义了阻尼因子 d，这个因子代表了用户按照跳转链接来上网的概率，通常可以取一个固定值 0.85，而 1-d=0.15 则代表了用户不是通过跳转链接的方式来访问网页的，比如直接输入网址。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/169.png" style="zoom: 50%;">

<p>其中 N 为网页总数，这样我们又可以重新迭代网页的权重计算了，因为加入了阻尼因子 d，一定程度上解决了等级泄露和等级沉没的问题。</p>
<p>通过数学定理（这里不进行讲解）也可以证明，最终 PageRank 随机浏览模型是可以收敛的，也就是可以得到一个稳定正常的 PR 值。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/170.png" style="zoom: 67%;">

<h3 id="PageRank实战"><a href="#PageRank实战" class="headerlink" title="PageRank实战"></a>PageRank实战</h3><p>使用工具实现PageRank算法</p>
<p>PageRank 算法工具在 sklearn 中并不存在，我们需要找到新的工具包。实际上有一个关于图论和网络建模的工具叫 NetworkX，它是用 Python 语言开发的工具，内置了常用的图与网络分析算法，可以方便我们进行网络数据分析。</p>
<p>上面举了一个网页权重的例子，假设一共有 4 个网页 A、B、C、D，它们之间的链接信息如图所示：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/162.png" style="zoom: 33%;">

<p>针对这个例子，我们看下用 NetworkX 如何计算 A、B、C、D 四个网页的 PR 值，具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建有向图</span></span><br><span class="line">G = nx.DiGraph() </span><br><span class="line"></span><br><span class="line"><span class="comment"># 有向图之间边的关系</span></span><br><span class="line">edges = [(<span class="string">"A"</span>, <span class="string">"B"</span>), (<span class="string">"A"</span>, <span class="string">"C"</span>), (<span class="string">"A"</span>, <span class="string">"D"</span>), (<span class="string">"B"</span>, <span class="string">"A"</span>), (<span class="string">"B"</span>, <span class="string">"D"</span>), (<span class="string">"C"</span>, <span class="string">"A"</span>), (<span class="string">"D"</span>, <span class="string">"B"</span>), (<span class="string">"D"</span>, <span class="string">"C"</span>)]</span><br><span class="line"><span class="keyword">for</span> edge <span class="keyword">in</span> edges:</span><br><span class="line">    G.add_edge(edge[<span class="number">0</span>], edge[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">pagerank_list = nx.pagerank(G, alpha=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"pagerank值是：\n"</span>, pagerank_list)</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pagerank值是：</span><br><span class="line"> &#123;&apos;A&apos;: 0.33333396911621094, &apos;B&apos;: 0.22222201029459634, &apos;C&apos;: 0.22222201029459634, &apos;D&apos;: 0.22222201029459634&#125;</span><br></pre></td></tr></table></figure>

<p>关键代码就nx.pagerank(G, alpha=1) 这一句话， 这里的alpha就是我们上面说的阻尼因子，代表用户按照跳转链接的概率。默认是0.85。这里是1，表示我们都是用跳转链接，不直接输入网址的那种。</p>
<p>好了，运行完这个例子之后，来看下 NetworkX 工具都有哪些常用的操作。</p>
<ol>
<li><p>关于图的创建</p>
<p>图可以分为无向图和有向图，在 NetworkX 中分别采用不同的函数进行创建。无向图指的是不用节点之间的边的方向，使用 <strong>nx.Graph()</strong> 进行创建；有向图指的是节点之间的边是有方向的，使用 <strong>nx.DiGraph</strong>() 来创建。在上面这个例子中，存在 A→D 的边，但不存在 D→A 的边。</p>
</li>
<li><p>关于节点的增加、删除和查询</p>
<p>如果想在网络中增加节点，可以使用 <strong>G.add_node</strong>(‘A’) 添加一个节点，也可以使用 <strong>G.add_nodes_from</strong>([‘B’,‘C’,‘D’,‘E’]) 添加节点集合。</p>
<p>如果想要删除节点，可以使用 <strong>G.remove_node</strong>(node) 删除一个指定的节点，也可以使用 <strong>G.remove_nodes_from</strong>([‘B’,‘C’,‘D’,‘E’]) 删除集合中的节点。</p>
<p>查询节点：如果你想要得到图中所有的节点，就可以使用 <strong>G.nodes</strong>()，也可以用 <strong>G.number_of_nodes</strong>() 得到图中节点的个数。</p>
</li>
<li><p>关于边的增加、删除、查询</p>
<p>增加边与添加节点的方式相同，使用 <strong>G.add_edge</strong>(“A”, “B”) 添加指定的“从 A 到 B”的边，也可以使用 <strong>add_edges_from</strong> 函数从边集合中添加。我们也可以做一个加权图，也就是说边是带有权重的，使用<strong>add_weighted_edges_from</strong> 函数从带有权重的边的集合中添加。在这个函数的参数中接收的是 1 个或多个三元组[u,v,w]作为参数，u、v、w 分别代表起点、终点和权重。</p>
<p>另外，我们可以使用 <strong>remove_edge</strong> 函数和 <strong>remove_edges_from</strong> 函数删除指定边和从边集合中删除。</p>
<p>另外可以使用 <strong>edges</strong>() 函数访问图中所有的边，使用 <strong>number_of_edges</strong>() 函数得到图中边的个数。</p>
</li>
</ol>
<p>以上是关于图的基本操作，如果我们创建了一个图，并且对节点和边进行了设置，就可以找到其中有影响力的节点，原理就是通过 PageRank 算法，使用 <strong>nx.pagerank</strong>(G) 这个函数，函数中的参数 G 代表创建好的图。</p>
<h3 id="用-PageRank-揭秘希拉里邮件中的人物关系"><a href="#用-PageRank-揭秘希拉里邮件中的人物关系" class="headerlink" title="用 PageRank 揭秘希拉里邮件中的人物关系"></a>用 PageRank 揭秘希拉里邮件中的人物关系</h3><p>数据集下载：<a href="https://github.com/cystanford/PageRank" target="_blank" rel="noopener">https://github.com/cystanford/PageRank</a></p>
<p>先了解下数据集：</p>
<p>整个数据集由三个文件组成：Aliases.csv，Emails.csv 和 Persons.csv，其中 Emails 文件记录了所有公开邮件的内容，发送者和接收者的信息。Persons 这个文件统计了邮件中所有人物的姓名及对应的 ID。因为姓名存在别名的情况，为了将邮件中的人物进行统一，我们还需要用 Aliases 文件来查询别名和人物的对应关系。</p>
<p>整个数据集包括了 9306 封邮件和 513 个人名，数据集还是比较大的。不过这一次我们不需要对邮件的内容进行分析，只需要通过邮件中的发送者和接收者（对应 Emails.csv 文件中的 MetadataFrom 和 MetadataTo 字段）来绘制整个关系网络。因为涉及到的人物很多，因此我们需要通过 PageRank 算法计算每个人物在邮件关系网络中的权重，最后筛选出来最有价值的人物来进行关系网络图的绘制。</p>
<p>了解了数据集和项目背景之后，我们来设计到执行的流程步骤：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/172.png" style="zoom: 67%;">

<ol>
<li>首先我们需要加载数据源；</li>
<li>在准备阶段：我们需要对数据进行探索，在数据清洗过程中，因为邮件中存在别名的情况，因此我们需要统一人物名称。另外邮件的正文并不在我们考虑的范围内，只统计邮件中的发送者和接收者，因此我们筛选 MetadataFrom 和 MetadataTo 这两个字段作为特征。同时，发送者和接收者可能存在多次邮件往来，需要设置权重来统计两人邮件往来的次数。次数越多代表这个边（从发送者到接收者的边）的权重越高；</li>
<li>在挖掘阶段：我们主要是对已经设置好的网络图进行 PR 值的计算，但邮件中的人物有 500 多人，有些人的权重可能不高，我们需要筛选 PR 值高的人物，绘制出他们之间的往来关系。在可视化的过程中，我们可以通过节点的 PR 值来绘制节点的大小，PR 值越大，节点的绘制尺寸越大。</li>
</ol>
<p>最终代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># 用 PageRank 挖掘希拉里邮件中的重要任务关系</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line">emails = pd.read_csv(<span class="string">"./input/Emails.csv"</span>)</span><br><span class="line"><span class="comment"># 读取别名文件</span></span><br><span class="line"><span class="comment"># 得到别名人物字典aliases，别名：人物id</span></span><br><span class="line">file = pd.read_csv(<span class="string">"./input/Aliases.csv"</span>)</span><br><span class="line">aliases = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> file.iterrows():</span><br><span class="line">    aliases[row[<span class="string">'Alias'</span>]] = row[<span class="string">'PersonId'</span>]</span><br><span class="line"><span class="comment"># 读取人名文件</span></span><br><span class="line">file = pd.read_csv(<span class="string">"./input/Persons.csv"</span>)</span><br><span class="line">persons = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> file.iterrows():</span><br><span class="line">    persons[row[<span class="string">'Id'</span>]] = row[<span class="string">'Name'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 针对别名进行转换        </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unify_name</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="comment"># 姓名统一小写</span></span><br><span class="line">    name = str(name).lower()</span><br><span class="line">    <span class="comment"># 去掉, 和 @后面的内容</span></span><br><span class="line">    name = name.replace(<span class="string">","</span>,<span class="string">""</span>).split(<span class="string">"@"</span>)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 别名转换</span></span><br><span class="line">    <span class="keyword">if</span> name <span class="keyword">in</span> aliases.keys():</span><br><span class="line">        <span class="keyword">return</span> persons[aliases[name]]</span><br><span class="line">    <span class="keyword">return</span> name</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 画网络图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_graph</span><span class="params">(graph, layout=<span class="string">'spring_layout'</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 使用 Spring Layout 布局，类似中心放射状</span></span><br><span class="line">    <span class="keyword">if</span> layout == <span class="string">'circular_layout'</span>:</span><br><span class="line">        positions=nx.circular_layout(graph)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        positions=nx.spring_layout(graph)</span><br><span class="line">    <span class="comment"># 设置网络图中的节点大小，大小与 pagerank 值相关，因为 pagerank 值很小所以需要 *20000</span></span><br><span class="line">    nodesize = [x[<span class="string">'pagerank'</span>]*<span class="number">20000</span> <span class="keyword">for</span> v,x <span class="keyword">in</span> graph.nodes(data=<span class="literal">True</span>)]</span><br><span class="line">    <span class="comment"># 设置网络图中的边长度</span></span><br><span class="line">    edgesize = [np.sqrt(e[<span class="number">2</span>][<span class="string">'weight'</span>]) <span class="keyword">for</span> e <span class="keyword">in</span> graph.edges(data=<span class="literal">True</span>)]</span><br><span class="line">    <span class="comment"># 绘制节点</span></span><br><span class="line">    nx.draw_networkx_nodes(graph, positions, node_size=nodesize, alpha=<span class="number">0.4</span>)</span><br><span class="line">    <span class="comment"># 绘制边</span></span><br><span class="line">    nx.draw_networkx_edges(graph, positions, edge_size=edgesize, alpha=<span class="number">0.2</span>)</span><br><span class="line">    <span class="comment"># 绘制节点的 label</span></span><br><span class="line">    nx.draw_networkx_labels(graph, positions, font_size=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># 输出希拉里邮件中的所有人物关系图</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将寄件人和收件人的姓名进行规范化</span></span><br><span class="line">emails.MetadataFrom = emails.MetadataFrom.apply(unify_name)</span><br><span class="line">emails.MetadataTo = emails.MetadataTo.apply(unify_name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置边的权重等于发邮件的次数</span></span><br><span class="line"><span class="comment"># defaultdict(list)构建一个默认值为[]的字典</span></span><br><span class="line">edges_weights_temp = defaultdict(list)</span><br><span class="line"><span class="comment"># zip获得以元组为元素的列表[(F,T,R),(F,T,R),...]</span></span><br><span class="line"><span class="comment"># row为一个(F,T,R)，row[0]为F，row[1]为T</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> zip(emails.MetadataFrom, emails.MetadataTo, emails.RawText):</span><br><span class="line">    temp = (row[<span class="number">0</span>], row[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> temp <span class="keyword">not</span> <span class="keyword">in</span> edges_weights_temp:</span><br><span class="line">        edges_weights_temp[temp] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        edges_weights_temp[temp] = edges_weights_temp[temp] + <span class="number">1</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># 转化格式 (from, to), weight =&gt; from, to, weight</span></span><br><span class="line">edges_weights = [(key[<span class="number">0</span>], key[<span class="number">1</span>], val) <span class="keyword">for</span> key, val <span class="keyword">in</span> edges_weights_temp.items()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个有向图</span></span><br><span class="line">graph = nx.DiGraph()</span><br><span class="line"><span class="comment"># 使用函数设置有向图中的路径及权重 (from, to, weight)</span></span><br><span class="line">graph.add_weighted_edges_from(edges_weights)</span><br><span class="line"><span class="comment"># 计算每个节点（人）的 PR 值，并作为节点的 pagerank 属性</span></span><br><span class="line">pagerank = nx.pagerank(graph)</span><br><span class="line"><span class="comment"># 将 pagerank 数值作为节点的属性</span></span><br><span class="line">nx.set_node_attributes(graph, name = <span class="string">'pagerank'</span>, values=pagerank)</span><br><span class="line"><span class="comment"># 画网络图</span></span><br><span class="line">show_graph(graph)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将完整的图谱进行精简</span></span><br><span class="line"><span class="comment"># 设置 PR 值的阈值，筛选大于阈值的重要核心节点</span></span><br><span class="line">pagerank_threshold = <span class="number">0.005</span></span><br><span class="line"><span class="comment"># 复制一份计算好的网络图</span></span><br><span class="line">small_graph = graph.copy()</span><br><span class="line"><span class="comment"># 剪掉 PR 值小于 pagerank_threshold 的节点</span></span><br><span class="line"><span class="keyword">for</span> n, p_rank <span class="keyword">in</span> graph.nodes(data=<span class="literal">True</span>):</span><br><span class="line">    <span class="keyword">if</span> p_rank[<span class="string">'pagerank'</span>] &lt; pagerank_threshold: </span><br><span class="line">        small_graph.remove_node(n)</span><br><span class="line"><span class="comment"># 画网络图,采用circular_layout布局让筛选出来的点组成一个圆</span></span><br><span class="line">show_graph(small_graph, <span class="string">'circular_layout'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画网络图是为了展示，直接输出pagerank字典，不把graph画出来也可：</span></span><br><span class="line">pagerank_small = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> key, val <span class="keyword">in</span> pagerank.items():</span><br><span class="line">    <span class="keyword">if</span> val &gt; <span class="number">0.005</span>:</span><br><span class="line">        pagerank_small[key] = val</span><br><span class="line">print(pagerank)</span><br><span class="line">print(pagerank_small)</span><br></pre></td></tr></table></figure>

<p>整体框架看一下前面实战的小代码</p>
<p>运行结果：</p>
<p>简化前：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/173.png" style="zoom: 67%;">

<p>简化后：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/174.png" style="zoom: 67%;">


<p>针对代码中的几个模块个简单的说明：</p>
<ul>
<li><p>函数定义</p>
<p>人物的名称需要统一，因此设置了 unify_name 函数，同时设置了 show_graph 函数将网络图可视化。NetworkX 提供了多种可视化布局，这里使用 spring_layout 布局，也就是呈中心放射状。</p>
<p>除了 spring_layout 外，NetworkX 还有另外三种可视化布局，circular_layout（在一个圆环上均匀分布节点），random_layout（随机分布节点 ），shell_layout（节点都在同心圆上）。</p>
</li>
<li><p>计算边权重</p>
<p>邮件的发送者和接收者的邮件往来可能不止一次，我们需要用两者之间邮件往来的次数计算这两者之间边的权重，所以用 edges_weights_temp 数组存储权重。而上面介绍过在 NetworkX 中添加权重边（即使用 add_weighted_edges_from 函数）的时候，接受的是 u、v、w 的三元数组，因此我们还需要对格式进行转换，具体转换方式见代码。</p>
</li>
<li><p>PR 值计算及筛选</p>
<p>使用 nx.pagerank(graph) 计算了节点的 PR 值。由于节点数量很多，我们设置了 PR 值阈值，即 pagerank_threshold=0.005，然后遍历节点，删除小于 PR 值阈值的节点，形成新的图 small_graph，最后对 small_graph 进行可视化（对应运行结果的第二张图）。</p>
</li>
</ul>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/171.png" style="zoom: 67%;">

<h2 id="24-AdaBoost"><a href="#24-AdaBoost" class="headerlink" title="24.AdaBoost"></a>24.AdaBoost</h2><p>集成的含义就是集思广益，博取众长，当我们做决定的时候，我们先听取多个专家的意见，再做决定。集成算法通常用两种： 投票选举（bagging）和再学习（boosting）。</p>
<p><strong>投票选举</strong>的场景类似把专家召集到一个会议桌前，当做一个决定的时候，让 K 个专家（K 个模型）分别进行分类（做出决定），然后选择出现次数最多的那个类（决定）作为最终的分类结果。（听说过伟大的随机森林吧，就是训练很多棵树，少数服从多数）</p>
<p><strong>再学习</strong>相当于把 K 个专家（K 个分类器）进行加权融合，形成一个新的超级专家（强分类器），让这个超级专家做判断。（而伟大的AdaBoost就是这种方式）</p>
<p>Boosting 的含义是提升，它的作用是每一次训练的时候都对上一次的训练进行改进提升，在训练的过程中这 K 个“专家”之间是有依赖性的，当引入第 K 个“专家”（第 K 个分类器）的时候，实际上是对前 K-1 个专家的优化。<br>而 bagging 在做投票选举的时候可以并行计算，也就是 K 个“专家”在做判断的时候是相互独立的，不存在依赖性。</p>
<h3 id="AdaBoost的工作原理"><a href="#AdaBoost的工作原理" class="headerlink" title="AdaBoost的工作原理"></a>AdaBoost的工作原理</h3><p>AdaBoost算法是一种<strong>再学习</strong>的一种方式，英文全称是 Adaptive Boosting，中文含义是<strong>自适应提升算法</strong>。它由 Freund 等人于 1995 年提出，是对 Boosting 算法的一种实现。</p>
<p>什么是 Boosting 算法呢？Boosting 算法是集成算法中的一种，同时也是一类算法的总称。这类算法通过训练多个弱分类器，将它们组合成一个强分类器，也就是我们俗话说的“三个臭皮匠，顶个诸葛亮”。为什么要这么做呢？因为臭皮匠好训练，诸葛亮却不好求。因此要打造一个诸葛亮，最好的方式就是训练多个臭皮匠，然后让这些臭皮匠组合起来，这样往往可以得到很好的效果。这就是 Boosting 算法的原理。</p>
<p><img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/175.png" alt></p>
<p>可以用多个弱分类器来组合一个强分类器，这些弱分类器是根据不同的权重组合而成的。</p>
<p>假设弱分类器为 Gi(x)，它在强分类器中的权重 αi，那么就可以得出强分类器 f(x)：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/176.png" style="zoom: 67%;">

<p>这里有两个问题：</p>
<ul>
<li><p>如何得到这些弱分类器，也就是在每次迭代训练的过程中，如何得到最优的弱分类器？</p>
</li>
<li><p>每个弱分类器的权重是如何计算的？<br>我们先来看一下第二个问题，如何计算权重？ 那第一感觉肯定是谁表现好，权重就越高啊。哈哈，还真是这样</p>
</li>
</ul>
<p>实际上在一个由 K 个弱分类器中组成的强分类器中，如果弱分类器的分类效果好，那么权重应该比较大，如果弱分类器的分类效果一般，权重应该降低。所以我们需要基于这个弱分类器对样本的分类错误率来决定它的权重，用公式表示就是：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/177.png" style="zoom: 67%;">

<p>其中 ei 代表第 i 个分类器的分类错误率。 </p>
<p>然后我们再来看下第一个问题，如何在每次训练迭代的过程中选择最优的弱分类器？</p>
<p>Adaboost是通过改变样本的数据分布来实现的，AdaBoost 会判断每次训练的样本是否正确分类，对于正确分类的样本，降低它的权重，对于被错误分类的样本，增加它的权重。再基于上一次得到的分类准确率，来确定这次训练样本中每个样本的权重。</p>
<p>然后将修改过权重的新数据集传递给下一层的分类器进行训练。这样做的好处就是，通过每一轮训练样本的动态权重，可以让训练的焦点集中到难分类的样本上，最终得到的弱分类器的组合更容易得到更高的分类准确率。</p>
<p>过程理解就是这样， 我的训练样本在开始的时候啊，都会有一个概率分布，也就是权重。比如n个样本，我假设每个样本的权重都是1/n，意味着同等重要， 但是我们训练出一个分类器A之后，如果这个分类器A能把之前的样本正确的分类，就说明这些正确分类的样本由A来搞定就可以了。 我们下一轮训练分类器B的时候就不需要太多的关注了，让B更多的去关注A分类错误的样本？ 那怎么做到这一点呢？ 那就把A分类正确的样本的权重减小，分类错误的样本的权重增大。这样，B在训练的时候，就能更加的关注这些错误样本了，因为一旦把这些样本分类错误，损失就会快速涨（权重大呀），为了使损失降低，B就尽可能的分类出这些A没有分出的样本，问题解决。那如果训练出来的B已经很好了，误差很小了，仍然有分不出来的怎么办？ 那同样的道理，把这些的权重增大，交给下一轮的C。 每一轮的分类器各有专长的。</p>
<p>怎么计算着每个样本的权重吧：</p>
<p>我们可以用 Dk+1 代表第 k+1 轮训练中，样本的权重集合，其中 Wk+1,1 代表第 k+1 轮中第一个样本的权重，以此类推 Wk+1,N 代表第 k+1 轮中第 N 个样本的权重，因此用公式表示为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/178.png" style="zoom: 67%;">

<p>第 k+1 轮中的样本权重，是根据该样本在第 k 轮的权重以及第 k 个分类器的准确率而定，具体的公式为：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/179.png" style="zoom: 67%;">

<p>这个公式保证的就是，如果当前分类器把样本分类错误了，那么样本的w就会变大，如果分类正确了，w就会减小。 这里的Zk是归一化系数。就是∑ (wk,i exp(-αkyiGk(xi))</p>
<h3 id="AdaBoost算法示例"><a href="#AdaBoost算法示例" class="headerlink" title="AdaBoost算法示例"></a>AdaBoost算法示例</h3><p>回忆一下，AdaBoost里面的两个问题：</p>
<ul>
<li><p>如何得到这些弱分类器，也就是在每次迭代训练的过程中，如何得到最优的弱分类器？ — 改变样本的权重或者叫数据分布</p>
</li>
<li><p>每个弱分类器（士兵）的权重是如何计算的？ — 通过误差率和那个公式</p>
</li>
</ul>
<p>看下面的例子，假设有10个训练样本：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/180.png">

<p>想通过AdaBoost构建一个强分类器，怎么做呢？ 模拟一下：</p>
<p>首先，我得先给这10个样本划分重要程度，也就是权重，由于是一开始，那就平等，都是1/10。即初始权重D1=(0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)。</p>
<p>假设我训练的3个基础分类器如下：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/181.png" style="zoom: 67%;">

<p>当然，这个是一次迭代训练一个，这里为了解释这个过程，先有这三个。</p>
<p>然后，我们进行第一轮的训练, 我们可以知道：</p>
<ul>
<li><p>分类器 f1 的错误率为 0.3，也就是 x 取值 6、7、8 时分类错误；</p>
</li>
<li><p>分类器 f2 的错误率为 0.4，即 x 取值 0、1、2、9 时分类错误；</p>
</li>
<li><p>分类器 f3 的错误率为 0.3，即 x 取值为 3、4、5 时分类错误。</p>
</li>
</ul>
<p>根据误差率最小，选择f1或者f3，我训练出一个分类器来如下(选择f1)：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/182.png" style="zoom: 67%;">

<p>这个分类器的错误率是0.3（x取值6, 7，8的时候分类错误），是误差率最低的了（怎么训练的？ 可以用一个决策树训练就可以啊）， 即e1 = 0.3</p>
<p>那么根据权重公式得到第一个弱分类器的权重：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/183.png" style="zoom: 67%;">


<p>然后，我们就得根据这个分类器，来更新我们的训练样本的权重了</p>
<p>根据这个公式，就可以计算权重矩阵为：D2=(0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.0715, 0.1666, 0.1666, 0.1666, 0.0715)。</p>
<p>你会发现，6, 7, 8样本的权重变大了，其他的权重变小（这就意味着，下一个分类器训练的时候，重点关注6, 7, 8这三个样本，）</p>
<p>接着我们进行第二轮的训练，继续统计三个分类器的准确率，可以得到：</p>
<ul>
<li>分类器 f1 的错误率为 0.1666 * 3，也就是 x 取值为 6、7、8 时分类错误。</li>
<li>分类器 f2 的错误率为 0.0715 * 4，即 x 取值为 0、1、2、9 时分类错误。</li>
<li>分类器 f3 的错误率为 0.0715 * 3，即 x 取值 3、4、5时分类错误。</li>
</ul>
<p>在这 3 个分类器中，f3 分类器的错误率最低，因此我们选择 f3 作为第二轮训练的最优分类器，即：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/184.png" style="zoom: 67%;">

<p>根据分类器权重公式得到：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/185.png" style="zoom: 67%;">


<p>同样，我们对下一轮的样本更新求权重值</p>
<p>可以得到 D3=(0.0455,0.0455,0.0455,0.1667, 0.1667,0.01667,0.1060, 0.1060, 0.1060, 0.0455)。</p>
<p>你会发现， G2分类错误的3，4， 5这三个样本的权重变大了，说明下一轮的分类器重点在上三个样本上面。</p>
<p>接下来我们开始第三轮的训练, 我们继续统计三个分类器的准确率，可以得到：分类器 f1 的错误率为 0.1060 * 3，也就是 x 取值 6、7、8 时分类错误；分类器 f2 的错误率为 0.0455 * 4，即 x 取值为 0、1、2、9 时分类错误；分类器 f3 的错误率为 0.1667 * 3，即 x 取值 3、4、5 时分类错误。</p>
<p>在这 3 个分类器中，f2 分类器的错误率最低，因此我们选择 f2 作为第三轮训练的最优分类器，即：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/186.png" style="zoom: 67%;">

<p>我们根据分类器权重公式得到：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/187.png" style="zoom: 67%;">

<p>假设我们只进行 3 轮的训练，选择 3 个弱分类器，组合成一个强分类器，那么最终的强分类器</p>
<p>G(x) = 0.4236G1(x) + 0.6496G2(x)+0.7514G3(x)。</p>
<p>简单梳理就是：</p>
<ul>
<li>确定初始样本的权重，然后训练分类器，根据误差最小，选择分类器，得到误差率，计算该分类器的权重<br>然后根据该分类器的误差去重新计算样本的权重</li>
<li>进行下一轮训练，若不停止，就重复上述过程。</li>
</ul>
<p>实际上AdaBoost算法是一个框架，你可以指定任意的分类器，通常我们可以采用<strong>CART分类器</strong>作为弱分类器。</p>
<p>AdaBoost算法的原理，可以把它理解为一种集成算法，通过训练不同的弱分类器，将这些弱分类器集成起来形成一个强分类器。在每一轮的训练中都会加入一个新的弱分类器，直到达到足够低的错误率或者达到指定的最大迭代次数为止。实际上每一次迭代都会引入一个新的弱分类器（这个分类器是每一次迭代中计算出来的，是新的分类器，不是事先准备好的）。</p>
<p>在弱分类器的集合中，你不必担心弱分类器太弱了。实际上它只需要比随机猜测的效果略好一些即可。如果随机猜测的准确率是50%的话，那么每个弱分类器的准确率只要大于50%就可用。AdaBoost的强大在于迭代训练的机制，这样通过K个“臭皮匠”的组合也可以得到一个“诸葛亮” （强分类器）。</p>
<p>当然在每一轮的训练中，我们都需要从众多“臭皮匠”中选择一个拔尖的，也就是这一轮训练评比中的最优“臭皮匠”，对应的就是错误率最低的分类器。当然每一轮的样本的权重都会发生变化，这样做的目的是为了让之前错误分类的样本得到更多概率的重复训练机会。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/188.png">

<h3 id="AdaBoost工具使用"><a href="#AdaBoost工具使用" class="headerlink" title="AdaBoost工具使用"></a>AdaBoost工具使用</h3><p>AdaBoost不仅可以用于分类问题，还可以用于回归问题。这个例子是一个回归问题。</p>
<p><strong>如何使用AdaBoost工具</strong></p>
<p>我们可以直接在 sklearn 中使用 AdaBoost。如果我们要用 AdaBoost 进行分类，需要在使用前引用代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br></pre></td></tr></table></figure>

<p>如果你看到了 Classifier 这个类，一般都会对应着 Regressor 类。AdaBoost 也不例外，回归工具包的引用代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostRegressor</span><br></pre></td></tr></table></figure>

<p>如何在sklearn中<strong>创建AdaBoost分类器</strong>：</p>
<p>分类的时候，需要这样的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AdaBoostClassifier(base_estimator=<span class="literal">None</span>, n_estimators=<span class="number">50</span>, learning_rate=<span class="number">1.0</span>, algorithm=’SAMME.R’, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>下面看看这些参数的含义：</p>
<ul>
<li>base_estimator：代表的是<strong>弱分类器</strong>。在 AdaBoost 的分类器和回归器中都有这个参数，在 AdaBoost 中默认使用的是<strong>决策树</strong>，一般我们不需要修改这个参数，当然你也可以指定具体的分类器。</li>
<li>n_estimators：算法的<strong>最大迭代次数</strong>，也是分类器的个数，每一次迭代都会引入一个新的弱分类器来增加原有的分类器的组合能力。默认是 50。</li>
<li>learning_rate：代表<strong>学习率</strong>，取值在 0-1 之间，默认是 1.0。如果学习率较小，就需要比较多的迭代次数才能收敛，也就是说学习率和迭代次数是有相关性的。当你调整 learning_rate 的时候，往往也需要调整 n_estimators 这个参数。</li>
<li>algorithm：代表我们要采用<strong>哪种 boosting 算法</strong>，一共有两种选择：SAMME 和 SAMME.R。默认是 SAMME.R。这两者之间的区别在于对弱分类权重的计算方式不同。</li>
<li>random_state：代表<strong>随机数种子</strong>的设置，默认是 None。随机种子是用来控制随机模式的，当随机种子取了一个值，也就确定了一种随机规则，其他人取这个值可以得到同样的结果。如果不设置随机种子，每次得到的随机数也就不同。</li>
</ul>
<p>如何<strong>创建AdaBoost回归</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AdaBoostRegressor(base_estimator=<span class="literal">None</span>, n_estimators=<span class="number">50</span>, learning_rate=<span class="number">1.0</span>, loss=‘linear’, random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>回归和分类的参数基本是一致的，不同点在于回归算法里没有 algorithm 这个参数，但多了一个 loss 参数。</p>
<ul>
<li>loss 代表<strong>损失函数</strong>的设置，一共有 3 种选择，分别为 linear、square 和 exponential，它们的含义分别是线性、平方和指数。默认是线性。一般采用线性就可以得到不错的效果。</li>
</ul>
<p>创建好 AdaBoost 分类器或回归器之后，我们就可以输入训练集对它进行训练。</p>
<p>我们使用 fit 函数，传入训练集中的样本特征值 train_X 和结果 train_y，模型会自动拟合。使用 predict 函数进行预测，传入测试集中的样本特征值 test_X，然后就可以得到预测结果。</p>
<h3 id="用AdaBoost对房价进行预测"><a href="#用AdaBoost对房价进行预测" class="headerlink" title="用AdaBoost对房价进行预测"></a>用AdaBoost对房价进行预测</h3><p>我们使用sklearn自带的波士顿房价数据集，用AdaBoost对房价进行预测:</p>
<p><strong>数据集</strong></p>
<p>这个数据集一共包括了 506 条房屋信息数据，每一条数据都包括了 13 个指标，以及一个房屋价位。</p>
<p>13 个指标的含义，可以参考下面的表格：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/191.png">


<p>处理思路（还是之前的处理套路）：</p>
<p>首先加载数据，将数据分割成训练集和测试集，然后创建 AdaBoost 回归模型，传入训练集数据进行拟合，再传入测试集数据进行预测，就可以得到预测结果。最后将预测的结果与实际结果进行对比，得到两者之间的误差。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">data=load_boston()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割数据，样本特征值 train_X 、结果 train_y</span></span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(data.data, data.target, test_size=<span class="number">0.25</span>, random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用AdaBoost回归模型</span></span><br><span class="line">regressor=AdaBoostRegressor()</span><br><span class="line">regressor.fit(train_x,train_y)</span><br><span class="line">pred_y = regressor.predict(test_x)</span><br><span class="line">mse = mean_squared_error(test_y, pred_y)</span><br><span class="line">print(<span class="string">"房价预测结果 "</span>, pred_y)</span><br><span class="line">print(<span class="string">"均方误差 = "</span>,round(mse,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">房价预测结果  [19.5862069  10.36744186 13.8556962  17.01666667 24.83653846 21.57818182</span><br><span class="line"> 30.08265306 18.39166667 30.10983607 19.64464286 30.08265306 32.53333333</span><br><span class="line"> 11.72571429 24.6871345  11.81971831 24.83653846 17.66141304 16.88518519</span><br><span class="line"> 27.9        26.12291667 17.66141304 17.9525     18.32222222 19.15945946</span><br><span class="line"> 31.02606061 18.32222222 21.57818182 24.83653846 11.81971831 30.45185185</span><br><span class="line"> 17.66141304 26.638      10.36744186 20.55242718 26.638      30.94375</span><br><span class="line"> 26.638      11.77142857 13.83246753 25.07005076 14.15901639 12.56393443</span><br><span class="line"> 30.08265306 17.1        27.12684211 19.15945946 18.39166667 19.15260417</span><br><span class="line"> 26.97983539 19.5862069  17.01666667 32.47894737 14.61836735 17.1</span><br><span class="line"> 25.45376344 20.41842105 25.45376344 17.01666667 26.12291667 21.91229947</span><br><span class="line"> 18.6962963  16.88518519 44.09285714 20.55242718 17.66141304 26.638</span><br><span class="line"> 26.43482587 11.77142857 18.41923077 28.12891566 21.39473684 18.41923077</span><br><span class="line"> 17.66141304 27.4728     19.32111111 45.16666667 15.78985507 11.72571429</span><br><span class="line"> 18.32222222 24.51157895 19.80408163 14.93108108 12.56393443 26.43482587</span><br><span class="line"> 20.55242718 20.76607143 46.96666667 17.1        44.10952381 31.2862069</span><br><span class="line"> 30.10983607 19.15260417 18.6962963  17.9525     15.78985507 32.74782609</span><br><span class="line"> 24.6        21.91229947 18.39166667 18.39166667 15.78985507 19.5862069</span><br><span class="line"> 27.15679012 26.43482587 12.12727273 14.61836735 11.72571429 27.12684211</span><br><span class="line"> 12.12727273 26.638      50.         12.69545455 17.26136364 26.43482587</span><br><span class="line"> 30.94375    24.6871345  21.77692308 19.80408163 27.7        20.76607143</span><br><span class="line"> 19.80408163 18.0516129  11.81971831 20.14333333 21.2        17.26136364</span><br><span class="line"> 42.44      ]</span><br><span class="line">均方误差 =  17.89</span><br></pre></td></tr></table></figure>

<p>这个数据集比较规范，不需要在数据清洗、规范化上花太多精力，代码简单</p>
<h3 id="AdaBoost与回归分析模型比较"><a href="#AdaBoost与回归分析模型比较" class="headerlink" title="AdaBoost与回归分析模型比较"></a>AdaBoost与回归分析模型比较</h3><p>下面对比一下<strong>决策树回归</strong>和<strong>KNN回归</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用决策树回归模型</span></span><br><span class="line">dec_regressor=DecisionTreeRegressor()</span><br><span class="line">dec_regressor.fit(train_x,train_y)</span><br><span class="line">pred_y = dec_regressor.predict(test_x)</span><br><span class="line">mse = mean_squared_error(test_y, pred_y)</span><br><span class="line">print(<span class="string">"决策树均方误差 = "</span>,round(mse,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用KNN回归模型</span></span><br><span class="line">knn_regressor=KNeighborsRegressor()</span><br><span class="line">knn_regressor.fit(train_x,train_y)</span><br><span class="line">pred_y = knn_regressor.predict(test_x)</span><br><span class="line">mse = mean_squared_error(test_y, pred_y)</span><br><span class="line">print(<span class="string">"KNN均方误差 = "</span>,round(mse,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">决策树均方误差 =  23.84</span><br><span class="line">KNN均方误差 =  27.87</span><br></pre></td></tr></table></figure>

<p>这里就会发现，AdaBoost 的均方误差更小，也就是结果更优。虽然 AdaBoost 使用了弱分类器，但是通过 50 个甚至更多的弱分类器组合起来而形成的强分类器，在很多情况下结果都优于其他算法。因此 AdaBoost 也是<strong>常用</strong>的分类和回归算法之一。</p>
<h3 id="AdaBoost与决策树模型的比较"><a href="#AdaBoost与决策树模型的比较" class="headerlink" title="AdaBoost与决策树模型的比较"></a>AdaBoost与决策树模型的比较</h3><p>在 sklearn 中 AdaBoost 默认采用的是决策树模型，我们可以随机生成一些数据，然后对比下 AdaBoost 中的弱分类器（也就是决策树弱分类器）、决策树分类器和 AdaBoost 模型在分类准确率上的表现。</p>
<p>如果想要随机生成数据，我们可以使用 sklearn 中的 make_hastie_10_2 函数生成二分类数据。假设我们生成 12000 个数据，取前 2000 个作为测试集，其余作为训练集。</p>
<p>设置AdaBoost迭代次数为200，错误率可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> zero_one_loss</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span>  AdaBoostClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置AdaBoost迭代次数</span></span><br><span class="line">n_estimators=<span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据</span></span><br><span class="line">X,y=datasets.make_hastie_10_2(n_samples=<span class="number">12000</span>,random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 从12000个数据中取前2000行作为测试集，其余作为训练集</span></span><br><span class="line">train_x, train_y = X[<span class="number">2000</span>:],y[<span class="number">2000</span>:]</span><br><span class="line">test_x, test_y = X[:<span class="number">2000</span>],y[:<span class="number">2000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># AdaBoost 中的弱分类器（也就是决策树弱分类器）</span></span><br><span class="line">dt_stump = DecisionTreeClassifier(max_depth=<span class="number">1</span>,min_samples_leaf=<span class="number">1</span>)</span><br><span class="line">dt_stump.fit(train_x, train_y)</span><br><span class="line">dt_stump_err = <span class="number">1.0</span>-dt_stump.score(test_x, test_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树分类器</span></span><br><span class="line">dt = DecisionTreeClassifier()</span><br><span class="line">dt.fit(train_x,  train_y)</span><br><span class="line">dt_err = <span class="number">1.0</span>-dt.score(test_x, test_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># AdaBoost分类器</span></span><br><span class="line">ada = AdaBoostClassifier(base_estimator=dt_stump,n_estimators=n_estimators)</span><br><span class="line">ada.fit(train_x,  train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 三个分类器的错误率可视化</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line"><span class="comment"># 设置plt正确显示中文</span></span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>]</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.plot([<span class="number">1</span>,n_estimators],[dt_stump_err]*<span class="number">2</span>, <span class="string">'k-'</span>, label=<span class="string">u'决策树弱分类器 错误率'</span>)</span><br><span class="line">ax.plot([<span class="number">1</span>,n_estimators],[dt_err]*<span class="number">2</span>,<span class="string">'k--'</span>, label=<span class="string">u'决策树模型 错误率'</span>)</span><br><span class="line">ada_err = np.zeros((n_estimators,))</span><br><span class="line"><span class="comment"># 遍历每次迭代的结果 i为迭代次数, pred_y为预测结果</span></span><br><span class="line"><span class="keyword">for</span> i,pred_y <span class="keyword">in</span> enumerate(ada.staged_predict(test_x)):</span><br><span class="line">     <span class="comment"># 统计错误率</span></span><br><span class="line">    ada_err[i]=zero_one_loss(pred_y, test_y)</span><br><span class="line"><span class="comment"># 绘制每次迭代的AdaBoost错误率 </span></span><br><span class="line">ax.plot(np.arange(n_estimators)+<span class="number">1</span>, ada_err, label=<span class="string">'AdaBoost Test 错误率'</span>, color=<span class="string">'orange'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'迭代次数'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'错误率'</span>)</span><br><span class="line">leg=ax.legend(loc=<span class="string">'upper right'</span>,fancybox=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/189.png" alt="189" style="zoom: 50%;">

<p>从图中你能看出来，弱分类器的错误率最高，只比随机分类结果略好，准确率稍微大于 50%。决策树模型的错误率明显要低很多。而 AdaBoost 模型在迭代次数超过 25 次之后，错误率有了明显下降，经过 125 次迭代之后错误率的变化形势趋于平缓。</p>
<p>因此我们能看出，虽然单独的一个决策树弱分类器效果不好，但是多个决策树弱分类器组合起来形成的 AdaBoost 分类器，分类效果要好于决策树模型。</p>
<p>AdaBoost现在用的不多了，无论是打比赛还是日常应用，都喜欢用xgboost，lightgbm，catboost这些算法了。 当然，虽然学习的深入，这些算法肯定也会大白话出来。 但是出来之前，还是先搞懂AdaBoost的原理吧，这样也好对比，而对比，印象也就越深刻。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/190.png" alt="189">

<h2 id="25-曲线拟合"><a href="#25-曲线拟合" class="headerlink" title="25.曲线拟合"></a>25.曲线拟合</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义x、y散点坐标</span></span><br><span class="line">x = np.arange(<span class="number">3200</span>, <span class="number">35000</span>, <span class="number">3200</span>)</span><br><span class="line">num1 = [<span class="number">0.472</span>, <span class="number">0.469</span>, <span class="number">0.447</span>, <span class="number">0.433</span>, <span class="number">0.418</span>, <span class="number">0.418</span>, <span class="number">0.418</span>, <span class="number">0.418</span>, <span class="number">0.418</span>, <span class="number">0.418</span>]</span><br><span class="line">num2 = [<span class="number">0.337</span>, <span class="number">0.327</span>, <span class="number">0.325</span>, <span class="number">0.316</span>, <span class="number">0.312</span>, <span class="number">0.311</span>, <span class="number">0.308</span>, <span class="number">0.305</span>, <span class="number">0.295</span>, <span class="number">0.290</span>]</span><br><span class="line">y1 = np.array(num1)</span><br><span class="line">y2 = np.array(num2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用3次多项式拟合</span></span><br><span class="line">f1 = np.polyfit(x, y1, <span class="number">3</span>)</span><br><span class="line">p1 = np.poly1d(f1)</span><br><span class="line">print(p1)  <span class="comment"># 打印出拟合函数</span></span><br><span class="line">yvals1 = p1(x)  <span class="comment"># 拟合y值</span></span><br><span class="line"></span><br><span class="line">f2 = np.polyfit(x, y2, <span class="number">3</span>)</span><br><span class="line">p2 = np.poly1d(f2)</span><br><span class="line">print(p2)</span><br><span class="line"><span class="comment"># 也可使用yvals=np.polyval(f1, x)</span></span><br><span class="line">yvals2 = p2(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plot1 = plt.plot(x, y1, <span class="string">'s'</span>, label=<span class="string">'original values'</span>)</span><br><span class="line">plot2 = plt.plot(x, yvals1, <span class="string">'r'</span>, label=<span class="string">'polyfit values'</span>)</span><br><span class="line">plot3 = plt.plot(x, y2, <span class="string">'s'</span>, label=<span class="string">'original values2'</span>)</span><br><span class="line">plot4 = plt.plot(x, yvals2, <span class="string">'r'</span>, label=<span class="string">'polyfit values2'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>, bbox_to_anchor=(<span class="number">1.05</span>, <span class="number">1.0</span>), borderaxespad=<span class="number">0.</span>)</span><br><span class="line">plt.title(<span class="string">'polyfitting'</span>)</span><br><span class="line">plt.savefig(<span class="string">'nihe1.png'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/197.png" alt="197" style="zoom:50%;">

<h2 id="26-DBSCAN聚类"><a href="#26-DBSCAN聚类" class="headerlink" title="26.DBSCAN聚类"></a>26.DBSCAN聚类</h2><p>DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用于<strong>非凸样本集</strong>。下面我们就DBSCAN算法的原理做一个总结。</p>
<h3 id="一、密度聚类原理"><a href="#一、密度聚类原理" class="headerlink" title="一、密度聚类原理"></a>一、密度聚类原理</h3><p>DBSCAN是一种基于<strong>密度</strong>的聚类算法，这类密度聚类算法一般假定：类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的，也就是说，在该类别任意样本周围不远处一定有同类别的样本存在。</p>
<p>通过将<strong>紧密相连</strong>的样本划为一类，这样就得到了一个聚类类别。通过将所有各组紧密相连的样本划为各个不同的类别，则我们就得到了最终的所有聚类类别结果。</p>
<h3 id="二、DBSCAN密度定义"><a href="#二、DBSCAN密度定义" class="headerlink" title="二、DBSCAN密度定义"></a>二、DBSCAN密度定义</h3><p>DBSCAN是如何描述密度聚类的。</p>
<p>DBSCAN是基于一组邻域来描述样本集的紧密程度的，参数(ϵ, MinPts)用来描述邻域的样本分布紧密程度。其中，ϵ描述了某一样本的<strong>邻域距离</strong>阈值，MinPts描述了某一样本的<strong>距离为ϵ的邻域中样本个数</strong>的阈值。</p>
<p>假设样本集是D=(x1,x2,…,xm)，则DBSCAN具体的密度描述定义如下：</p>
<ol>
<li><p>ϵ-邻域：对于xj∈D，其ϵ-邻域包含样本集D中，与xj的距离不大于ϵ的子样本集，即Nϵ(xj)={xi∈D|distance(xi,xj)≤ϵ}, 这个子样本集的个数记为|Nϵ(xj)|　</p>
</li>
<li><p>核心对象：对于某一样本xj∈D，如果其ϵ-邻域对应的Nϵ(xj)至少包含MinPts个样本，即如果|Nϵ(xj)|≥MinPts，则xj是核心对象。　</p>
</li>
<li><p>密度直达：如果xi位于xj的ϵ-邻域中，且xj是核心对象，则称xi由xj密度直达。注意反之不一定成立，即此时不能说xj由xi密度直达, 除非且xi也是核心对象。</p>
</li>
<li><p>密度可达：对于xi和xj,如果存在样本样本序列p1,p2,…,pT,满足p1=xi,pT=xj, 且pt+1由pt密度直达，则称xj由xi密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本p1,p2,…,pT−1均为核心对象，因为只有核心对象才能使其他样本密度直达。注意密度可达也不满足对称性，这个可以由密度直达的不对称性得出。</p>
</li>
<li><p>密度相连：对于xi和xj，如果存在核心对象样本xk，使xi和xj均由xk密度可达，则称xi和xj密度相连。注意密度相连关系是满足对称性的。</p>
</li>
</ol>
<p>从下图可以很容易看出理解上述定义，图中MinPts=5，红色的点都是核心对象，因为其ϵ-邻域至少有5个样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的超球体内，如果不在超球体内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列。在这些密度可达的样本序列的ϵ-邻域内所有的样本相互都是密度相连的。</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/198.png" alt="197" style="zoom: 67%;">

<h3 id="三、DBSCAN密度聚类思想"><a href="#三、DBSCAN密度聚类思想" class="headerlink" title="三、DBSCAN密度聚类思想"></a>三、DBSCAN密度聚类思想</h3><p>DBSCAN的聚类定义很简单：由密度可达关系导出的<strong>最大密度</strong>的<strong>相连样本</strong>集合，即为我们最终聚类的一个类别，或者说一个簇。</p>
<p>这个DBSCAN的簇里面可以有一个或者多个核心对象。如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的ϵ-邻域里；如果有多个核心对象，则簇里的任意一个核心对象的ϵ-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。这些核心对象的ϵ-邻域里所有的样本的集合组成的一个DBSCAN聚类簇。</p>
<p>那么怎么才能找到这样的簇样本集合呢？DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止。</p>
<p>基本上这就是DBSCAN算法的主要内容了，是不是很简单？但是我们还是有三个问题没有考虑：</p>
<ul>
<li><p>第一个是一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。</p>
</li>
<li><p>第二个是距离的度量问题，即如何计算某样本和核心对象样本的距离。在DBSCAN中，一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和KNN分类算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用KD树或者球树来快速的搜索最近邻。概念参考KNN算法</p>
</li>
<li><p>第三种问题比较特殊，某些样本可能到两个核心对象的距离都小于ϵ，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，此时DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说DBSCAN的算法不是完全稳定的算法。</p>
</li>
</ul>
<h3 id="四、DBSCAN聚类算法"><a href="#四、DBSCAN聚类算法" class="headerlink" title="四、DBSCAN聚类算法"></a>四、DBSCAN聚类算法</h3><p>下面对DBSCAN聚类算法的流程做一个总结：</p>
<p>输入：样本集D=(x1,x2,…,xm)，邻域参数(ϵ,MinPts), 样本距离度量方式</p>
<p>输出： 簇划分C.　</p>
<ol>
<li><p>初始化核心对象集合Ω=∅, 初始化聚类簇数k=0，初始化未访问样本集合Γ = D,  簇划分C = ∅</p>
</li>
<li><p>对于j=1,2,…m, 按下面的步骤找出所有的核心对象：</p>
<p>a) 通过距离度量方式，找到样本xj的ϵ-邻域子样本集Nϵ(xj)</p>
<p>b) 如果子样本集样本个数满足|Nϵ(xj)|≥MinPts， 将样本xj加入核心对象样本集合：Ω=Ω∪{xj}</p>
</li>
<li><p>如果核心对象集合Ω=∅，则算法结束，</p>
<p>否则转入步骤4。</p>
</li>
<li><p>在核心对象集合Ω中，随机选择一个核心对象o，初始化当前簇核心对象队列Ωcur={o}, 初始化类别序号k=k+1，初始化当前簇样本集合Ck={o}，更新未访问样本集合Γ=Γ−{o}</p>
</li>
<li><p>如果当前簇核心对象队列Ωcur=∅，则当前聚类簇Ck生成完毕, 更新簇划分C={C1,C2,…,Ck}, 更新核心对象集合Ω=Ω−Ck， 转入步骤3。</p>
<p>否则更新核心对象集合Ω=Ω−Ck。</p>
</li>
<li><p>在当前簇核心对象队列Ωcur中取出一个核心对象o′，通过邻域距离阈值ϵ找出所有的ϵ-邻域子样本集Nϵ(o′)，令Δ=Nϵ(o′)∩Γ, 更新当前簇样本集合Ck=Ck∪Δ, 更新未访问样本集合Γ=Γ−Δ,  更新Ωcur=Ωcur∪(Δ∩Ω)−o′，转入步骤5.</p>
</li>
</ol>
<p>输出结果为： 簇划分C={𝐶1,𝐶2,…,𝐶𝑘}</p>
<h3 id="五、小结优缺点"><a href="#五、小结优缺点" class="headerlink" title="五、小结优缺点"></a>五、小结优缺点</h3><p>和传统的K-Means算法相比，DBSCAN较大的不同就是不需要输入类别数k，当然它较大的优势是可以发现任意形状的聚类簇，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点，这点和BIRCH算法类似。</p>
<p>那么我们什么时候需要用DBSCAN来聚类呢？一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多。如果数据集不是稠密的，则不推荐用DBSCAN来聚类。</p>
<p>下面对DBSCAN算法的优缺点做一个总结。</p>
<p>DBSCAN的主要优点有：</p>
<p>1） 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</p>
<p>2） 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。</p>
<p>3） 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</p>
<p>DBSCAN的主要缺点有：</p>
<p>1）如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</p>
<p>2） 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。</p>
<p>3） 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。</p>
<p>要判断样本集是不是凸样本集比较难。一般的做法是对kmeans，使用不同的k和不同的初始值进行聚类如果效果总是很差，很可能就是因为不是凸集的原因。这是可以换dbscan之类的方法做尝试，如果效果比较好基本就是这个问题了。</p>
<h3 id="六、scikit-learn中的DBSCAN"><a href="#六、scikit-learn中的DBSCAN" class="headerlink" title="六、scikit-learn中的DBSCAN"></a>六、scikit-learn中的DBSCAN</h3><h4 id="scikit-learn中的DBSCAN类"><a href="#scikit-learn中的DBSCAN类" class="headerlink" title="scikit-learn中的DBSCAN类"></a>scikit-learn中的DBSCAN类</h4><p>在scikit-learn中，DBSCAN算法类为sklearn.cluster.DBSCAN。要熟练的掌握用DBSCAN类来聚类，除了对DBSCAN本身的原理有较深的理解以外，还要对最近邻的思想有一定的理解。集合这两者，就可以玩转DBSCAN了。</p>
<h4 id="DBSCAN类重要参数"><a href="#DBSCAN类重要参数" class="headerlink" title="DBSCAN类重要参数"></a>DBSCAN类重要参数</h4><p>DBSCAN类的重要参数也分为两类，一类是DBSCAN算法本身的参数，一类是最近邻度量的参数，下面对这些参数做一个总结。</p>
<ol>
<li><p><strong>eps</strong>： DBSCAN算法参数，即ϵ-邻域的距离阈值，和样本距离超过ϵ的样本点不在ϵ-邻域内。默认值是0.5.一般需要通过在多组值里面选择一个合适的阈值。eps过大，则更多的点会落在核心对象的ϵ-邻域，此时我们的类别数可能会减少， 本来不应该是一类的样本也会被划为一类。反之则类别数可能会增大，本来是一类的样本却被划分开。</p>
</li>
<li><p><strong>min_samples</strong>： DBSCAN算法参数，即样本点要成为核心对象所需要的ϵ-邻域的样本数阈值。默认值是5。一般需要通过在多组值里面选择一个合适的阈值。通常和eps一起调参。在eps一定的情况下，min_samples过大，则核心对象会过少，此时簇内部分本来是一类的样本可能会被标为噪音点，类别数也会变多。反之min_samples过小的话，则会产生大量的核心对象，可能会导致类别数过少。</p>
</li>
<li><p><strong>metric</strong>：最近邻距离度量参数。可以使用的距离度量较多，一般来说DBSCAN使用默认的欧式距离（即p=2的闵可夫斯基距离）就可以满足我们的需求。可以使用的距离度量参数有：</p>
<p>a) 欧式距离 “euclidean”: $\sqrt{\sum\limits_{i=1}^{n}(x_i-y_i)^2}$</p>
<p>b) 曼哈顿距离 “manhattan”： $\sum\limits_{i=1}^{n}|x_i-y_i|$</p>
<p>c) 切比雪夫距离“chebyshev”: $max|x_i-y_i|  (i = 1,2,…n)$</p>
<p>d) 闵可夫斯基距离 “minkowski”: $\sqrt[p]{\sum\limits_{i=1}^{n}(|x_i-y_i|)^p}$ p=1为曼哈顿距离， p=2为欧式距离。</p>
<p>e) 带权重闵可夫斯基距离 “wminkowski”: $\sqrt[p]{\sum\limits_{i=1}^{n}(w*|x_i-y_i|)^p}$ 其中w为特征权重</p>
<p>f) 标准化欧式距离 “seuclidean”: 即对于各特征维度做了归一化以后的欧式距离。此时各样本特征维度的均值为0，方差为1。</p>
<p>g) 马氏距离“mahalanobis”：$\sqrt{(x-y)^TS^{-1}(x-y)}$ 其中，𝑆−1S−1为样本协方差矩阵的逆矩阵。当样本分布独立时， S为单位矩阵，此时马氏距离等同于欧式距离。</p>
</li>
</ol>
<p>　　还有一些其他不是实数的距离度量，一般在DBSCAN算法用不上，这里也就不列了。</p>
<ol start="4">
<li><p><strong>algorithm</strong>：最近邻搜索算法参数，算法一共有三种，第一种是蛮力实现，第二种是KD树实现，第三种是球树实现。这三种方法在<a href="http://www.cnblogs.com/pinard/p/6061661.html" target="_blank" rel="noopener">K近邻法(KNN)原理小结</a>中都有讲述，如果不熟悉可以去复习下。对于这个参数，一共有4种可选输入，‘brute’对应第一种蛮力实现，‘kd_tree’对应第二种KD树实现，‘ball_tree’对应第三种的球树实现， ‘auto’则会在上面三种算法中做权衡，选择一个拟合最好的最优算法。需要注意的是，如果输入样本特征是稀疏的时候，无论我们选择哪种算法，最后scikit-learn都会去用蛮力实现‘brute’。个人的经验，一般情况使用默认的 ‘auto’就够了。 如果数据量很大或者特征也很多，用”auto”建树时间可能会很长，效率不高，建议选择KD树实现‘kd_tree’，此时如果发现‘kd_tree’速度比较慢或者已经知道样本分布不是很均匀时，可以尝试用‘ball_tree’。而如果输入样本是稀疏的，无论你选择哪个算法最后实际运行的都是‘brute’。</p>
</li>
<li><p><strong>leaf_size</strong>：最近邻搜索算法参数，为使用KD树或者球树时， 停止建子树的叶子节点数量的阈值。这个值越小，则生成的KD树或者球树就越大，层数越深，建树时间越长，反之，则生成的KD树或者球树会小，层数较浅，建树时间较短。默认是30. 因为这个值一般只影响算法的运行速度和使用内存大小，因此一般情况下可以不管它。</p>
</li>
<li><p><strong>p</strong>: 最近邻距离度量参数。只用于闵可夫斯基距离和带权重闵可夫斯基距离中p值的选择，p=1为曼哈顿距离， p=2为欧式距离。如果使用默认的欧式距离不需要管这个参数。</p>
</li>
</ol>
<p>以上就是DBSCAN类的主要参数介绍，其实需要调参的就是两个参数eps和min_samples，这两个值的组合对最终的聚类效果有很大的影响。</p>
<h4 id="scikit-learn-DBSCAN聚类实例"><a href="#scikit-learn-DBSCAN聚类实例" class="headerlink" title="scikit-learn DBSCAN聚类实例"></a>scikit-learn DBSCAN聚类实例</h4><p>完整代码参见:<a href="https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/dbscan_cluster.ipynb" target="_blank" rel="noopener">https://github.com/ljpzzz/machinelearning/blob/master/classic-machine-learning/dbscan_cluster.ipynb</a></p>
<p>首先，生成一组随机数据，为了体现DBSCAN在非凸数据的聚类优点，生成了三簇数据，两组是非凸的。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">%matplotlib inline</span><br><span class="line">X1, y1=datasets.make_circles(n_samples=<span class="number">5000</span>, factor=<span class="number">.6</span>,</span><br><span class="line">                                      noise=<span class="number">.05</span>)</span><br><span class="line">X2, y2 = datasets.make_blobs(n_samples=<span class="number">1000</span>, n_features=<span class="number">2</span>, centers=[[<span class="number">1.2</span>,<span class="number">1.2</span>]], cluster_std=[[<span class="number">.1</span>]],</span><br><span class="line">               random_state=<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">X = np.concatenate((X1, X2))</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>数据输出：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/199.png" alt="199" style="zoom:50%;">

<p>首先看看K-Means的聚类效果，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">y_pred = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">9</span>).fit_predict(X)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/200.png" alt="199" style="zoom:50%;">

<p>那么如果使用DBSCAN效果如何呢？我们先不调参，直接用默认参数，看看聚类效果,代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line">y_pred = DBSCAN().fit_predict(X)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/201.png" alt="199" style="zoom:50%;">

<p>全部分为一类了。</p>
<p>看来我们需要对DBSCAN的两个关键的参数eps和min_samples进行调参。</p>
<p>从上图我们可以发现，类别数太少，我们需要增加类别数，那么我们可以减少𝜖ϵ-邻域的大小，默认是0.5，我们减到0.1看看效果。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_pred = DBSCAN(eps = <span class="number">0.1</span>).fit_predict(X)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/202.png" alt="199" style="zoom:50%;">

<p>可以看到聚类效果有了改进，至少边上的那个簇已经被发现出来了。此时我们需要继续调参增加类别，有两个方向都是可以的，一个是继续减少eps，另一个是增加min_samples。我们现在将min_samples从默认的5增加到10，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_pred = DBSCAN(eps = <span class="number">0.1</span>, min_samples = <span class="number">10</span>).fit_predict(X)</span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<img src="//mangosTeeN96.github.io/2020/01/08/python数据分析算法/203.png" alt="199" style="zoom:50%;">

<p>可见现在聚类效果基本已经可以让我们满意了。</p>
<p>上面这个例子只是理解DBSCAN调参的一个基本思路，在实际运用中可能要考虑很多问题，以及更多的参数组合。</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/数据分析/" rel="tag"><i class="fa fa-tags"></i> 数据分析</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/12/28/python数据分析/" rel="next" title="python数据分析">
                <i class="fa fa-chevron-left"></i> python数据分析
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/02/14/天池-数据分析/" rel="prev" title="天池-数据分析">
                天池-数据分析 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="MangosTeen">
            
              <p class="site-author-name" itemprop="name">MangosTeen</p>
              <div class="site-description motion-element" itemprop="description">想养一只猫</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">16</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://github.com/mangosTeeN96" title="Git &rarr; https://github.com/mangosTeeN96" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="mailto:tiansj0810@163.com" title="Mail &rarr; mailto:tiansj0810@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                作品链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://htmlpreview.github.io/?https://github.com/mangosTeeN96/education/blob/master/Offline%20Website/index.html" title="http://htmlpreview.github.io/?https://github.com/mangosTeeN96/education/blob/master/Offline%20Website/index.html" rel="noopener" target="_blank">tianchi</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#16-决策树"><span class="nav-text">16.决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树的工作原理"><span class="nav-text">决策树的工作原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#构造"><span class="nav-text">构造</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#剪枝"><span class="nav-text">剪枝</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何构造决策树"><span class="nav-text">如何构造决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#纯度："><span class="nav-text">纯度：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#信息熵：表示信息的不确定度"><span class="nav-text">信息熵：表示信息的不确定度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#信息增益（ID3算法）："><span class="nav-text">信息增益（ID3算法）：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在-ID3-算法上进行改进的-C4-5-算法"><span class="nav-text">在 ID3 算法上进行改进的 C4.5 算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CART算法"><span class="nav-text">CART算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CART-分类树的工作流程"><span class="nav-text">CART 分类树的工作流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何使用-CART-算法来创建分类树"><span class="nav-text">如何使用 CART 算法来创建分类树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CART-回归树的工作流程"><span class="nav-text">CART 回归树的工作流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何使用-CART-回归树做预测"><span class="nav-text">如何使用 CART 回归树做预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CART-决策树的剪枝"><span class="nav-text">CART 决策树的剪枝</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#总结："><span class="nav-text">总结：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#※-决策树之泰坦尼克号生存预测"><span class="nav-text">※ 决策树之泰坦尼克号生存预测</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sklearn-中的决策树模型"><span class="nav-text">sklearn 中的决策树模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Titanic-乘客生存预测"><span class="nav-text">Titanic 乘客生存预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#生存预测的关键流程"><span class="nav-text">生存预测的关键流程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#模块-1：数据探索"><span class="nav-text">模块 1：数据探索</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#模块-2：数据清洗"><span class="nav-text">模块 2：数据清洗</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#模块-3：特征选择"><span class="nav-text">模块 3：特征选择</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#模块-4：决策树模型"><span class="nav-text">模块 4：决策树模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#模块-5：模型预测-amp-评估"><span class="nav-text">模块 5：模型预测 &amp; 评估</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#模块-6：决策树可视化"><span class="nav-text">模块 6：决策树可视化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#完整代码"><span class="nav-text">完整代码</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树模型使用技巧总结"><span class="nav-text">决策树模型使用技巧总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#17-朴素贝叶斯分类"><span class="nav-text">17.朴素贝叶斯分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#贝叶斯原理"><span class="nav-text">贝叶斯原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#朴素贝叶斯"><span class="nav-text">朴素贝叶斯</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#朴素贝叶斯分类工作原理"><span class="nav-text">朴素贝叶斯分类工作原理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#离散数据案例"><span class="nav-text">离散数据案例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#连续数据案例"><span class="nav-text">连续数据案例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#朴素贝叶斯分类器工作流程"><span class="nav-text">朴素贝叶斯分类器工作流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sklearn-机器学习包"><span class="nav-text">sklearn 机器学习包</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#如何求-TF-IDF"><span class="nav-text">如何求 TF-IDF</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#如何对文档进行分类"><span class="nav-text">如何对文档进行分类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#18-SVM分类"><span class="nav-text">18.SVM分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、SVM-的工作原理"><span class="nav-text">一、SVM 的工作原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、-用-SVM-如何解决多分类问题"><span class="nav-text">二、 用 SVM 如何解决多分类问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三、-如何在-sklearn-中使用-SVM"><span class="nav-text">三、 如何在 sklearn 中使用 SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#四、-如何用-SVM-进行乳腺癌检测"><span class="nav-text">四、 如何用 SVM 进行乳腺癌检测</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#19-KNN分类"><span class="nav-text">19.KNN分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、KNN-的工作原理"><span class="nav-text">一、KNN 的工作原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-值如何选择"><span class="nav-text">K 值如何选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#距离如何计算"><span class="nav-text">距离如何计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KD-树"><span class="nav-text">KD 树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#用-KNN-做回归"><span class="nav-text">用 KNN 做回归</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、-如何在-sklearn-中使用-KNN"><span class="nav-text">二、 如何在 sklearn 中使用 KNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三、-如何用-KNN-对手写数字进行识别分类"><span class="nav-text">三、 如何用 KNN 对手写数字进行识别分类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#20-K-Means聚类"><span class="nav-text">20.K-Means聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、K-Means-的工作原理"><span class="nav-text">一、K-Means 的工作原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、如何给亚洲球队做聚类"><span class="nav-text">二、如何给亚洲球队做聚类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三、如何使用-sklearn-中的-K-Means-算法"><span class="nav-text">三、如何使用 sklearn 中的 K-Means 算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#四、用KMeans对图像进行分割"><span class="nav-text">四、用KMeans对图像进行分割</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#五、总结"><span class="nav-text">五、总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#21-EM聚类"><span class="nav-text">21.EM聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、EM-算法的工作原理"><span class="nav-text">一、EM 算法的工作原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、EM-聚类的工作原理"><span class="nav-text">二、EM 聚类的工作原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三、-如何使用-EM-工具包"><span class="nav-text">三、 如何使用 EM 工具包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#四、-如何用-EM-算法对王者荣耀数据进行聚类"><span class="nav-text">四、 如何用 EM 算法对王者荣耀数据进行聚类</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#22-关联规则挖掘（Apriori、FP-Growth算法）"><span class="nav-text">22.关联规则挖掘（Apriori、FP-Growth算法）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、-关联规则中的几个概念"><span class="nav-text">一、 关联规则中的几个概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、-Apriori-的工作原理"><span class="nav-text">二、 Apriori 的工作原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三、-Apriori-的改进算法：FP-Growth-算法"><span class="nav-text">三、 Apriori 的改进算法：FP-Growth 算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#四、-使用-Apriori-工具包"><span class="nav-text">四、 使用 Apriori 工具包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#五、-挖掘导演是如何选择演员的"><span class="nav-text">五、 挖掘导演是如何选择演员的</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#23-PageRank算法-重要性评估"><span class="nav-text">23.PageRank算法(重要性评估)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PageRank原理"><span class="nav-text">PageRank原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PageRank实战"><span class="nav-text">PageRank实战</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用-PageRank-揭秘希拉里邮件中的人物关系"><span class="nav-text">用 PageRank 揭秘希拉里邮件中的人物关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#24-AdaBoost"><span class="nav-text">24.AdaBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaBoost的工作原理"><span class="nav-text">AdaBoost的工作原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaBoost算法示例"><span class="nav-text">AdaBoost算法示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaBoost工具使用"><span class="nav-text">AdaBoost工具使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用AdaBoost对房价进行预测"><span class="nav-text">用AdaBoost对房价进行预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaBoost与回归分析模型比较"><span class="nav-text">AdaBoost与回归分析模型比较</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaBoost与决策树模型的比较"><span class="nav-text">AdaBoost与决策树模型的比较</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#25-曲线拟合"><span class="nav-text">25.曲线拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#26-DBSCAN聚类"><span class="nav-text">26.DBSCAN聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一、密度聚类原理"><span class="nav-text">一、密度聚类原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二、DBSCAN密度定义"><span class="nav-text">二、DBSCAN密度定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三、DBSCAN密度聚类思想"><span class="nav-text">三、DBSCAN密度聚类思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#四、DBSCAN聚类算法"><span class="nav-text">四、DBSCAN聚类算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#五、小结优缺点"><span class="nav-text">五、小结优缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#六、scikit-learn中的DBSCAN"><span class="nav-text">六、scikit-learn中的DBSCAN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#scikit-learn中的DBSCAN类"><span class="nav-text">scikit-learn中的DBSCAN类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DBSCAN类重要参数"><span class="nav-text">DBSCAN类重要参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#scikit-learn-DBSCAN聚类实例"><span class="nav-text">scikit-learn DBSCAN聚类实例</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">MangosTeen</span>

  

  
</div>





        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.2"></script>




  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  


  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  <script async src="/js/cursor/fireworks.js"></script>




</body>
</html>
